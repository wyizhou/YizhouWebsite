<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>(一)漫话中文分词：最大匹配,双向最大,最小词数 | Lucas&#39;s Website</title>
<meta name="title" content="(一)漫话中文分词：最大匹配,双向最大,最小词数" />
<meta name="description" content="中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：
词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等 字标注分词：如HMM（隐马尔可夫）模型等 而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。
词典分词 基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是&quot;结婚/的/和/尚未/结婚/的&quot;，而实际中则会被分词为&quot;结婚/的/和尚/未/结婚/的&quot;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如Trie树和AC自动机，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。
正向最大匹配算法（Forward Maximum Matching） 正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的&quot;最大&quot;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置&#43;1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置&#43;1开始再次进行切分和查询。 比如，有一段文本&quot;中文分词算法&quot;，字典中只包含了一个词&quot;分词&quot;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到&quot;中文&quot;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到&quot;中&quot;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[&ldquo;中&rdquo;]。 继续进行第二次切分，得到的文本为&quot;文分&quot;，进行查询词典，第一次查询&quot;文分&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;文&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;文&quot;，加上次的结果记作[&ldquo;中&rdquo;,&ldquo;文&rdquo;] 继续第三次切分，得到文本&quot;分词&quot;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;]。 继续第四次切分，得到文本&quot;算法&quot;，进行查询字典，第一次查询&quot;算法&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;算&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;算&quot;，加上次的结果记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;, &ldquo;法&rdquo;] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：
sentence = &#39;中文分词算法&#39; # 输入的句子 cutList = [&#39;分词&#39;] # 分词词典 start = 0 #设置切分起始位置 maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度 cut_result = [] # 设置一个空的分词结果 while (start &lt;= len(sentence)): #开始循环，如果start大于等于句子长度则停止分词 end = start &#43; maxWidth # 计算每次切分的停止位置 word = sentence[start: end] # 开始切分，文本为变量start和end的区间内字符 while ( word ) : # python对于空字符串会转换为False if ( word in cutList ) : # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环 cut_result." />
<meta name="keywords" content="数学与算法," />


<meta property="og:url" content="https://vec6.com/blog/chinesecutwords-1/">
  <meta property="og:site_name" content="Lucas&#39;s Website">
  <meta property="og:title" content="(一)漫话中文分词：最大匹配,双向最大,最小词数">
  <meta property="og:description" content="中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：
词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等 字标注分词：如HMM（隐马尔可夫）模型等 而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。
词典分词 基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是&#34;结婚/的/和/尚未/结婚/的&#34;，而实际中则会被分词为&#34;结婚/的/和尚/未/结婚/的&#34;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如Trie树和AC自动机，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。
正向最大匹配算法（Forward Maximum Matching） 正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的&#34;最大&#34;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置&#43;1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置&#43;1开始再次进行切分和查询。 比如，有一段文本&#34;中文分词算法&#34;，字典中只包含了一个词&#34;分词&#34;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到&#34;中文&#34;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到&#34;中&#34;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[“中”]。 继续进行第二次切分，得到的文本为&#34;文分&#34;，进行查询词典，第一次查询&#34;文分&#34;在字典中不存在，去掉最后一个字符，继续以剩余部分’文’查询第二次，未查询到，那么返回最后这个字符&#34;文&#34;，加上次的结果记作[“中”,“文”] 继续第三次切分，得到文本&#34;分词&#34;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[“中”, “文”, “分词”]。 继续第四次切分，得到文本&#34;算法&#34;，进行查询字典，第一次查询&#34;算法&#34;在字典中不存在，去掉最后一个字符，继续以剩余部分’算’查询第二次，未查询到，那么返回最后这个字符&#34;算&#34;，加上次的结果记作[“中”, “文”, “分词”, “算”] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[“中”, “文”, “分词”, “算”, “法”] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：
sentence = &#39;中文分词算法&#39; # 输入的句子 cutList = [&#39;分词&#39;] # 分词词典 start = 0 #设置切分起始位置 maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度 cut_result = [] # 设置一个空的分词结果 while (start &lt;= len(sentence)): #开始循环，如果start大于等于句子长度则停止分词 end = start &#43; maxWidth # 计算每次切分的停止位置 word = sentence[start: end] # 开始切分，文本为变量start和end的区间内字符 while ( word ) : # python对于空字符串会转换为False if ( word in cutList ) : # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环 cut_result.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2020-11-08T10:36:34+00:00">
    <meta property="article:modified_time" content="2020-11-08T10:36:34+00:00">
    <meta property="article:tag" content="数学与算法">



<meta name="twitter:card" content="summary"><meta name="twitter:title" content="(一)漫话中文分词：最大匹配,双向最大,最小词数">
<meta name="twitter:description" content="中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：
词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等 字标注分词：如HMM（隐马尔可夫）模型等 而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。
词典分词 基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是&quot;结婚/的/和/尚未/结婚/的&quot;，而实际中则会被分词为&quot;结婚/的/和尚/未/结婚/的&quot;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如Trie树和AC自动机，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。
正向最大匹配算法（Forward Maximum Matching） 正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的&quot;最大&quot;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置&#43;1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置&#43;1开始再次进行切分和查询。 比如，有一段文本&quot;中文分词算法&quot;，字典中只包含了一个词&quot;分词&quot;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到&quot;中文&quot;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到&quot;中&quot;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[&ldquo;中&rdquo;]。 继续进行第二次切分，得到的文本为&quot;文分&quot;，进行查询词典，第一次查询&quot;文分&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;文&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;文&quot;，加上次的结果记作[&ldquo;中&rdquo;,&ldquo;文&rdquo;] 继续第三次切分，得到文本&quot;分词&quot;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;]。 继续第四次切分，得到文本&quot;算法&quot;，进行查询字典，第一次查询&quot;算法&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;算&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;算&quot;，加上次的结果记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;, &ldquo;法&rdquo;] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：
sentence = &#39;中文分词算法&#39; # 输入的句子 cutList = [&#39;分词&#39;] # 分词词典 start = 0 #设置切分起始位置 maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度 cut_result = [] # 设置一个空的分词结果 while (start &lt;= len(sentence)): #开始循环，如果start大于等于句子长度则停止分词 end = start &#43; maxWidth # 计算每次切分的停止位置 word = sentence[start: end] # 开始切分，文本为变量start和end的区间内字符 while ( word ) : # python对于空字符串会转换为False if ( word in cutList ) : # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环 cut_result.">




  <meta itemprop="name" content="(一)漫话中文分词：最大匹配,双向最大,最小词数">
  <meta itemprop="description" content="中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：
词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等 字标注分词：如HMM（隐马尔可夫）模型等 而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。
词典分词 基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是&quot;结婚/的/和/尚未/结婚/的&quot;，而实际中则会被分词为&quot;结婚/的/和尚/未/结婚/的&quot;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如Trie树和AC自动机，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。
正向最大匹配算法（Forward Maximum Matching） 正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的&quot;最大&quot;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置&#43;1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置&#43;1开始再次进行切分和查询。 比如，有一段文本&quot;中文分词算法&quot;，字典中只包含了一个词&quot;分词&quot;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到&quot;中文&quot;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到&quot;中&quot;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[&ldquo;中&rdquo;]。 继续进行第二次切分，得到的文本为&quot;文分&quot;，进行查询词典，第一次查询&quot;文分&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;文&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;文&quot;，加上次的结果记作[&ldquo;中&rdquo;,&ldquo;文&rdquo;] 继续第三次切分，得到文本&quot;分词&quot;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;]。 继续第四次切分，得到文本&quot;算法&quot;，进行查询字典，第一次查询&quot;算法&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;算&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;算&quot;，加上次的结果记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;, &ldquo;法&rdquo;] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：
sentence = &#39;中文分词算法&#39; # 输入的句子 cutList = [&#39;分词&#39;] # 分词词典 start = 0 #设置切分起始位置 maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度 cut_result = [] # 设置一个空的分词结果 while (start &lt;= len(sentence)): #开始循环，如果start大于等于句子长度则停止分词 end = start &#43; maxWidth # 计算每次切分的停止位置 word = sentence[start: end] # 开始切分，文本为变量start和end的区间内字符 while ( word ) : # python对于空字符串会转换为False if ( word in cutList ) : # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环 cut_result.">
  <meta itemprop="datePublished" content="2020-11-08T10:36:34+00:00">
  <meta itemprop="dateModified" content="2020-11-08T10:36:34+00:00">
  <meta itemprop="wordCount" content="304">
  <meta itemprop="keywords" content="数学与算法">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #f2f2f2;
  }

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
    overflow-x: auto;
  }

  div.highlight pre {
    background-color: initial;
    color: initial;
  }

  div.highlight code {
    background-color: unset;
    color: unset;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }

  @media (prefers-color-scheme: dark) {
    body {
      background-color: #333;
      color: #ddd;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    strong,
    b {
      color: #eee;
    }

    a {
      color: #8cc2dd;
    }

    code {
      background-color: #777;
    }

    pre code {
      color: #ddd;
    }

    blockquote {
      color: #ccc;
    }

    textarea,
    input {
      background-color: #252525;
      color: #ddd;
    }

    .helptext {
      color: #aaa;
    }
  }

</style>


  


  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">


MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-screen-web@1.321.0/style.min.css"/>
  <style>
    body {
      font-family: "LXGW WenKai Screen";
      font-weight: normal;
    }
  </style></head>

<body>
  <header><a href="/" class="title">
  <h2>Lucas&#39;s Website</h2>
</a>
<nav><a href="/">Home</a>


<a href="/blog">Blog</a>

</nav>
</header>
  <main>

<h1>(一)漫话中文分词：最大匹配,双向最大,最小词数</h1>
<p>
  <i>
    <time datetime='2020-11-08' pubdate>
      2020-11-08
    </time>
  </i>
</p>

<content>
  <p>中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：</p>
<ol>
<li>词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等</li>
<li>字标注分词：如HMM（隐马尔可夫）模型等</li>
</ol>
<p>而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。</p>
<h2 id="词典分词">词典分词</h2>
<p>基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是&quot;结婚/的/和/尚未/结婚/的&quot;，而实际中则会被分词为&quot;结婚/的/和尚/未/结婚/的&quot;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如<a href="https://zh.wikipedia.org/wiki/Trie">Trie树</a>和<a href="https://zh.wikipedia.org/wiki/AC%E8%87%AA%E5%8A%A8%E6%9C%BA%E7%AE%97%E6%B3%95">AC自动机</a>，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。</p>
<h3 id="正向最大匹配算法forward-maximum-matching">正向最大匹配算法（Forward Maximum Matching）</h3>
<p>正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的&quot;最大&quot;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置+1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置+1开始再次进行切分和查询。 比如，有一段文本&quot;中文分词算法&quot;，字典中只包含了一个词&quot;分词&quot;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到&quot;中文&quot;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到&quot;中&quot;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[&ldquo;中&rdquo;]。 继续进行第二次切分，得到的文本为&quot;文分&quot;，进行查询词典，第一次查询&quot;文分&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;文&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;文&quot;，加上次的结果记作[&ldquo;中&rdquo;,&ldquo;文&rdquo;] 继续第三次切分，得到文本&quot;分词&quot;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;]。 继续第四次切分，得到文本&quot;算法&quot;，进行查询字典，第一次查询&quot;算法&quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&rsquo;算&rsquo;查询第二次，未查询到，那么返回最后这个字符&quot;算&quot;，加上次的结果记作[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[&ldquo;中&rdquo;, &ldquo;文&rdquo;, &ldquo;分词&rdquo;, &ldquo;算&rdquo;, &ldquo;法&rdquo;] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：</p>
<pre tabindex="0"><code>sentence = &#39;中文分词算法&#39; # 输入的句子
cutList = [&#39;分词&#39;]  # 分词词典

start = 0 #设置切分起始位置
maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度
cut_result = [] # 设置一个空的分词结果

while (start &lt;= len(sentence)):  #开始循环，如果start大于等于句子长度则停止分词
    end = start + maxWidth     # 计算每次切分的停止位置
    word = sentence[start: end] # 开始切分，文本为变量start和end的区间内字符
    while ( word ) :  # python对于空字符串会转换为False
        if ( word in cutList ) :  # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环
            cut_result.append(word)
            start = start + len(word) - 1  # 然后将开始位置设置为当前开始位置加上被匹配词的长度
            break 
        if (len(word[:-1]) == 0):
            cut_result.append(word) # 如果最后一个字符也没有被匹配到，那么返回最后一个字符
            break
        word = word[:-1]  # 将word去掉最后一个字符串并重新计算
    start = start + 1  # 将位置加1

print(cut_result)
#[&#39;中&#39;, &#39;文&#39;, &#39;分词&#39;, &#39;算&#39;, &#39;法&#39;] 
</code></pre><h3 id="反向最大匹配算法backward-maximum-matching">反向最大匹配算法（Backward Maximum Matching）</h3>
<p>反向最大匹配算法与正向最大匹配算法是相反的，比如&quot;中文分词算法&quot;文本的正向最大匹配算法在切分宽度为2的时候，是从&quot;中文&quot;开始切分的，而反向则是从&quot;算法&quot;开始切分的。 除了反向的切分，其中对于切分块内的文本依次去掉最后一个字符也变为了依次去掉第一个字符，比如正向第一个切分块&quot;中文&quot;后，如果没有匹配到，则去掉&quot;文&quot;，再对&quot;中&quot;字符进行匹配，而反向则是拿到&quot;算法&quot;后，如果没有匹配到，则是去掉&quot;算&quot;，再对&quot;法&quot;进行匹配。 反向最大匹配算法对比于正向最大匹配算法来说，可以解决一定的交集型歧义，比如本文&quot;他说的确实在理&quot;，理想情况下希望的分词结果中包含&quot;确实&quot;这一词，而正向最大匹配算法结果为&quot;他/说/的确/实/在理&quot;，而反向最大匹配算法的结果为&quot;他/说/的/确实/在理&quot;。 这两种方式很难区分到底谁好谁坏，比如上面的问题中，如果你希望的分词为&quot;的确&quot;，但是如果使用反向的话就很难被分出来。 反向最大匹配算法具体的实现代码：</p>
<pre tabindex="0"><code>sentence = &#39;他说的确实在理&#39; # 输入的句子
cutList = [&#39;的确&#39;, &#39;确实&#39;]  # 分词词典
start = len(sentence) #设置切分起始位置为该文本的最后一个字符
maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度
cut_result = [] # 设置一个空的分词结果

while (start &gt; 0):  #开始循环，如果start大于等于句子长度则停止分词
    end = start - maxWidth   # 计算结束位置，结束位置为开始位置减去宽度
    word = sentence[end: start] # 开始切分，文本为变量end和start的区间内字符
    while ( word ) :  # python对于空字符串会转换为False
        if ( word in cutList ) :  # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环
            cut_result.insert(0,word)
            start = start - len(word) + 1  # 然后将开始位置设置为当前开始位置加上被匹配词的长度
            break
        if (len(word) == 1):
            cut_result.insert(0, word) # 返回最后一个字符
            break
        word = word[1:]  # 将word去掉第一个字符串并重新计算
    start = start - 1  # 将位置减1
cut_result.insert(0, sentence[0]) # 将剩余的第一个字符添加进结果

print(cut_result)
#[&#39;他&#39;, &#39;说&#39;, &#39;的&#39;, &#39;确实&#39;, &#39;在&#39;, &#39;理&#39;] 
</code></pre><h3 id="双向最大匹配算法bidirectional-maximum-match">双向最大匹配算法（Bidirectional Maximum Match）</h3>
<p>双向最大匹配算法是将正向和反向结果的颗粒度进行比较的一种算法，本质上是一种规则系统，该规则为如下：</p>
<ol>
<li>返回词数最少的结果</li>
<li>返回单字词更少的结果</li>
<li>如果两则都相同优先返回反向最大匹配算法结果</li>
</ol>
<p>因为双向最大匹配算法实际上是一种规则系统，只需要对结果进行判断优先返回哪种结果，所以这里就不过多的说明。但需要注意的是采用双向最大匹配算法实际上运行了两种算法，所以对于运算量来说是双倍。</p>
<h3 id="最少词数算法minimal-word-count">最少词数算法（Minimal Word Count）</h3>
<p>最少词数算法也被称为最少切分算法，最少词数算法的本质是将一段文本分词的结果最少，最少次数算法整个过程是将字典按照长度进行排序，首先对最长的字典中的词进行匹配字符串，如果有则切分，并继续匹配下一个字典中的词，如果没有则继续匹配按照顺序匹配。 比如&quot;独立自主和平等互利的原则&quot;，正向匹配的结果为&quot;独立自主/和平/等/互利/的/原则&quot;，而最少词数的结果&quot;独立自主/和/平等互利/的/原则&quot;。 下面为一个非常简单的实现：</p>
<pre tabindex="0"><code>sentence = &#39;独立自主和平等互利的原则&#39; # 输入的句子
cutList = [&#39;独立自主&#39;, &#39;平等互利&#39;, &#39;独立&#39;, &#39;自主&#39;, &#39;和平&#39;, &#39;平等&#39;, &#39;互利&#39;, &#39;原则&#39;]  # 分词词典
cutList = sorted(cutList, key=len, reverse=True) # 字典排序
for cut in cutList:
    if(&#39;/%s&#39;%cut not in sentence and &#39;%s/&#39;%cut not in sentence and cut in sentence) :
        sentence = sentence.replace(cut, &#39;/%s/&#39;%cut)

print(sentence)
#/独立自主/和/平等互利/的/原则 
</code></pre><h2 id="参考文档">参考文档</h2>
<ol>
<li><a href="https://www.cnblogs.com/cyandn/p/10891608.html">https://www.cnblogs.com/cyandn/p/10891608.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/103392455">https://zhuanlan.zhihu.com/p/103392455</a></li>
<li><a href="http://www.matrix67.com/blog/archives/4212">http://www.matrix67.com/blog/archives/4212</a></li>
<li><a href="https://kexue.fm/archives/3908">https://kexue.fm/archives/3908</a></li>
</ol>

</content>
<p>
  
  <a href="https://vec6.com/tags/%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95/">#数学与算法</a>
  
</p>

  </main>
  <footer>Made with <a href="https://github.com/janraasch/hugo-bearblog/">Hugo ʕ•ᴥ•ʔ Bear</a>



</footer>

    

</body>

</html>
