<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math on Lucas Wu</title>
    <link>https://0b44ccdd.vec6.com/categories/math/</link>
    <description>Recent content in Math on Lucas Wu</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 27 Jan 2025 10:53:34 +0000</lastBuildDate>
    <atom:link href="https://0b44ccdd.vec6.com/categories/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ridge回归</title>
      <link>https://0b44ccdd.vec6.com/blog/ridgereg/</link>
      <pubDate>Mon, 27 Jan 2025 10:53:34 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/ridgereg/</guid>
      <description>&lt;p&gt;Ridge回归简单的来讲，是用于解决算法解释数据的时候，数据中特征高度相关（多重共线性）导致的过拟合，即完美拟合训练数据，无法很好的预测新数据，这是因为把训练数据的噪音也学习了，Ridge原理是在回归的基础上增加了一个“惩罚项”，让模型不要依赖某些变量。&lt;/p&gt;&#xA;&lt;p&gt;比如预测房价的例子，现在有面积、房间数量、地理位置和朝向等特征，其中面积和房间数量就是高度相关的变量，因为面积大了，房间数量也一定会增加，所以当把面积（X1）和房间数量（X2）输入给算法，模型则有可能出现X1系数很大，X2系数很小，或者反过来的情况，导致模型解释能力变差，回归系数也无法稳定，主要是因为无法知道其中X1和X2对于模型的贡献情况，就好比两个员工一起做事，交付了后，老板无法评估这个成果每个人付贡献谁更大。&lt;/p&gt;&#xA;&lt;p&gt;Ridge回归目标函数为，这个回归函数为要解决的问题的数学公式（理论解）：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\min_{\beta} \sum_{i=1}^{n} (y_i - \hat{y}i)^2 + \alpha \sum{j=1}^{p} \beta_j^2&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;而将上述目标函数推导为矩阵公式后，得到如下矩阵公式（解析解），为什么要推导？这是因为Ridge回归的目标函数是一个数学表达式，是一个理论解，而解析解（即推导后的公式）则是一个计算机可以直接求解的矩阵公式。&lt;/p&gt;&#xA;&lt;p&gt;这个公式实际上对比最小二乘法解系数的公式中添加了正则项，也就是通过增大a达到增大分母的作用，让整体系数变小，同时越大的系数，就会惩罚越大。&#xA;$$&#xA;\beta = (X^T X + \alpha I)^{-1} X^T Y&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>矩阵计算</title>
      <link>https://0b44ccdd.vec6.com/blog/matrixcalc/</link>
      <pubDate>Sun, 12 Jan 2025 23:53:34 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/matrixcalc/</guid>
      <description>&lt;p&gt;矩阵计算本质是来源于线性代数的定义，其计算方式是通过行与列的逐项匹配、相乘并求和来生成一个新的矩阵，同时矩阵A的每一行代表一个观测点或者样本，每一列代表每个观测点或样本的不同特征。&lt;/p&gt;&#xA;&lt;p&gt;矩阵的计算是基于矩阵的形状（维度）进行的，比如矩阵A为m行n列，矩阵B为p行q列，那么只要n = p，则满足矩阵计算的规则，所以&lt;strong&gt;矩阵的计算必须要满足被乘矩阵A的列数 = 乘数矩阵B的行数才能进行矩阵计算，同时输出的矩阵的形状也是为m行*q列，即行数来自于左侧矩阵A的行，列数来自于右侧矩阵B的列数。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;举例，如下两个矩阵，矩阵A为顾客购买水果的数据，对应一个2*2的矩阵：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;购买苹果数量&lt;/th&gt;&#xA;          &lt;th&gt;购买香蕉数量&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;顾客1&lt;/td&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;顾客2&lt;/td&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;矩阵B为水果的单价，对应为一个2*1的列向量：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;单价&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;苹果&lt;/td&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;香蕉&lt;/td&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;上述的两个矩阵满组矩阵计算的需求，即A的列数 = B的行数，然后结果为[2,1]的矩阵。&lt;/p&gt;&#xA;&lt;p&gt;矩阵计算，矩阵乘法的规则是&lt;strong&gt;行乘列并逐项求和&lt;/strong&gt;：&#xA;$$&#xA;\begin{bmatrix}&#xA;1 &amp;amp; 3 \newline&#xA;2 &amp;amp; 1&#xA;\end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix}&#xA;2 \newline&#xA;3&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix}&#xA;1 \cdot 2 + 3 \cdot 3 \newline&#xA;2 \cdot 2 + 1 \cdot 3&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix}&#xA;11 \newline&#xA;7&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>多次多项式回归</title>
      <link>https://0b44ccdd.vec6.com/blog/multiplepolynomialregression/</link>
      <pubDate>Sat, 11 Jan 2025 23:53:34 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/multiplepolynomialregression/</guid>
      <description>&lt;p&gt;线性回归的核心是描述直线关系，适用于变量间的简单线性关系，而多次多项式回归则赋予模型描述非线性的回归模型（比如曲线以及更复杂的曲线），它本质上是在线性回归基础上扩展的，增加模型的灵活性，更好的拟合非线性的数据趋势，多次多项式根据需要拟合的数据趋势要求不同，可以进行包含一次、两次、三次等多项式的回归，而每次增加次项，则会增加线条的波动能力。&lt;/p&gt;&#xA;&lt;h2 id=&#34;多项式回归&#34;&gt;多项式回归&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;一次多项式&lt;/strong&gt;（线性回归）&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;作用：a0，决定了线条的“斜率”，描述x增加时，y的线性变化。&lt;/li&gt;&#xA;&lt;li&gt;特性：斜率是均匀的，整个线条是直的。&lt;/li&gt;&#xA;&lt;li&gt;适用场景：适合于简单的线性增长或者减少。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;二次多项式&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;作用：a1x决定了线条是否“弯曲”，即呈现出正U(a2 &amp;gt; 0)或者倒U(a2 &amp;lt; 0)的形状&lt;/li&gt;&#xA;&lt;li&gt;特性：解释数据中整体的加速或减速的趋势，比如汽车加速（a2 &amp;gt; 0 ），速度逐渐变快。&lt;/li&gt;&#xA;&lt;li&gt;适用场景：适用于简单非线性趋势。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;三次及以上的多项式&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;作用：a2x^2决定了线条的“波动”能力，描述线条的多个拐点。&lt;/li&gt;&#xA;&lt;li&gt;特性：增加了对于细节的捕捉能力，允许在数据中不同区域出现不同趋势&lt;/li&gt;&#xA;&lt;li&gt;适用场景：复杂的非线性趋势。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;在使用多项式回归的时候，需要根据数据的特性，优先选择低阶多项式，如果数据趋势较为负责，可以逐步增加项式，但需要注意的是过高的项次会导致曲线过渡的贴合每个数据，导致模型预测的时候无法泛化到新数据，即过拟合。&lt;/p&gt;&#xA;&lt;p&gt;多次多项式公式为：&#xA;$$&#xA;y = a_0 + a_1x + a_2x^2 + a_3x^3 + \dots + a_nx^n&#xA;$$&#xA;&lt;strong&gt;公式的每部分含义：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;y：目标变量（预测值）。&lt;/li&gt;&#xA;&lt;li&gt;x：自变量（输入值）。&lt;/li&gt;&#xA;&lt;li&gt;$a_0,$ $a_1$, $a_2$, …：系数（模型需要学习的权重）。&lt;/li&gt;&#xA;&lt;li&gt;$x^2,$ $x^3$ …, $x^n$：自变量的高次项，用于拟合非线性数据。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;下面通过一个外卖的例子来说明一次多项式、二次多项式以及三次多项式的不同之处：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;距离（公里,自变量x）&lt;/th&gt;&#xA;          &lt;th&gt;配送时间（分钟，因变量y）&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;20&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;35&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;70&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;观测数据方面，可以明显看见数据是非线性增长，即距离越远，配送时间越长，所以使用线性回归（即一次多项式）来拟合数据，会出现部分特征无法拟合，下面用不同的多项式来拟合数据。&lt;/p&gt;&#xA;&lt;h3 id=&#34;一次多项式回归&#34;&gt;一次多项式回归&lt;/h3&gt;&#xA;&lt;p&gt;一次多项式回归，即线性回归，公式为：&#xA;$$&#xA;y = a * x + b&#xA;$$&#xA;结合数据做简单的平均变化法可以得斜率（系数）a，公式：&#xA;$$&#xA;a = (70 - 10) / (5 - 1) = 15&#xA;$$&#xA;再算出截率b：&#xA;$$&#xA;10 = 15 * 1 + a1 \newline&#xA;等于 \newline&#xA;a1 = -5&#xA;$$&#xA;最终预测公式为：&#xA;$$&#xA;y = 15 * x + (-5)&#xA;$$&#xA;当距离为3公里的时候，预测为40分钟。&lt;/p&gt;</description>
    </item>
    <item>
      <title>线性回归</title>
      <link>https://0b44ccdd.vec6.com/blog/linearreg/</link>
      <pubDate>Sat, 11 Jan 2025 23:53:34 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/linearreg/</guid>
      <description>&lt;h2 id=&#34;简单线性回归&#34;&gt;简单线性回归&lt;/h2&gt;&#xA;&lt;p&gt;线性回归是用来找到两个变量之间关系的方法，它试图找到一个“最佳线路”（即拟合），表示当自变量变化的时候，因变量如何变化，线性回归的公式为如下：&#xA;$$&#xA;y = a \cdot x + b&#xA;$$&#xA;其变量的解释为：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;y：因变量，也是输出或者预测值。&lt;/li&gt;&#xA;&lt;li&gt;x：自变量，也是输入。&lt;/li&gt;&#xA;&lt;li&gt;a：为斜率，表示x自变量每增加1，y会增加或者减少多少。&lt;/li&gt;&#xA;&lt;li&gt;b：为截距，表示当x=0的时候，y的值。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;线性回归可以简单理解为找到一条最能反应数据点总体趋势的数学表达式（比如直线），比如在散点图中，画一条尽可能靠近所有点的直线，而在这条线中，截距b表示直线在y轴上的起点，在当x增加时，直线沿着斜率a的方向延长，延长后的值即为y值。&lt;/p&gt;&#xA;&lt;p&gt;举一个生活中的例子，通过距离长短（自变量，即x）预测时间（因变量, 即y）的时长：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;送货距离(km)&lt;/th&gt;&#xA;          &lt;th&gt;送货时间(min)&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;20&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;25&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;35&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;通过上述的数据，可以知道当送货距离每增加1公里的时候，送货时间增加5分钟，所以在送货距离为2的时候，公式为：&#xA;$$&#xA;15 = a \cdot 2 + b&#xA;$$&#xA;计算斜率a有很多方法，具体取决于解决的问题背景、数据特性等，这里采用简单的“&lt;strong&gt;平均变化法&lt;/strong&gt;”来计算斜率a，“&lt;strong&gt;平均变化法&lt;/strong&gt;”的思路就是选择数据中任意的两个点来估算斜率a，公式为：&#xA;$$&#xA;a = \frac{\Delta y}{\Delta x_1}&#xA;$$&#xA;举例的数据中，取抽取第一个点和最后一个点来计算斜率a为（不一定要用最后一个点和第一个点，取具备代表的两个点即可）：&#xA;$$&#xA;(35 - 10) / (5 - 1)  = 6.25&#xA;$$&#xA;这个时候将斜率带入线性回归的公式中：&#xA;$$&#xA;15 = 6.25 \cdot 2 + b&#xA;$$&#xA;然后通过公式计算出截率b为2.5，因此公式为：&#xA;$$&#xA;b = y - a \cdot x \newline&#xA;b = 15 - 6.25 \cdot 2  = 2.5&#xA;$$&#xA;最后我们就得到了一个预测模型，当我们输入x的时候，就会预测出y的值：&#xA;$$&#xA;y = 6.25 \cdot x + 2.5&#xA;$$&#xA;通过这个模型可以进行计算当距离达到了3KM的时候，预测出来的送货时间为21.25分钟：&#xA;$$&#xA;6.25 \cdot 3 + 2.5  = 21.25&#xA;$$&#xA;需要注意的是模型并不会精准的预测出在上述数据表中对应的值，因为拟合的直线是代表的整体的趋势，在实际的数据中拟合的直线也不会穿过每个数据点，所以更多的时候是考虑了所有数据点的情况去拟合一条合适的直线，所以预测的是有偏差的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>变量的相关性计算</title>
      <link>https://0b44ccdd.vec6.com/blog/variablecorrelation/</link>
      <pubDate>Sat, 11 Jan 2025 21:53:34 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/variablecorrelation/</guid>
      <description>&lt;p&gt;相关性是衡量变量之间的相互依存的程度，也就是通过观察一个变量（自变量，解释变量）在变化的时候，如何影响到另一个变量（因变量，响应变量）的变化。&lt;/p&gt;&#xA;&lt;p&gt;相关性并不代表因果关系，即使两个变量之间有很强的相关性，也不意味着一个变量的变化导致了另一个变量的变化，比如冰淇淋销售量和溺水事件之间可能存在正相关关系，但并不意味着吃冰淇淋会导致溺水，实际原因可能是夏天天气炎热，导致人们都更多地购买冰淇淋并去游泳。&lt;/p&gt;&#xA;&lt;h2 id=&#34;pearson系数&#34;&gt;Pearson系数&lt;/h2&gt;&#xA;&lt;p&gt;Pearson（皮尔逊系数）相关系数通常用字母“r”标识，用于衡量两个&lt;strong&gt;连续性&lt;/strong&gt;变量之间的线性关系强度和方向，也就是说它告诉我们两个变量是如何在一起变化的。&lt;/p&gt;&#xA;&lt;p&gt;皮尔逊相关系数会返回一个取值范围在-1到+1之间的一个值：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;正相关：如果越靠近+1，则代表两个变量之间呈现正相关，即随着自变量的提高，因变量也随着提高。&lt;/li&gt;&#xA;&lt;li&gt;负相关：如果越靠近-1，则代表两个变量之间呈现负相关，即随着自变量的提高，因变量随着降低。&lt;/li&gt;&#xA;&lt;li&gt;无相关：实际上越靠近0，说明两个变量之间没有线性关系。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;进一步根据r值进行解读：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;0.7 ≤ |r| ≤ 1：强相关&lt;/li&gt;&#xA;&lt;li&gt;0.3 ≤ |r| &amp;lt; 0.7：中等相关&lt;/li&gt;&#xA;&lt;li&gt;0 ≤ |r| &amp;lt; 0.3：弱相关&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;chi-square-test&#34;&gt;Chi-Square Test&lt;/h2&gt;&#xA;&lt;p&gt;Chi-Square Test（卡方检验）是一种统计方法，用于判断两个分类变量（指能够被分为不同类别或组别的变量）之间是否存在相关性，原理是通过比对实际数据和理论数据（基于变量之间独立建立的）差异，看是否存在相关。&lt;/p&gt;&#xA;&lt;p&gt;卡方检验能根据如下男女生喜欢巧克力的观测数据，解决两个问题：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;下面的数据中，男生喜欢巧克力的概率为60%，女生为80%，那么60%与80%的差异是偶然的吗？&lt;/li&gt;&#xA;&lt;li&gt;差异大到是不是可以认为“性别”影响了概率？&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;编号&lt;/th&gt;&#xA;          &lt;th&gt;性别&lt;/th&gt;&#xA;          &lt;th&gt;是否喜欢巧克力&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;不喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;不喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;不喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;男&lt;/td&gt;&#xA;          &lt;td&gt;不喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;16&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;17&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;18&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;19&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;不喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;20&lt;/td&gt;&#xA;          &lt;td&gt;女&lt;/td&gt;&#xA;          &lt;td&gt;不喜欢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;要了解卡方检验，就需要了解其计算原理，卡方检验计算阶段为如下的5个阶段。&lt;/p&gt;</description>
    </item>
    <item>
      <title>标准差</title>
      <link>https://0b44ccdd.vec6.com/blog/standarddeviation/</link>
      <pubDate>Sat, 04 Jan 2025 11:20:35 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/standarddeviation/</guid>
      <description>&lt;p&gt;标准差（σ）是用来衡量一组数据“分散程度”的指标，标准差越大，说明数据之间的差距越大，也就是离平均值越大。标准差越小，说明数据相对比较“集中”。&lt;/p&gt;&#xA;&lt;p&gt;标准差也可以看见数据内部是否比较稳定不分散，典型的问题是人均收入，通过标准差可以查看数据内部是否有较大分散导致的不合理（被平均）。&lt;/p&gt;&#xA;&lt;p&gt;比如给定5个人收入数据，如果只是看平均值，那么每个人的人均收入在10400元，对于A、B、C是被平均的。&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;人员&lt;/th&gt;&#xA;          &lt;th&gt;薪酬&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;A&lt;/td&gt;&#xA;          &lt;td&gt;2000&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;B&lt;/td&gt;&#xA;          &lt;td&gt;3000&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;C&lt;/td&gt;&#xA;          &lt;td&gt;7000&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;D&lt;/td&gt;&#xA;          &lt;td&gt;15000&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;E&lt;/td&gt;&#xA;          &lt;td&gt;25000&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;计算步骤&#34;&gt;计算步骤&lt;/h2&gt;&#xA;&lt;p&gt;通过标准差的计算，可以通过均数和标准差两个值来观察数据是否合理，标准差步骤按照如下阶段分段进行。&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-计算数据组的平均值μ&#34;&gt;1. 计算数据组的平均值（μ）&lt;/h3&gt;&#xA;&lt;p&gt;为什么要算平均值，平均值就像“整体水平”的代表，用它来衡量每个人的收入是比平均低还是比平均高，所以这个步骤计算出来，为后续衡量差距做准备。&#xA;$$&#xA;\mu = (2000 + 3000 + 7000 + 15000 + 25000) / 5 = 10,400&#xA;$$&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-计算数值与平均数差值&#34;&gt;2. 计算数值与平均数差值&lt;/h3&gt;&#xA;&lt;p&gt;计算数值与平均数差值这个步骤的目的是计算到底哪些是低于还是高于平均值的，正数是高于，负数是低于。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;2000 - 10400  = -8,400 \newline&#xA;3000 - 10400  = -7,400 \newline&#xA;7000 - 10400  = -3,400 \newline&#xA;15000 - 10400  = 4,600 \newline&#xA;25000 - 10400  = 14,600&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>归一化（Normalization）和标准化（standardization）</title>
      <link>https://0b44ccdd.vec6.com/blog/normalizationandstandardization/</link>
      <pubDate>Sat, 04 Jan 2025 10:53:34 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/normalizationandstandardization/</guid>
      <description>&lt;h2 id=&#34;normalization和standardization的区别&#34;&gt;Normalization和Standardization的区别&lt;/h2&gt;&#xA;&lt;p&gt;两者是指通常将数据按照一定的规则进行缩放或变换，使她们满足一定数值的范围或者分布的过程。&lt;/p&gt;&#xA;&lt;p&gt;normalization，中文也被称为归一化，主要指的是把数据压缩到某个固定的区间（常见[0,1])，比如 Simple Feature Scaling、 Min-Max，虽然，Simple Feature Scaling返回的区间并不是一定是[0,1]的区间，但是该算法是用了最大值来固定界限对数据进行缩放，场景会用于常见对于数值严格在某些区间内的算法或者场景，比如神经网络的激活函数。&lt;/p&gt;&#xA;&lt;p&gt;standardization，中文也被称为标准化，主要指的是让数据均值为0，标准差为1，进而可以用不同的量纲的变量通过变换后，能够统一的量纲进行比较。比如Z-Score。&lt;/p&gt;&#xA;&lt;h2 id=&#34;为什么需要normalization和standardization&#34;&gt;为什么需要Normalization和Standardization&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;统一特征量纲：如果有些特征数字太大，而有些特征数字太小，会导致模型训练或者度量距离产生更大的影响，导致模型更“偏爱”这些特征。&lt;/li&gt;&#xA;&lt;li&gt;加快模型训练收敛速度：神经网络、梯度下降相关算法，如果输入特征分布范围相差太大，会导致训练速度变慢。&lt;/li&gt;&#xA;&lt;li&gt;提高算法性能：像KNN等基于距离度量的算法或基于距离矩阵的算法（聚类），如特征不在相似的数值区间，结果就会受到极大特征值的干扰，归一化后，各特征的贡献更均衡。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;基础的三种算法&#34;&gt;基础的三种算法&lt;/h2&gt;&#xA;&lt;p&gt;常见的、最基础的三种算法为Simple Feature Scaling、Min-Max以及Z-Score，但在数据科学中选择“最合适”是最重要的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;simple-feature-scaling&#34;&gt;Simple Feature Scaling&lt;/h3&gt;&#xA;&lt;p&gt;Simple Feature Scaling的公式为如下：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;x^i = \frac{x}{ max(x)}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;其中x’为标准化后的值，x为当前数值，max(x)为数据集中最大值。&lt;/p&gt;&#xA;&lt;h4 id=&#34;作用&#34;&gt;作用&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;让数值统一范围&lt;/strong&gt;：不缩小范围，特征差距可能有几倍甚至几十百倍，会导致某些算法受到数值较大的影响以及训练速度受到影响。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;计算简单&lt;/strong&gt;：只需要知道最大值，就能快速完成缩放。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;对等Min-Max&lt;/strong&gt;：在数据最小值等于0或者近似0的时候，与Min-Max的结果几乎一致。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;原理&#34;&gt;原理&lt;/h4&gt;&#xA;&lt;p&gt;比如有一堆数[10,20,30,40]，它们最大值是40，如果用每个值去除以最大值得到的为：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;10 / 40  = 0.25 \newline&#xA;20 / 40  = 0.5 \newline&#xA;30 /40  = 0.75 \newline&#xA;40 / 40  = 1&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;那这个时候就可以把原本10-40的区间的值，缩放到了0.25-1区间的值。&lt;/p&gt;&#xA;&lt;h4 id=&#34;缺点1-受到的极值影响特别大&#34;&gt;缺点1-受到的极值影响特别大&lt;/h4&gt;&#xA;&lt;p&gt;也就是数据里面最“极端”的高点( max(x) ），如果如果这个值特别大就会让其它的值缩放得特别小，导致信息变得不明显。&lt;/p&gt;&#xA;&lt;p&gt;比如数据集[10,20,30, 40000]，然后计算缩放的区间范围为：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;10 / 40000  = 0.00025 \newline&#xA;20 / 40000  = 0.0005 \newline&#xA;30 / 40000  = 0.00075 \newline&#xA;40000 / 40000  = 1&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>理解条件概率</title>
      <link>https://0b44ccdd.vec6.com/blog/learning-conditional-probability/</link>
      <pubDate>Tue, 03 Nov 2020 09:03:53 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/learning-conditional-probability/</guid>
      <description>&lt;h2 id=&#34;样本空间ω&#34;&gt;样本空间（Ω）&lt;/h2&gt;&#xA;&lt;p&gt;样本空间通常指实验或随机所有可能的集合，我们常在说一个概率的时候，实际上是默认忽略掉了样本空间，比如说事件A的概率，实际上指样本空间中，事件A的数量与样本空间的占比。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://0b44ccdd.vec6.com/post/images/2020/11/1604372452-AC4572F1-2252-4346-BB20-0D4FFDF6358B.jpeg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;比如丢硬币，硬币只有正面和反面，那么硬币的样本空间则为 正面，反面正面，反面，这个时候常说的正面的概率为二分之一，实际指的是正面事件的数量与样本空间的占比，也就是1/2。 再比如说丢骰子，一个骰子有6种可能，分别对应1-6不同的数值，那么丢骰子的样本空间则为1，2，3，4，5，61，2，3，4，5，6，这个时候丢到5个事件概率则为数字5在样本空间出现的次数与样本空间总数的占比。&lt;/p&gt;&#xA;&lt;h2 id=&#34;独立事件&#34;&gt;独立事件&lt;/h2&gt;&#xA;&lt;p&gt;独立事件是指不受过去已发生的事件而影响的事件，典型的例子就是抛硬币，不管你抛多少次硬币始终正面或反面的概率为0.5，而该硬币的样本空间如下：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://0b44ccdd.vec6.com/post/images//2020/11/1604371899-47382406-BF21-44F0-9AE6-589942D9B6A6.jpeg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;独立事件的概率计算公式为如下：&#xA;$$&#xA;事件发生的概率(P)=事件在样本空间中的数量/样本空间的事件总数&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;比如用抛硬币的例子，计算正面的概率则为：&#xA;$$&#xA;P(正面) = \frac{1}{2}&#xA;$$&#xA;而除了单个独立事件，有些时候也会求多个独立事件的概率，而多个独立事件的概率则是每个独立事件发生的概率的积。 比如掷3次骰子都为6的概率是多少？需要注意因为掷骰子是一个独立事件，即每次掷的骰子样本空间都一样，并且没有因为第一次掷骰子的结果会影响到下一次。 骰子的样本空间为下，从中能够得到单次掷骰子为6的概率为1/6：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://0b44ccdd.vec6.com/post/images//2020/11/1604373121-30D9A77B-E1A3-4F16-95E6-EF8235C3F866.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;而这个时候只需要将三次掷骰子的概率相乘就得到了三次都为6的概率：&#xA;$$&#xA;P(3次6) = \frac{1}{6} \times \frac{1}{6} \times \frac{1}{6} = \frac{1}{216}&#xA;$$&lt;/p&gt;&#xA;&lt;h2 id=&#34;相关事件&#34;&gt;相关事件&lt;/h2&gt;&#xA;&lt;p&gt;相关事件和独立事件是相对的，相关事件的发生概率会受到过去已发生事件的影响，每个事件都和上一个事件有关联，这些事件便是相关的。 比如一个布袋中有5个球，其中包含2个蓝球，三个红球，布袋(样本空间)则为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://0b44ccdd.vec6.com/post/images/2020/11/1604371925-C998AEB6-33C2-48A6-AD8E-7036323E6712.jpeg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;这个时候如果随机拿一颗蓝球的概率是多少？概率为2/5。 但是此时求第二次拿到蓝球的概率是多少？这个时候就会有两种情况发生：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;第一次拿到红球，这个时候整个样本空间少了一个红球，所以第二次拿到蓝球的概率为2/4&lt;/li&gt;&#xA;&lt;li&gt;第二次拿到蓝球，这个时候整个样本空间少了一个篮球，所以第二次拿到蓝球的概率为1/4&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;用图表示则为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://0b44ccdd.vec6.com/post/images//2020/11/1604371934-961D07E9-2DF6-4261-A652-87E57CFB1409.jpeg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;所以此时，如果算第一次拿到红球后，第二次拿到蓝球的概率则为：&#xA;$$&#xA;P(第二次拿篮球) = \frac{3}{5} \times \frac{2}{4} = 0.3&#xA;$$&#xA;如果算第一次拿到蓝球后，第二次拿到红球的概率则为：&#xA;$$&#xA;P(第二次拿篮球) = \frac{2}{5} \times \frac{1}{4} = 0.1&#xA;$$&lt;/p&gt;&#xA;&lt;h2 id=&#34;条件概率&#34;&gt;条件概率&lt;/h2&gt;&#xA;&lt;p&gt;条件概率是研究相关事件的，指的是当B事件发生后，A事件发生的概率，用&amp;quot;｜“来表示&amp;quot;以下发生的条件下”，表示为公式：&#xA;$$&#xA;P(A|B)&#xA;$$&#xA;比如上面的例子，第二个蓝球的概率是多少，这个问题就是条件概率，因为第二次抽中蓝球的概率是基于第一次拿了一颗球过后发生的事件。 这个时候可以将第一次抽中红球记作事件A，第二次抽蓝球为事件B，因为第二次抽球是在事件A发生的情况下而发生的，所以记作 P(B∣A)&lt;em&gt;P&lt;/em&gt;(&lt;em&gt;B&lt;/em&gt;∣&lt;em&gt;A&lt;/em&gt;) ，表示在A发生后，B发生的概率。 而这个概率可以根据下图来得到，即2/4：&lt;/p&gt;</description>
    </item>
    <item>
      <title>理解连续数据和离散数据</title>
      <link>https://0b44ccdd.vec6.com/blog/continuous-data-and-discrete-data/</link>
      <pubDate>Tue, 27 Oct 2020 07:35:35 +0000</pubDate>
      <guid>https://0b44ccdd.vec6.com/blog/continuous-data-and-discrete-data/</guid>
      <description>&lt;p&gt;统计学中，将一种类型的数据总称为变量，而变量的数据称为观测，而变量的具体取值为观测值，比如下面的数据中，age和name都是变量，而18和’大红’都具体的取值被称为观测值。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;age,name&#xA;18,’大红’&#xA;21,’小花’ &#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;同理，在统计学中，离散数据也被称为离散变量，连续数据也被称为连续变量，而如何区分两种变量的区别？ 连续变量可以理解为取值范围在理论上是连续不断的，而离散变量则可以理解为取值范围是间断不连续的，他们之间的区别并无数量之分，都是无穷个。 比如家庭数量人口只有1、2、3、4个人口，不可能为1.2、1.8、2.4这样来表示人口，所以家庭人口是离散变量。 而年龄取值上通常为了方便而说是18岁、17岁、30岁，但是如果按照实际取值，则可以取为18.32、17.55、30.67岁，17.55岁则表示年龄为17岁6个月18天，而且出生的时间还可以精确到小时、分、秒等单位，所以年龄为连续变量。&lt;/p&gt;&#xA;&lt;h3 id=&#34;参考资料&#34;&gt;参考资料&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/101155810&#34;&gt;关于连续和离散的理解&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/152635216&#34;&gt;定量和定性变量、连续和离散变量，到底怎么分？&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1eW411E7eu?p=23&#34;&gt;图解概率笔记：葉丙成概率公开课&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
