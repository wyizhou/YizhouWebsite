<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DataScience on Lucas Wu</title>
    <link>https://vec6.com/tags/datascience/</link>
    <description>Recent content in DataScience on Lucas Wu</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 27 Jan 2025 10:53:34 +0000</lastBuildDate>
    <atom:link href="https://vec6.com/tags/datascience/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gird Search</title>
      <link>https://vec6.com/blog/gridsearch/</link>
      <pubDate>Mon, 27 Jan 2025 10:53:34 +0000</pubDate>
      <guid>https://vec6.com/blog/gridsearch/</guid>
      <description>&lt;p&gt;在了解网格搜索之前，需要明白参数(Parameter)和超参数(HyperParameter)的区别，Parameter是指算法通过学习数据自动计算出来得到的值，比如线性回归中的\beta系数，而HyperParameter是指人为设定、控制算法学习或者训练过程的变量，它不会被直接学习，但是会影响学习的行为和最终的学习效果。&lt;/p&gt;&#xA;&lt;p&gt;比如开车超参数就是指你可以控制踩油门的力度、驾驶模式（运动、节能或者正常），而参数则是发动机的扭矩输出、油耗和摩擦力等，是汽车本身决定的，无法直接调整的。&lt;/p&gt;&#xA;&lt;p&gt;而Grid Search的作用尝试遍历不同的超参数组合方式，生成不同的模型，并计算模型的评估值，最终找到最优超参数组合，让模型表现更好。&lt;/p&gt;&#xA;&lt;h2 id=&#34;举例&#34;&gt;举例&lt;/h2&gt;&#xA;&lt;p&gt;Sklearn提供了Grid Search搜索的函数，可以通过如下方式导入：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GridSearchCV&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;下面就通过演示通过Ridge回归预测房价的问题，然后通过Grid Search进行找到最佳超参数，数据为如下表格，价格为万元：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;area(X1)&lt;/th&gt;&#xA;          &lt;th&gt;room(X2)&lt;/th&gt;&#xA;          &lt;th&gt;price（Y）&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;77&lt;/td&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;220&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;89&lt;/td&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;230&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;99&lt;/td&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;260&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;110&lt;/td&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;320&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;125&lt;/td&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;410&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;代码为：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GridSearchCV&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.linear_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Ridge&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;area&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;77&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;89&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;125&lt;/span&gt;], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;room&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;price&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;220&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;230&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;260&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;320&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;410&lt;/span&gt;]})&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;parameters_grid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.001&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;]}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ridge_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Ridge()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GridSearchCV(ridge_model, parameters_grid, cv&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;#评估指标默认使用R^2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(data[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;area&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;room&amp;#39;&lt;/span&gt;]], data[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;]]) &lt;span style=&#34;color:#75715e&#34;&gt;# 训练模型&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;best_estimator_ &lt;span style=&#34;color:#75715e&#34;&gt;#查看最佳的超参数，输出为alpha:100&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cv_results_ &lt;span style=&#34;color:#75715e&#34;&gt;#查看对应超参数以及对应的评估指标的值&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; param, mean_val &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(grid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cv_results_[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;params&amp;#39;&lt;/span&gt;], grid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cv_results_[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mean_test_score&amp;#39;&lt;/span&gt;]):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  print(param, mean_val)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;输出最佳的R^2值为-3.85，alpha为100&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&amp;#39;alpha&amp;#39;: 0.001} -5.673602295795709&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&amp;#39;alpha&amp;#39;: 0.01} -5.706292846025019&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&amp;#39;alpha&amp;#39;: 0.1} -5.9416525607274835&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&amp;#39;alpha&amp;#39;: 1} -6.2458204928344285&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&amp;#39;alpha&amp;#39;: 10} -4.4122468477234005&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&amp;#39;alpha&amp;#39;: 100} -3.8580109543701897&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Ridge回归</title>
      <link>https://vec6.com/blog/ridgereg/</link>
      <pubDate>Mon, 27 Jan 2025 10:53:34 +0000</pubDate>
      <guid>https://vec6.com/blog/ridgereg/</guid>
      <description>&lt;p&gt;Ridge回归简单的来讲，是用于解决算法解释数据的时候，数据中特征高度相关（多重共线性）导致的过拟合，即完美拟合训练数据，无法很好的预测新数据，这是因为把训练数据的噪音也学习了，Ridge原理是在回归的基础上增加了一个“惩罚项”，让模型不要依赖某些变量。&lt;/p&gt;&#xA;&lt;p&gt;比如预测房价的例子，现在有面积、房间数量、地理位置和朝向等特征，其中面积和房间数量就是高度相关的变量，因为面积大了，房间数量也一定会增加，所以当把面积（X1）和房间数量（X2）输入给算法，模型则有可能出现X1系数很大，X2系数很小，或者反过来的情况，导致模型解释能力变差，回归系数也无法稳定，主要是因为无法知道其中X1和X2对于模型的贡献情况，就好比两个员工一起做事，交付了后，老板无法评估这个成果每个人付贡献谁更大。&lt;/p&gt;&#xA;&lt;p&gt;Ridge回归目标函数为，这个回归函数为要解决的问题的数学公式（理论解）：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\min_{\beta} \sum_{i=1}^{n} (y_i - \hat{y}i)^2 + \alpha \sum{j=1}^{p} \beta_j^2&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;而将上述目标函数推导为矩阵公式后，得到如下矩阵公式（解析解），为什么要推导？这是因为Ridge回归的目标函数是一个数学表达式，是一个理论解，而解析解（即推导后的公式）则是一个计算机可以直接求解的矩阵公式。&lt;/p&gt;&#xA;&lt;p&gt;这个公式实际上对比最小二乘法解系数的公式中添加了正则项，也就是通过增大a达到增大分母的作用，让整体系数变小，同时越大的系数，就会惩罚越大。&#xA;$$&#xA;\beta = (X^T X + \alpha I)^{-1} X^T Y&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pandas之Groupby</title>
      <link>https://vec6.com/blog/groupbyofpandas/</link>
      <pubDate>Sun, 12 Jan 2025 21:53:34 +0000</pubDate>
      <guid>https://vec6.com/blog/groupbyofpandas/</guid>
      <description>&lt;p&gt;Pandas中的groupby用于将某些类别进行数据分组，然后对每个组分别执行基础的统计以及复杂的聚合、转换和过滤等操作。&lt;/p&gt;&#xA;&lt;p&gt;常见的用途有：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;汇总统计&lt;/strong&gt;：快速计算每组的总和、平均值等。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据对比&lt;/strong&gt;：比较不同组之间的表现差异。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;异常值检测&lt;/strong&gt;：发现某些组内的异常数据点。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征工程&lt;/strong&gt;：在机器学习中，利用组间统计量创建新特征（如组内平均、组内计数等）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据清洗&lt;/strong&gt;：根据组的统计信息进行数据修正或填补缺失值。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;基础统计&#34;&gt;基础统计&lt;/h2&gt;&#xA;&lt;p&gt;比如数据电商的水果销售数据，然后进行多种不同分组的统计，第一种按照水果类别进行统计总销售额：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Date&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-01&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-01&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-02&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-02&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-03&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-03&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-04&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-04&amp;#39;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;水果&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;蔬菜&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;水果&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;饮料&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;蔬菜&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;饮料&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;水果&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;蔬菜&amp;#39;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sell&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;220&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;180&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;160&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;190&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;210&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;170&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(data)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grouped &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;)[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sell&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; print(grouped)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Category&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;水果    &lt;span style=&#34;color:#ae81ff&#34;&gt;630&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;蔬菜    &lt;span style=&#34;color:#ae81ff&#34;&gt;480&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;饮料    &lt;span style=&#34;color:#ae81ff&#34;&gt;370&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Name: Sell, dtype: int64&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;第二种为按照两个数据进行分组，这里按照日期和水果类别统计：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grouped &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Date&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sell&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; print(grouped)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Date        Category&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2023&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;01&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;01&lt;/span&gt;  水果          &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            蔬菜          &lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2023&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;01&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt;  水果          &lt;span style=&#34;color:#ae81ff&#34;&gt;220&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            饮料          &lt;span style=&#34;color:#ae81ff&#34;&gt;180&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2023&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;01&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;03&lt;/span&gt;  蔬菜          &lt;span style=&#34;color:#ae81ff&#34;&gt;160&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            饮料          &lt;span style=&#34;color:#ae81ff&#34;&gt;190&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2023&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;01&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;04&lt;/span&gt;  水果          &lt;span style=&#34;color:#ae81ff&#34;&gt;210&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            蔬菜          &lt;span style=&#34;color:#ae81ff&#34;&gt;170&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Name: Sell, dtype: int64&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;复杂统计&#34;&gt;复杂统计&lt;/h2&gt;&#xA;&lt;p&gt;常见的复杂统计主要几种在聚合(aggregation)、转换(transformation)和过滤(Filtering)。&lt;/p&gt;</description>
    </item>
    <item>
      <title>长格式和宽格式</title>
      <link>https://vec6.com/blog/longwidedata/</link>
      <pubDate>Sat, 11 Jan 2025 23:53:34 +0000</pubDate>
      <guid>https://vec6.com/blog/longwidedata/</guid>
      <description>&lt;p&gt;长格式和宽格式根据应用场景（如制图、统计和模型输入）的需求不同，转换为合适的数据格式，比如某些模型只能接受宽格式，长格式适合分组统计等。&lt;/p&gt;&#xA;&lt;p&gt;长格式指&lt;strong&gt;每一行记录一个实体的一个属性或者测量值&lt;/strong&gt;，比如下面的数据，每行记录了张三或者李四的一个科目的结果，即每个人占多行，不同的科目分数在不同行。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  姓名,科目,成绩&#xA;1,张三,数学,95&#xA;2,张三,语文,89&#xA;3,李四,数学,80&#xA;4,李四,数学,85&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;宽格式指每一行记录一个实体的所有属性或测量值，比如下面的数据，每行记录了张三或者李四的所有科目的结果，即每个人占一行，所有的科目分数在不同列。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  姓名,数学,语文&#xA;1,张三,95,89&#xA;2,李四,80,85&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;pandas中的长格式转宽格式&#34;&gt;Pandas中的长格式转宽格式&lt;/h2&gt;&#xA;&lt;p&gt;Pandas中使用pivot和pivot_table将长格式转换为宽格式，他们的区别在于pivot需要索引和列的组合为唯一，比如张三不能出现两次数学，否则就会报错，但是如果数据没有重复组合，pivot的效率要比pivot_table高。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;张三&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;张三&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;张三&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;李四&amp;#34;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catetory&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;数学&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;语文&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;数学&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;数学&amp;#34;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;95&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;89&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;85&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(data)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pivot(index&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;, columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;catetory&amp;#39;&lt;/span&gt;, values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;: Index contains duplicate entries, cannot reshape&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;pivot_table和pivot的主要区别为对于重复值的处理，pivot_table在遇见重复的index+column的重复数据的时候，通过指定聚合函数（如mean、sum、max等），计算出一个单一的值来填充重塑后的表格。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;张三&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;张三&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;张三&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;李四&amp;#34;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catetory&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;数学&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;语文&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;数学&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;数学&amp;#34;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;95&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;89&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;85&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(data)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pivot_table(index&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;, columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;catetory&amp;#39;&lt;/span&gt;, values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;, aggfunc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 下面的92.5为 (95 + 90) / 2 &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;catetory    数学    语文&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;name&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;张三        &lt;span style=&#34;color:#ae81ff&#34;&gt;92.5&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;89.0&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;李四        &lt;span style=&#34;color:#ae81ff&#34;&gt;85.0&lt;/span&gt;   NaN &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;pandas中的宽格式转长格式&#34;&gt;Pandas中的宽格式转长格式&lt;/h2&gt;&#xA;&lt;p&gt;宽格式转换长格式，实际上就是将多个列融化为两列（一个变量标识符）和一个值列，在Pandas中常用melt和wide_to_long方法。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pandas处理无效值和缺失值</title>
      <link>https://vec6.com/blog/handlingmissinginvalidvalues/</link>
      <pubDate>Sat, 04 Jan 2025 11:20:35 +0000</pubDate>
      <guid>https://vec6.com/blog/handlingmissinginvalidvalues/</guid>
      <description>&lt;p&gt;数据集中的数据会经常遇见缺失值或者无效值，在Pandas中缺失值可能会被直接识别为NaN，而无效值则是由数据自带的，比如乱码、无意义的字符等。&lt;/p&gt;&#xA;&lt;h2 id=&#34;处理缺失值的常见方式&#34;&gt;处理缺失值的常见方式&lt;/h2&gt;&#xA;&lt;p&gt;如何处理缺失值和无效值就显得特别重要，一般可以采用如下的一些方式方法。&lt;/p&gt;&#xA;&lt;h3 id=&#34;丢弃缺失值-drop-the-missing-values&#34;&gt;丢弃缺失值 Drop the missing values&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Drop the variable：如果存在变量（列）大量存在缺失值的问题，可以考虑删除整个变量&lt;/li&gt;&#xA;&lt;li&gt;Drop the data entry：如果变量没有存在大量缺失，只有少数几条数据，可以考虑删除当前样本（即行数据）。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;替换缺失值-replace-the-missing-values&#34;&gt;替换缺失值 Replace the missing values&lt;/h3&gt;&#xA;&lt;p&gt;替换缺失值的好处在于不会浪费数据。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Replace it with an average（of Similar datapoints）：通过计算相似数据点的整个数据的平均值去填充，&lt;strong&gt;但该方法只能适用于连续数值类型的。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Replace it by frequency：对于不是数值类型的缺失值，可以通过相似数据的最高的频率数据去填充。&lt;/li&gt;&#xA;&lt;li&gt;Replace it based on other functions：这个需要基于分析人员根据行的其它数据知道这些缺失值可能会是什么？这个通常需要分析人员对业务非常熟悉。&lt;/li&gt;&#xA;&lt;li&gt;Leave it as missing data：保留缺失观察数据，即使在统计的时候，缺失的观察数据可能会缺失某些功能。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;pandas去除无效数据&#34;&gt;Pandas去除无效数据&lt;/h2&gt;&#xA;&lt;p&gt;Pandas中去除无效数据的方式可以通过dropna方法，默认情况下是整行都是na值则会删除，但如果只想根据某一列或者多列来删除，比如根据price一列是否有数据来检查是否删除当前行。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dropna(subset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 如果当前行price为NaN，则删除当前一行&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# axis = 0 代表price为NaN，则删除当前一整行&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Dropna中的inplace参数可以设定是否修改原数据帧（DataFrame），默认为False，即返回一个修改后的副本。&lt;/p&gt;&#xA;&lt;h2 id=&#34;pandas-填充无效值&#34;&gt;Pandas 填充无效值&lt;/h2&gt;&#xA;&lt;p&gt;Pandas中填充无效值可以使用DataFrame.replace方法，该方法接受两个值，一个为查找值，另一个为替换值。&lt;/p&gt;&#xA;&lt;p&gt;比如下面可以将无效值”?”替换为np.nan：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;?&amp;#34;&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nan)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>归一化（Normalization）和标准化（standardization）</title>
      <link>https://vec6.com/blog/normalizationandstandardization/</link>
      <pubDate>Sat, 04 Jan 2025 10:53:34 +0000</pubDate>
      <guid>https://vec6.com/blog/normalizationandstandardization/</guid>
      <description>&lt;h2 id=&#34;normalization和standardization的区别&#34;&gt;Normalization和Standardization的区别&lt;/h2&gt;&#xA;&lt;p&gt;两者是指通常将数据按照一定的规则进行缩放或变换，使她们满足一定数值的范围或者分布的过程。&lt;/p&gt;&#xA;&lt;p&gt;normalization，中文也被称为归一化，主要指的是把数据压缩到某个固定的区间（常见[0,1])，比如 Simple Feature Scaling、 Min-Max，虽然，Simple Feature Scaling返回的区间并不是一定是[0,1]的区间，但是该算法是用了最大值来固定界限对数据进行缩放，场景会用于常见对于数值严格在某些区间内的算法或者场景，比如神经网络的激活函数。&lt;/p&gt;&#xA;&lt;p&gt;standardization，中文也被称为标准化，主要指的是让数据均值为0，标准差为1，进而可以用不同的量纲的变量通过变换后，能够统一的量纲进行比较。比如Z-Score。&lt;/p&gt;&#xA;&lt;h2 id=&#34;为什么需要normalization和standardization&#34;&gt;为什么需要Normalization和Standardization&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;统一特征量纲：如果有些特征数字太大，而有些特征数字太小，会导致模型训练或者度量距离产生更大的影响，导致模型更“偏爱”这些特征。&lt;/li&gt;&#xA;&lt;li&gt;加快模型训练收敛速度：神经网络、梯度下降相关算法，如果输入特征分布范围相差太大，会导致训练速度变慢。&lt;/li&gt;&#xA;&lt;li&gt;提高算法性能：像KNN等基于距离度量的算法或基于距离矩阵的算法（聚类），如特征不在相似的数值区间，结果就会受到极大特征值的干扰，归一化后，各特征的贡献更均衡。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;基础的三种算法&#34;&gt;基础的三种算法&lt;/h2&gt;&#xA;&lt;p&gt;常见的、最基础的三种算法为Simple Feature Scaling、Min-Max以及Z-Score，但在数据科学中选择“最合适”是最重要的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;simple-feature-scaling&#34;&gt;Simple Feature Scaling&lt;/h3&gt;&#xA;&lt;p&gt;Simple Feature Scaling的公式为如下：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;x^i = \frac{x}{ max(x)}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;其中x’为标准化后的值，x为当前数值，max(x)为数据集中最大值。&lt;/p&gt;&#xA;&lt;h4 id=&#34;作用&#34;&gt;作用&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;让数值统一范围&lt;/strong&gt;：不缩小范围，特征差距可能有几倍甚至几十百倍，会导致某些算法受到数值较大的影响以及训练速度受到影响。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;计算简单&lt;/strong&gt;：只需要知道最大值，就能快速完成缩放。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;对等Min-Max&lt;/strong&gt;：在数据最小值等于0或者近似0的时候，与Min-Max的结果几乎一致。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;原理&#34;&gt;原理&lt;/h4&gt;&#xA;&lt;p&gt;比如有一堆数[10,20,30,40]，它们最大值是40，如果用每个值去除以最大值得到的为：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;10 / 40  = 0.25 \newline&#xA;20 / 40  = 0.5 \newline&#xA;30 /40  = 0.75 \newline&#xA;40 / 40  = 1&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;那这个时候就可以把原本10-40的区间的值，缩放到了0.25-1区间的值。&lt;/p&gt;&#xA;&lt;h4 id=&#34;缺点1-受到的极值影响特别大&#34;&gt;缺点1-受到的极值影响特别大&lt;/h4&gt;&#xA;&lt;p&gt;也就是数据里面最“极端”的高点( max(x) ），如果如果这个值特别大就会让其它的值缩放得特别小，导致信息变得不明显。&lt;/p&gt;&#xA;&lt;p&gt;比如数据集[10,20,30, 40000]，然后计算缩放的区间范围为：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;10 / 40000  = 0.00025 \newline&#xA;20 / 40000  = 0.0005 \newline&#xA;30 / 40000  = 0.00075 \newline&#xA;40000 / 40000  = 1&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>用于数据科学的SQL</title>
      <link>https://vec6.com/blog/sqlforanalysis/</link>
      <pubDate>Sun, 29 Dec 2024 21:36:34 +0000</pubDate>
      <guid>https://vec6.com/blog/sqlforanalysis/</guid>
      <description>&lt;p&gt;如今在数据科学中使用SQL查询数据是常见的操作，而大部分以表格（行和列）展示数据的数据格式，几乎都支持使用SQL进行查询，比如Dataframe、Excel。&lt;/p&gt;&#xA;&lt;p&gt;同时通过Jupyter Notebook提供的Magic Statements和IPython_SQL，能够让SQL和Python进行非常方便的结合。&lt;/p&gt;&#xA;&lt;h2 id=&#34;sql&#34;&gt;SQL&lt;/h2&gt;&#xA;&lt;p&gt;SQL全称为Structured Query Language 结构化查询语言，是基于Edgar F. Codd在1970年提出的关系模型理论，该模型引入了表（表格形式的数据结构）来表示数据以及其关系的概念，所以可以理解为最初的定位是用于操作和查询关系型数据库。&lt;/p&gt;&#xA;&lt;p&gt;SQL 于 1986 年被 &lt;strong&gt;ANSI&lt;/strong&gt;（美国国家标准学会）采纳，并于 1987 年被 &lt;strong&gt;ISO&lt;/strong&gt;（国际标准化组织）采纳为关系型数据库的标准语言，所以目前制定标准的主要由ANSI和ISO两个组织。&lt;/p&gt;&#xA;&lt;p&gt;SQL是一个标准，也是一个总称，SQL的定义了两个核心部分，第一个是标准的查询语句和标准的数据类型。&lt;/p&gt;&#xA;&lt;p&gt;除了上述的这些子集，还有一些扩展的高级功能的子集，比如对非结构化数据的支持，使得SQL能够支持NoSQL、NewSQL以及大数据系统应用。&lt;/p&gt;&#xA;&lt;h2 id=&#34;查询语句&#34;&gt;查询语句&lt;/h2&gt;&#xA;&lt;p&gt;查询语句主要是围绕着DDL、DML、DCL以及TCL几个核心的子集开展，而常见的SELCT、CREATE都是属于这些子集，核心子集每个数据库（DBMS）都是支持的，但是每个数据库针对于SQL的扩展都会都会有一些差异，所以如果考虑到迁移性，应该尽可能使用标准的核心SQL语句。&lt;/p&gt;&#xA;&lt;h3 id=&#34;ddldata-definition-language&#34;&gt;DDL（Data Definition Language）&lt;/h3&gt;&#xA;&lt;p&gt;DDL（数据定义语言），理解为数据库的地基，负责定义和修改数据库结构和对象（如表、视图和索引等），典型的语句有CREATE、ALTER和DROP等。&lt;/p&gt;&#xA;&lt;h3 id=&#34;dmldata-manipulation-language&#34;&gt;DML（Data Manipulation Language）&lt;/h3&gt;&#xA;&lt;p&gt;DML（数据操作语言），作用于操作数据库的数据，对数据库中的数据进行查询、插入、更新和删除等操作，典型的语句有 Select查询数据、 Insert插入数据、Update更新修改数据、 Delete删除数据等。&lt;/p&gt;&#xA;&lt;p&gt;DQL（Data Query Language）是专门用于查询的子集，但是是由DML下的子集，所以DML包含查询。&lt;/p&gt;&#xA;&lt;h3 id=&#34;dcldata-control-language&#34;&gt;DCL（Data Control Language）&lt;/h3&gt;&#xA;&lt;p&gt;DCL（数据控制语言），作用于控制用户权限，管理数据库对象的访问权限，典型的语句有Grant和Revoke等。&lt;/p&gt;&#xA;&lt;h3 id=&#34;tcltransaction-control-language&#34;&gt;TCL（Transaction Control Language）&lt;/h3&gt;&#xA;&lt;p&gt;TCL（事物控制语言）是SQL子集，作用于管理事物，确保数据库操作的完整性，典型的语句有Commit、Rollback和Savepoint等。&lt;/p&gt;&#xA;&lt;h2 id=&#34;数据类型&#34;&gt;数据类型&lt;/h2&gt;&#xA;&lt;p&gt;SQL除了操作查询数据的语句部分，还有定义了数据的类型，比如数值、字符串、日期、布尔值等，大部分都是比较好理解的，但日期则可以根据需求不同采用不同的日期类型，SQL定义了DATE、TIME和TIMESTAMP类型，主要差异在于：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;DATE：DATE类型有8位数，分别为YYYYMMDD。&lt;/li&gt;&#xA;&lt;li&gt;TIME：TIME类型有6位数，分别为HHMMSS。&lt;/li&gt;&#xA;&lt;li&gt;TIMESTAMP：TIMESTAMP有20位数，分别为YYYYXXDDHHMMSSZZZZZZ，其中XX代表月份，ZZZZZZ代表微秒。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;一些记录&#34;&gt;一些记录&lt;/h2&gt;&#xA;&lt;p&gt;大部分SQL可以很容易查询到用法，但是部分查询在性能和实现则有一定的区别，同时也有存在多种解决方式。&lt;/p&gt;&#xA;&lt;h3 id=&#34;primary-key&#34;&gt;Primary Key&lt;/h3&gt;&#xA;&lt;p&gt;数据库中主键有如下几个功能：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;唯一性：设置为主键的属性（数据库中被映射为了列），是唯一值，能够避免数据重复。&lt;/li&gt;&#xA;&lt;li&gt;一致性和完整性：主键确保表每一行都有一个唯一的标识符，这对于维护数据的一致性非常重要。&lt;/li&gt;&#xA;&lt;li&gt;关联性：在关系型数据库中，主键用于建立表与表之间的关系（通过外键引用）。&lt;/li&gt;&#xA;&lt;li&gt;加速：数据库在创建主键的时候，会为主键创建索引，使得通过主键查询会非常快。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;er模型和erd图&#34;&gt;ER模型和ERD图&lt;/h3&gt;&#xA;&lt;h4 id=&#34;entity-relationship-modeler模型&#34;&gt;Entity-Relationship Model（ER模型）&lt;/h4&gt;&#xA;&lt;p&gt;ER模型是用于设计实体关系模型的基础理论，关注实体（比如人）、实体的属性（比如人的头发、身高、体重、肤色等）以及相互之间的关联，ER模型的好处是可以针对字段进行规划关联、字段类型、主键以及多对一还是一对多等关系，同时设计完成后可以自动化生成sql进行自动建立数据库结构。&lt;/p&gt;&#xA;&lt;h4 id=&#34;entity-relationship-diagramerd图&#34;&gt;Entity Relationship Diagram（ERD图）&lt;/h4&gt;&#xA;&lt;p&gt;而ERD则是ER的一种图的实现，基于ER理论通过ERD进行设计实体关系图，会更加直观。&lt;/p&gt;&#xA;&lt;h4 id=&#34;优势&#34;&gt;优势&lt;/h4&gt;&#xA;&lt;p&gt;如果只是画图，很多都可以画，但是ER模型和ERD都是专用数据库设计的，所以有很多其它画图工具达不到的功能，比如自动根据ERD图生成SQL脚本。&lt;/p&gt;&#xA;&lt;h3 id=&#34;字符串查询和范围查询&#34;&gt;字符串查询和范围查询&lt;/h3&gt;&#xA;&lt;p&gt;SQL使用字符串进行查询，可以只给出部分关键词进行模糊查询，其中有%代表通配符（即匹配任何事情）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>数据科学方法论</title>
      <link>https://vec6.com/blog/data-science-methodology/</link>
      <pubDate>Wed, 20 Nov 2024 23:36:34 +0000</pubDate>
      <guid>https://vec6.com/blog/data-science-methodology/</guid>
      <description>&lt;p&gt;数据分析方法论是一个系统化的方法论，覆盖了数据科学的全生命周期，这套方法论对于新手学习数据科学也比较重要，Data Science  Methodology 包含如下的10点。&lt;/p&gt;&#xA;&lt;h2 id=&#34;business-understanding&#34;&gt;Business understanding&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：是从业务的视角确定业务的核心问题和期望，确保数据科学的分析方向与业务需求一致，同时确保业务问题具体、明确，方便后续转化为数据科学问题。&lt;/p&gt;&#xA;&lt;p&gt;流程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;明确目标：确定业务的核心问题和期望，比如“如何提高用户的复购率？”&lt;/li&gt;&#xA;&lt;li&gt;分解问题：将业务多个可操作的问题，比如哪些用户复购？复购周期是多长？用户流水的主要原因是什么？&lt;/li&gt;&#xA;&lt;li&gt;提出关键问题：针对具体业务提问，确保问题的答案能为后续分析提供明确方向，比如用户复购率的基准是什么？与行业平均水平相比如何？哪些产品复购率高？影响复购率的主要因素有哪些（价格、物流和推销等）？&lt;/li&gt;&#xA;&lt;li&gt;最终目标：将业务目标转化为数据科学问题，比如能否通过用户行为数据预测哪些用户可能复购，并针对这些用户设计个性化营销策略？&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;analytic-approach&#34;&gt;Analytic approach&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：选择最合适当前业务问题的分析方法，为后续数据准备、建模等环球打下基础，将业务问题转化为数据科学问题。&lt;/p&gt;&#xA;&lt;p&gt;流程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;描述性分析：了解过去发生了什么？比如销量趋势分析，历史销售数据。&lt;/li&gt;&#xA;&lt;li&gt;分类分析：将数据进行分组，比如将客户行为分类为优质客户、中等客户和风险客户。&lt;/li&gt;&#xA;&lt;li&gt;预测分析：通过学习历史数据，对未来的数据进行预测，比如用户推荐购买。&lt;/li&gt;&#xA;&lt;li&gt;策略性分析（优化分析）：找到最优解，比如推荐最适合的促销方式。&lt;/li&gt;&#xA;&lt;li&gt;诊断性分析：了解为什么会这样？比如为什么目标促销活动很差。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;data-requirements&#34;&gt;Data requirements&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：明确分析问题需要哪些数据，以及这些数据的范围和粒度，是否与业务相关，确保后续的数据收集工作高效且有针对性。&lt;/p&gt;&#xA;&lt;p&gt;流程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;列出所需的数据：通过明确的业务问题，确定需要的具体数据类型和字段。结合Business understading的目标，拆解具体的数据需求。确定数据来源（客户数据、交易数据、外部数据等）。&lt;/li&gt;&#xA;&lt;li&gt;定义数据范围：时间范围（一年、一月或者一周？）、粒度（按天汇总、还是按月汇总）。&lt;/li&gt;&#xA;&lt;li&gt;与业务目标结合：确保收集的数据直接服务问题，而不是获取一切可以获取的数据。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;data-collection&#34;&gt;Data collection&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：从不同来源获取需要的数据，确保质量来源，为后续的数据理解、清洗和建模奠定基础。这个阶段主要关注数据来源问题，比如是否因为导入导出或者编码问题导致的异常或者无效值。&lt;/p&gt;&#xA;&lt;p&gt;流程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;明确数据来源：内部数据和外部数据。&lt;/li&gt;&#xA;&lt;li&gt;确保数据质量：数据是否完整、是否有缺失值。数据是否一致，比如同一个字段在不同的数据库定义不一样。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;data-understanding&#34;&gt;Data understanding&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：对数据进行初步的分析，发现数据中的模式和问题。这个阶段关注数据的模式、异常值和缺失的业务意义，评估数据是否满足需求，然后将这些有问题的数据进行评估和记录，到下一个数据准备阶段进行处理。&lt;/p&gt;&#xA;&lt;p&gt;流程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;探索数据：了解数据的基本情况，比如分布、中位数，是否有缺失值和异常值，通过可视化探索数据分布情况等。&lt;/li&gt;&#xA;&lt;li&gt;回答问题：数据是否满足支持目标？数据是否存在要清洗的问题？&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;data-preparation&#34;&gt;Data preparation&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：提高数据的可用性和质量。将原始数据转化为结构化、规范化的数据集，清理问题数据，增强数据的分析和建模价值。&lt;/p&gt;&#xA;&lt;p&gt;流程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;数据清洗：去除数据中的错误和噪声，提高数据质量。比如处理缺失值（删除、填补）、处理异常值和处理无效值。&lt;/li&gt;&#xA;&lt;li&gt;数据格式化：确保数据的格式和类型一致，便于后续处理和分析。比如统一数据格式（日期格式一致）和标准化分类字段（比如北京和北京市）。&lt;/li&gt;&#xA;&lt;li&gt;特征工程：为建模提取和构造新特征，以提高模型性能。&#xA;&lt;ol&gt;&#xA;&lt;li&gt;创建新特征：比如从日期中提取出日、月份新字段。&lt;/li&gt;&#xA;&lt;li&gt;特征转换：比如归一化（将数值缩放到相同的范围，如0-1），标准化&lt;/li&gt;&#xA;&lt;li&gt;编码分类变量：比如将类别变量转换为数值变量，比如成都为[1,0,0]，上海为[0,1,0]。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;数据集成：将来自不同数据源的数据整合。比如合并数据集（多表通过唯一ID关联合并为一张表）、重复数据处理。&lt;/li&gt;&#xA;&lt;li&gt;数据采样：从大数据中提取有代表性的小样本，提高效率。比如随机采样（从百万数据中，随机抽取10万），分层采样（按照某些特征分层抽出样本，保证样本分布与总体一致）。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;modeling&#34;&gt;Modeling&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：构建数学或统计模型，捕获数据中的规律或关系，建模只适合开发描述性模型和预测性模型，这是因为有以下几个原因：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;数据建模的基础是数据驱动：描述性和预测性是根据已有数据构建，核心是从数据中提取规律和模式，而不需要额外的领域知识或约束条件，而诊断性和规范性则需要更多的额外知识或推理逻辑。&lt;/li&gt;&#xA;&lt;li&gt;数据建模不关注因果关系：描述性和预测性不关注数据中的因果关系，只关注数据中的相关性，诊断性和规范性分析需要解释“为什么和怎么办”，所以超出了传统的数据建模范畴。&lt;/li&gt;&#xA;&lt;li&gt;数据建模关注“是什么”和“会发生什么”：描述性分析回答“是什么（提取数据特征）”、预测性分析回答“会发生什么”（未来结果）、诊断性分析回答“为什么”，规范性分析回答“怎么办”，后两者需要不同的方法论来支持。&lt;/li&gt;&#xA;&lt;li&gt;数据建模的工具与目标契合：数据建模的工具（回归分析、决策树和神经网络等）最适合用来描述和预测。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;流程：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;明确模型目标：是预测问题（回归）？还是分类问题（分类）？还是发现隐藏模式（无监督学习）？&lt;/li&gt;&#xA;&lt;li&gt;选择合适的算法：比如回归（线性回归、Lasso等）、分类（决策树、逻辑回归等）、聚类（K-means、层次聚类等）、降维（PCA、t-SNE等）和时间序列（ARIMA、LSTM等）。&lt;/li&gt;&#xA;&lt;li&gt;分割数据：将数据分为训练集、验证集和测试集，确保模型可以在未见过的数据上表现良好。&lt;/li&gt;&#xA;&lt;li&gt;模型训练：使用训练集让模型学习数据的规律。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：评估模型的性能和适用性，确保模型能够满足业务需求和预期目标，该阶段与Modeling阶段是相辅相成的，会反复在两个阶段之间来回，以让模型达到最佳和最适合业务的状态。&lt;/p&gt;&#xA;&lt;p&gt;核心评估方法-诊断措施阶段&#xA;通过一系列定量指标和可视化工具对模型进行性能评估，识别模型的优势和潜在问题，比如性能指标（如准确率、MSE）、可视化工具（如混淆矩阵、残差图）、交叉验证等&lt;/p&gt;&#xA;&lt;p&gt;常见分类模型常用的定量指标有：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;准确率（Accuracy）：用于预测正确的比例。&lt;/li&gt;&#xA;&lt;li&gt;精准率（Precision）：用于被预测为正类中实际为正的比例。&lt;/li&gt;&#xA;&lt;li&gt;召回率（Recall）：用于实际正类中被预测为正类的比例。&lt;/li&gt;&#xA;&lt;li&gt;F1分数（F1-Score）：精确率和召回率的调和平均数。&lt;/li&gt;&#xA;&lt;li&gt;ROC-AUC：衡量模型区分正负样本的能力。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;常见回归模型有：&lt;/p&gt;</description>
    </item>
    <item>
      <title>人工智能基础名词理解</title>
      <link>https://vec6.com/blog/inteliigence-terms/</link>
      <pubDate>Sat, 26 Jun 2021 06:48:33 +0000</pubDate>
      <guid>https://vec6.com/blog/inteliigence-terms/</guid>
      <description>&lt;h2 id=&#34;人工智能artificial-intelligence-ai&#34;&gt;人工智能（Artificial Intelligence, AI）&lt;/h2&gt;&#xA;&lt;p&gt;人工智能是一个比较广泛的概念，这个概念实际上指的是让机器像人一样思考，其最早由计算机科学之父阿兰图灵在1950年的一篇《计算机器与智能》论文中写出“如果电脑能在5分钟能回答由人类测试者提出的一系列的问题，且超过30%回答让测试者误认为人类所答，则电脑通过测试”，这段话也直接启蒙式的开启了人工智能领域的研究。 而“人工智能”一词，第一次出现在1956年，达特茅斯大学召开的学术会议室，由人工智能之父约翰·麦卡锡首次提出。 通常人工智能被分为弱人工智能和强人工智能，前者可以让机器有一定程度的学习、理解和推理能力，后者则是由自适应能力，比如解决一些之前没有遇见过的问题，我们常在电影里看见的机器人就是一种强人工智能。&lt;/p&gt;&#xA;&lt;h2 id=&#34;机器学习machine-learning-ml&#34;&gt;机器学习（Machine Learning, ML）&lt;/h2&gt;&#xA;&lt;p&gt;机器学习为人工智能的一个子领域，也可以理解为弱人工智能的一种实现，而机器学习做的事情是让机器取模拟和实现人类的学习行为，以获得新的技能和知识。 人工智能领域的先驱Arthur Samuel在1959年给出的机器学习定义为“不直接编程，却能赋予计算机提供能力的方法”，而美国工程院院士Tom Mitchell则给出了一个更明确的含义，指出“机器学习是通过某项人物的经验数据提高了在该人物上的能力”。&lt;/p&gt;&#xA;&lt;p&gt;机器学习关注的是通过从数据中学习，并得到能够作出预测或决策的算法/模型（机器训练的产物），这里的特点是机器通过数据进行学习，而不是通过硬编码。&lt;/p&gt;&#xA;&lt;p&gt;而如今机器学习已在多个领域得到了很好的应用，大致上可以将机器学习的分为几个研究方向：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;模式识别&lt;/li&gt;&#xA;&lt;li&gt;自然语言处理&lt;/li&gt;&#xA;&lt;li&gt;数据挖掘&lt;/li&gt;&#xA;&lt;li&gt;计算机视觉&lt;/li&gt;&#xA;&lt;li&gt;语言识别&lt;/li&gt;&#xA;&lt;li&gt;统计学习&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;算法模型&#34;&gt;算法/模型&lt;/h2&gt;&#xA;&lt;p&gt;机器学习中，算法和模型（两者意思一样）实际上指的是一个东西，这是因为在训练前需要根据自己需求找到一个合适的算法，这个算法此时是通用的、未做调整和以及初始化的一个工具，而通过训练，即数据驱动下的优化（数据的学习）、参数的调整后，这个算法则具备了特定任务执行的能力，常见的算法比如回归算法、SVM向量机、聚类算法、降维算法、决策树、朴素贝叶斯等。&lt;/p&gt;&#xA;&lt;p&gt;而根据这些算法可以分为&lt;strong&gt;监督学习&lt;/strong&gt;、&lt;strong&gt;无监督学习&lt;/strong&gt;、&lt;strong&gt;半监督学习&lt;/strong&gt;、&lt;strong&gt;强化学习&lt;/strong&gt;，其中&lt;strong&gt;监督学习&lt;/strong&gt;在日语中被称为“有老师的学习”，本质上是让机器学习带有“&lt;strong&gt;标准答案&lt;/strong&gt;”的数据，然后再让机器学习做题，根据做题的结果对比标准答案，根据误差进行调整，经过多次反复，让机器的误差越来越小。 像上面这样在带有标签（答案）的数据上学习的过程被称为“&lt;strong&gt;训练&lt;/strong&gt;”，而训练用到的数据被称为“&lt;strong&gt;训练集&lt;/strong&gt;”，但也被叫做“&lt;strong&gt;数据集&lt;/strong&gt;”，因为该数据集是被拿来训练的，所以被称为训练集，同样训练集在自然语言处理中被称为“&lt;strong&gt;语料库&lt;/strong&gt;”，在训练集里面每一个数据被称为“&lt;strong&gt;样本&lt;/strong&gt;”，在训练过程中反复针对误差作出的调整则被称为“&lt;strong&gt;调参&lt;/strong&gt;”。&lt;/p&gt;&#xA;&lt;p&gt;而训练出来的结果则为称为“&lt;strong&gt;模型&lt;/strong&gt;”，模型其实也是算法，但为了区分，所以将机器学习的结果称为模型。模型可以用来针对训练集相似类型的问题去得到一个结论（值），这个过程则被称为&amp;quot;&lt;strong&gt;预测&lt;/strong&gt;&amp;quot; 。&lt;/p&gt;&#xA;&lt;p&gt;上述的步骤实际上就是针对一个算法，然后通过数据驱动的优化、不断的调整参数，具备了特定任务的执行能力后，而得到的一个模型，这个模型可以去执行特定的任务，比如识别这个是不是“猫”。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;无监督学习&lt;/strong&gt;在日语中被称为“没有老师的学习”，这就意味着数据不含标准答案，其作用是通过数据本身的结构或分布中来进行学习，典型的就是聚类，比如你给一只狗的照片和一只鸟的照片，虽然不能识别出这是鸟或狗，但无监督学习相关算法能做到这两张照片不是一类数据。&lt;/p&gt;&#xA;&lt;p&gt;无监督学习算法并不是完整不需要数据驱动的优化，而是不依赖于有明确答案的数据指导，但仍然可以从数据中学习和调整参数来实现目标。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;半监督学习&lt;/strong&gt;是利用多个模型对同一个数据进行预测，如果这些结果多数一致，则可以将这个数据和结果放在一起作为新的训练集，由于半监督学习可以利用标注数据来丰富未标注数据，所以目前正是热门的研究。&lt;/p&gt;&#xA;&lt;p&gt;之所以半监督学习这样热门是因为带有“标准答案”的数据集几乎都是由人工整理和标注，需要大量的人力、成本、时间，也被叫为“&lt;strong&gt;黄金数据（Gold Data）&lt;/strong&gt;”，所以半监督学习则可以用少量的标注数据集来得到更多的标注数据集来减少其人工、成本、时间。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;则是针对的是需要一系列彼此关联决策的问题，比如自动驾驶、电子竞技等，这类问题往往需要一边预测，一边跟着环境的反馈规划下一次决策。&lt;/p&gt;&#xA;&lt;h2 id=&#34;人工神经网络artificial-neural-networks-ann&#34;&gt;人工神经网络（Artificial Neural Networks, ANN）&lt;/h2&gt;&#xA;&lt;p&gt;人工神经网络在1943年提出，人工神经网络通过模仿人脑的神经系统以及连接方式，所以人工神经网络更是一个框架或者计算模型，而人脑最基本单元的&lt;strong&gt;神经元&lt;/strong&gt;在神经网络中被称为&lt;strong&gt;感知器&lt;/strong&gt;（1958年），而通过很多的感知器进行组合，模仿组合成人脑的神经网络。&lt;/p&gt;&#xA;&lt;p&gt;这里需要注意的是感知器实际上算法模型，但是是一个简单的线性分类器，但是无数个线性分类器就可以解决很复杂的任务。&lt;/p&gt;&#xA;&lt;h2 id=&#34;深度学习deep-learning-dl&#34;&gt;深度学习（Deep Learning, DL）&lt;/h2&gt;&#xA;&lt;p&gt;深度学习是机器学习的一个子领域，主要包含了使用多层神经网络（DNN）类似的结构的机器学习方法，深度学习包含深度神经网络（DNN）、卷积神经网络（CNN）、递归神经网络（RNN）等。&lt;/p&gt;&#xA;&lt;h2 id=&#34;参考文章&#34;&gt;参考文章&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/33892253&#34;&gt;什么是机器学习？&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/wangzhongqiu/p/9856628.html&#34;&gt;人工智能、机器学习、深度学习、神经网络概念说明&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/50422088/answer/120936714&#34;&gt;神经网络啥时候改名叫“深度学习”了？&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/30545893/answer/152317466&#34;&gt;深度学习和人工智能之间是什么样的关系？&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://book.douban.com/subject/34856701/&#34;&gt;《自然语言处理入门》&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(二)漫话中文分词：Trie、KMP、AC自动机</title>
      <link>https://vec6.com/blog/chinesecutwords-2/</link>
      <pubDate>Wed, 18 Nov 2020 08:51:13 +0000</pubDate>
      <guid>https://vec6.com/blog/chinesecutwords-2/</guid>
      <description>&lt;h2 id=&#34;trie树&#34;&gt;Trie树&lt;/h2&gt;&#xA;&lt;p&gt;在上一篇文章当中，说到了一些匹配的算法，但是算法有了，还得需要一个高效的数据结构，不能只是通过[&amp;lsquo;中国人&amp;rsquo;, &amp;lsquo;中东人&amp;rsquo;]等结构来进行存放，可以想象一下，如果有几十万的词，那么这个列表的占用的内存非常大。 Trie树，也被称为前缀树，该词源自单词&lt;code&gt;retrieval&lt;/code&gt;，发音和&lt;code&gt;try&lt;/code&gt;相同，Trie树可为词库提供一种高效的分词数据结构，该结构本质上是一种树状数据结构，比如&amp;quot;中国人&amp;quot;、&amp;ldquo;中东人&amp;quot;三个字符串构造的Trie树为下图，图中能够很清楚的看见，Trie树结构能够很好的节省相同前缀单词所浪费的空间，因为这两个词都是以&amp;quot;中&amp;quot;开头，所以可以使用同一个父辈节点。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://vec6.com/post/images/2020/11/1605767937-1-0012449.jpg&#34; alt=&#34;1605767937-1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;除此之外，Trie树还对查询的速度有一定的优化，如果以列表存放词来说，如果列表存放的词达到了20万个，那么最坏的情况是你需要匹配的词在存放于列表最后，那么就相当于要将这20万个词全部遍历，可想而知浪费了非常多的计算资源。 而Trie查询的次数最大的次数取决于查找的字符串长度，比如&lt;code&gt;中国人&lt;/code&gt;，那么查询次数最大仅为3次。 下图为基于同一份10万左右的词典，待分词文本为字符长度150，使用正向最大匹配算法在列表和Trie两种结构上进行分词的运行时间，从下图可以看出来差距非常大。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://vec6.com/post/images/2020/11/1605767940-2-0012449.jpg&#34; alt=&#34;1605767940-2&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Trie树的查找方式则是通过层层查询，而不是直接遍历词典，比如&amp;quot;中国人&amp;rdquo;，首先会查找第一层中是否有&amp;quot;中&amp;quot;这个字符，如果没有查询到则返回查询失败，如果有则继续查找&amp;quot;中&amp;quot;字符对应的下一层是否有&amp;quot;国&amp;quot;，如果没有则返回查询识别，如果有则继续查找&amp;quot;国&amp;quot;下一层是否有&amp;quot;人&amp;quot;，此时找到存在&amp;quot;人&amp;quot;这个节点，并且该节点标注为蓝色，表明是一个词，所以返回该字符串为一个词。 其实要实现这样的数据结构，大致的功能点为下面两点：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;查询词&lt;/li&gt;&#xA;&lt;li&gt;添加词&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;除此之外还需要考虑如果标记词的结束节点，首先可以约定，默认情况都返回&amp;quot;False&amp;quot;表示为未查询到或设置失败，而返回&amp;quot;True&amp;quot;则表示查询到或设置成功，每个节点为一个字符，而字典当中的&lt;code&gt;__value&lt;/code&gt;表示是否为结束节点（即一个词的尾字符），如果是则为&lt;code&gt;True&lt;/code&gt;，不是则为&lt;code&gt;False&lt;/code&gt;，整体可以采用函数或者类来定义。 实现代码：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Trie&lt;/span&gt;():  &lt;span style=&#34;color:#75715e&#34;&gt;#定义一个Trie类型&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):  &lt;span style=&#34;color:#75715e&#34;&gt;#为这个生成的实例定义一个名为_children的对象，用于存放词的Trie结构&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_children &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_add_word&lt;/span&gt;(self, word):  &lt;span style=&#34;color:#75715e&#34;&gt;# 定义一个添加词的实例方法&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        child &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_children  &lt;span style=&#34;color:#75715e&#34;&gt;# 首先会将_children的对象赋值给child&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i,char &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(word):  &lt;span style=&#34;color:#75715e&#34;&gt;# 然后从头遍历添加词的每一个字符&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; char &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; child:  &lt;span style=&#34;color:#75715e&#34;&gt;# 查看当前字符是否存在Trie树上&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                child[char] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;__value&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;} &lt;span style=&#34;color:#75715e&#34;&gt;# 如果没有则新建一个对象，并设置特殊key__value为False，表明这不是一个结尾字符&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; (len(word) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):  &lt;span style=&#34;color:#75715e&#34;&gt;# 判断是否为结尾字符&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                child[char][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;__value&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 如果是则将特殊key：__value设为True，表明为结尾字符&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            child &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; child[char]  &lt;span style=&#34;color:#75715e&#34;&gt;# 如果还有字符，则将当前字符对象更新为child，那么下一次查找则是基于上一次对象下&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 添加完成返回True&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_get_word&lt;/span&gt;(self, word):  &lt;span style=&#34;color:#75715e&#34;&gt;# 查找词&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        child &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_children  &lt;span style=&#34;color:#75715e&#34;&gt;# 同样设置一个child变量，用于控制当前的字符对象 &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; char &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; word:  &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            child &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; child&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(char)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; child &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; :  &lt;span style=&#34;color:#75715e&#34;&gt;# 只要其中一个没有查找到，那么说明匹配识别，则返回False&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; child[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;__value&amp;#39;&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;# 如果没有匹配失败则返回特殊__value的值&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;#回True表示词典中存在该词，返回False表示不存在或者传递进来的词不成词 &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;将Trie实现后，就可以在正向或者反向等算法中来进行使用，从而提高运算的效率，但是使用Trie树的时候，可能无法动态的计算其词的长度，所以根据上一篇文章当中修改的最大正向匹配算法的长度计算我手动计算填写。 下面的代码是基于&lt;a href=&#34;https://vec6.com/posts/2022-04-15-1-chinese-words-cut/&#34;&gt;《[一]漫话中文分词：最大匹配,双向最大,最小词数》&lt;/a&gt;文章中的最大正向匹配算法，但其中的词典则是使用Trie结构，改动了两处：&lt;/p&gt;</description>
    </item>
    <item>
      <title>(一)漫话中文分词：最大匹配,双向最大,最小词数</title>
      <link>https://vec6.com/blog/chinesecutwords-1/</link>
      <pubDate>Sun, 08 Nov 2020 10:36:34 +0000</pubDate>
      <guid>https://vec6.com/blog/chinesecutwords-1/</guid>
      <description>&lt;p&gt;中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等&lt;/li&gt;&#xA;&lt;li&gt;字标注分词：如HMM（隐马尔可夫）模型等&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。&lt;/p&gt;&#xA;&lt;h2 id=&#34;词典分词&#34;&gt;词典分词&lt;/h2&gt;&#xA;&lt;p&gt;基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是&amp;quot;结婚/的/和/尚未/结婚/的&amp;quot;，而实际中则会被分词为&amp;quot;结婚/的/和尚/未/结婚/的&amp;quot;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如&lt;a href=&#34;https://zh.wikipedia.org/wiki/Trie&#34;&gt;Trie树&lt;/a&gt;和&lt;a href=&#34;https://zh.wikipedia.org/wiki/AC%E8%87%AA%E5%8A%A8%E6%9C%BA%E7%AE%97%E6%B3%95&#34;&gt;AC自动机&lt;/a&gt;，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。&lt;/p&gt;&#xA;&lt;h3 id=&#34;正向最大匹配算法forward-maximum-matching&#34;&gt;正向最大匹配算法（Forward Maximum Matching）&lt;/h3&gt;&#xA;&lt;p&gt;正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的&amp;quot;最大&amp;quot;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置+1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置+1开始再次进行切分和查询。 比如，有一段文本&amp;quot;中文分词算法&amp;quot;，字典中只包含了一个词&amp;quot;分词&amp;quot;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到&amp;quot;中文&amp;quot;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到&amp;quot;中&amp;quot;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[&amp;ldquo;中&amp;rdquo;]。 继续进行第二次切分，得到的文本为&amp;quot;文分&amp;quot;，进行查询词典，第一次查询&amp;quot;文分&amp;quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&amp;rsquo;文&amp;rsquo;查询第二次，未查询到，那么返回最后这个字符&amp;quot;文&amp;quot;，加上次的结果记作[&amp;ldquo;中&amp;rdquo;,&amp;ldquo;文&amp;rdquo;] 继续第三次切分，得到文本&amp;quot;分词&amp;quot;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[&amp;ldquo;中&amp;rdquo;, &amp;ldquo;文&amp;rdquo;, &amp;ldquo;分词&amp;rdquo;]。 继续第四次切分，得到文本&amp;quot;算法&amp;quot;，进行查询字典，第一次查询&amp;quot;算法&amp;quot;在字典中不存在，去掉最后一个字符，继续以剩余部分&amp;rsquo;算&amp;rsquo;查询第二次，未查询到，那么返回最后这个字符&amp;quot;算&amp;quot;，加上次的结果记作[&amp;ldquo;中&amp;rdquo;, &amp;ldquo;文&amp;rdquo;, &amp;ldquo;分词&amp;rdquo;, &amp;ldquo;算&amp;rdquo;] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[&amp;ldquo;中&amp;rdquo;, &amp;ldquo;文&amp;rdquo;, &amp;ldquo;分词&amp;rdquo;, &amp;ldquo;算&amp;rdquo;, &amp;ldquo;法&amp;rdquo;] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sentence &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;中文分词算法&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 输入的句子&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cutList &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;分词&amp;#39;&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;# 分词词典&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;#设置切分起始位置&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;maxWidth &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(max(cutList, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;len)) &lt;span style=&#34;color:#75715e&#34;&gt;# 得到字典当中最大的切分宽度&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cut_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [] &lt;span style=&#34;color:#75715e&#34;&gt;# 设置一个空的分词结果&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (start &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; len(sentence)):  &lt;span style=&#34;color:#75715e&#34;&gt;#开始循环，如果start大于等于句子长度则停止分词&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    end &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; start &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; maxWidth     &lt;span style=&#34;color:#75715e&#34;&gt;# 计算每次切分的停止位置&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sentence[start: end] &lt;span style=&#34;color:#75715e&#34;&gt;# 开始切分，文本为变量start和end的区间内字符&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; ( word ) :  &lt;span style=&#34;color:#75715e&#34;&gt;# python对于空字符串会转换为False&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ( word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; cutList ) :  &lt;span style=&#34;color:#75715e&#34;&gt;# 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            cut_result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(word)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; start &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; len(word) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# 然后将开始位置设置为当前开始位置加上被匹配词的长度&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (len(word[:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            cut_result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(word) &lt;span style=&#34;color:#75715e&#34;&gt;# 如果最后一个字符也没有被匹配到，那么返回最后一个字符&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; word[:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;# 将word去掉最后一个字符串并重新计算&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; start &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# 将位置加1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(cut_result)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#[&amp;#39;中&amp;#39;, &amp;#39;文&amp;#39;, &amp;#39;分词&amp;#39;, &amp;#39;算&amp;#39;, &amp;#39;法&amp;#39;] &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;反向最大匹配算法backward-maximum-matching&#34;&gt;反向最大匹配算法（Backward Maximum Matching）&lt;/h3&gt;&#xA;&lt;p&gt;反向最大匹配算法与正向最大匹配算法是相反的，比如&amp;quot;中文分词算法&amp;quot;文本的正向最大匹配算法在切分宽度为2的时候，是从&amp;quot;中文&amp;quot;开始切分的，而反向则是从&amp;quot;算法&amp;quot;开始切分的。 除了反向的切分，其中对于切分块内的文本依次去掉最后一个字符也变为了依次去掉第一个字符，比如正向第一个切分块&amp;quot;中文&amp;quot;后，如果没有匹配到，则去掉&amp;quot;文&amp;quot;，再对&amp;quot;中&amp;quot;字符进行匹配，而反向则是拿到&amp;quot;算法&amp;quot;后，如果没有匹配到，则是去掉&amp;quot;算&amp;quot;，再对&amp;quot;法&amp;quot;进行匹配。 反向最大匹配算法对比于正向最大匹配算法来说，可以解决一定的交集型歧义，比如本文&amp;quot;他说的确实在理&amp;quot;，理想情况下希望的分词结果中包含&amp;quot;确实&amp;quot;这一词，而正向最大匹配算法结果为&amp;quot;他/说/的确/实/在理&amp;quot;，而反向最大匹配算法的结果为&amp;quot;他/说/的/确实/在理&amp;quot;。 这两种方式很难区分到底谁好谁坏，比如上面的问题中，如果你希望的分词为&amp;quot;的确&amp;quot;，但是如果使用反向的话就很难被分出来。 反向最大匹配算法具体的实现代码：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
