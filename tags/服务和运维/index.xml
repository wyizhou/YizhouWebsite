<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>服务和运维 on Yizhou</title>
    <link>/tags/%E6%9C%8D%E5%8A%A1%E5%92%8C%E8%BF%90%E7%BB%B4/</link>
    <description>Recent content in 服务和运维 on Yizhou</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Oct 2023 13:24:56 +0000</lastBuildDate>
    <atom:link href="/tags/%E6%9C%8D%E5%8A%A1%E5%92%8C%E8%BF%90%E7%BB%B4/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>最近搭建了一台家用Nas</title>
      <link>/posts/home-nas-2023/</link>
      <pubDate>Thu, 19 Oct 2023 13:24:56 +0000</pubDate>
      <guid>/posts/home-nas-2023/</guid>
      <description>从两个月前，我着手搭建了一套家用Nas，起因之前自己的资料一直存放在ICloud，因为自己的资料比较多，在600多G，所以自己一直购买的国区2T，每个月68元，一年816元，也不算便宜了。再加上自己的资料和储存的数据越来越大，所以就有就有了这个需求。
对于资料不多、储存量不大的用户，不建议使用Nas，使用自带的云储存足够了，自己组建Nas只有在一定储存量级，才会有价值，所以对于数据量少的朋友，看看就行了，别折腾了。
同时自己电脑有两台，一台苹果笔记本，一台Windows笔记本，ICloud对于Windows系统同步是真的难用，甚至打开文件夹都会卡顿，所以也是其自己搭建Nas的一个小因素了。
选购之前对比了多种方式，购买其他国内云储存服务、自己购买云主机自己搭建、购买成品NAS、自己搭组NAS。
先说说前两者，国内云服务研究了一番后，排除了，具体原因暂时不好说。然后就是云主机搭建，后面研究一番发现不论国内还是国内的云主机，硬盘都贵得要死。
再说说后两者，也是我纠结最久的，成品Nas的优点就是不用折腾、省心以及有技术服务支撑，缺点就是配置低、价格贵。自己组Nas的优点在于便宜、配置高，缺点就是折腾，也不省心。
那作为以技术出身的人来说，自己组Nas的缺点到成了一个有趣的过程，而配置高也满足了技术人员常常的幻想”配置不够“这一假说。
所以最终思考了一下，准备折腾一下自己，组建Nas就成了最优选择。
选购 既然是自己组建，就得自己挑选硬件、考虑配置、选择系统，系统我选择的是群晖，也就是所谓的黑群晖，主要考虑到配套的软件足够好用，关于硬件选配，可以参考隔壁网这篇文章《2022年组建群晖实体机的一些建议 - 黑白群晖 - 隔壁网 (gebi1.com)》。
我的配置目前是：
CPU：考虑到需要看电影做解码，所以选择了i3-8100T，然后带T属于低功耗，对于Nas来说也完全够用了。 内存：32G。 硬盘：8TB硬盘两块，用于存放资料、文件。16TB硬盘一块，用于存放电影。 主板：选择了大厂微星的Z370M Mortar。 机箱：Treasure宝藏盒，快700多的一个机箱了，已经算比较贵了，但是买回来看了一下外观，真香。 风扇：利民AXP90 电源：Flex台达250W电源 这里需要说一个事情，我没有配备缓存，群晖的系统可以配备2块M2 Nvme硬盘作为缓存组，可以用于读写缓存，单盘只能作为读缓存。我研究了一番后，决定还是不用缓存，第一个对于我这样的家用环境上缓存提升不大，也有很多家用小伙伴测试后发了相关的结论。第二个就是缓存可能导致故障，第三个性价比不高。
系统 系统采用的是群晖，既然是自己搭建的，所以也就是所谓的黑群晖了，采用的引导是最简单的引导arpl-i18n，该引导是基于巴西大佬arpl的版本上改进的，对于两者我还真不知道差距在哪里，但只要好用，然后用的人多就行了。
然后黑群晖的引导是需要一个U盘的，正常情况下U盘可以插在机箱前面或者后面，但是对于花了700多买了这么漂亮的机箱，U盘插在前后，犹如”一坨牛屎抹在鲜花上“。
所以我就购买了一个主板的usb插座，然后将引导U盘插在上面，这样就可以把usb隐藏在机箱内了。
备份 我的备份组合主要集中为四种，分别RAID1+Cloud Sync+Hyper Backup网盘+Hyper Backup硬盘，其中RAID1虽然说起来不算是”备份“方案，但本质提供冗余还是一种数据的保障，所以也就算了进来。
Hyper Backup则是提供了一个完整的套件、文件增量备份，每天执行一次。
Cloud Sync则是作为一个补充，提供一个实时的文件同步，将文件同步到我的Onedrive，至于为什么是Onedrive，第一个原因是支持，第二个原因是因为我每年要买Office365，所以有1TB的免费网盘容量。
Hyper Backup是目前我觉得最好的备份方案，Hyper Backup支持备份到云服务商、本地USB储存设备等方式。Hyper Backup我采用了两种方式，分别为阿里云盘通过Alist以WebDAV的方式挂载到本地，然后Hyper Backup选择WebDAV方式进行备份，好处在于阿里云盘服务器在国内比较稳定，关于如何通过查看Alist关于阿里云盘的文档。
第二种方式，我采用的是本地USB插入硬盘的方式，这个方式有个好处在于能无限扩大备份目的地的容量，比如你可以通过一个硬盘盒做多个盘用USB接入，同时配置好了也比较省电省电。
为什么说省电呢？在设置为USB为备份目的地的时候，配置可以选择，当备份完成后自动断开卸载USB，然后再加上硬盘盒的自动休眠，就可以在备份完成后的耗电降到最低，比如下图（因为我目前已经设置完成，所以找不到这个选项了，就借用了一张图片）。
但这种方式有一个问题，就是你在下次备份的时候没有办法自动挂载，你需要通过定时任务，将在定时的自动备份任务开始之前将USB重新挂载好。
重新挂载USB需要两个步骤，第一个步骤确定USB编号，需要打开群晖的SSH，然后进入到终端后使用lsusb命令，我找到我备份目的地的硬盘为2-3：
第二个步骤是将如下代码放入群晖的任务计划，选择运行的用户为root（一定注意，卸载挂载外置设备需要root权限）：
然后将如下代码放入脚本中，其中的2-3需要替换为备份的外置目的地设备编号。
echo 0 &amp;gt; /sys/bus/usb/devices/2-3/authorized echo 1 &amp;gt; /sys/bus/usb/devices/2-3/authorized 最后，设定好计划时间，我目前是在Hyper Backup之前的5分钟挂载。
结尾 说完了后，说说成品Nas和自己搭建Nas推荐哪个，整体就是完全不建议没有技术或者不想折腾的人来自己搭建Nas，更建议直接买群晖或者其他厂商的成品Nas，对于有技术的用户，这种方式就挺推荐的。
搭建到如今，也算是对稳定性比较信任了，将自己照片、文件都转移到了群晖，同时也说服了自己的爱人将手机照片转到了群晖，不得不说群晖的配套软件是真的好用，买软件送硬件也不是无道理，看来以后买一个白裙也不是不可以。
参考文章 # Synology NAS 的 USB 外接硬碟在系統中的順序 / HyperBackup 備份後自動卸載該如何重新掛載 2022年组建群晖实体机的一些建议 - 黑白群晖 - 隔壁网 (gebi1.</description>
    </item>
    <item>
      <title>GPU服务器的多人环境搭建</title>
      <link>/posts/gpu-server-lxd-multiplexing/</link>
      <pubDate>Fri, 21 Oct 2022 04:52:07 +0000</pubDate>
      <guid>/posts/gpu-server-lxd-multiplexing/</guid>
      <description>环境 CPU: Intel(R) Xeon(R) Gold 6154* 2
Momery: 128G GPU: 3080TI * 2
Disk: 16T HD &amp;amp; 512 SSD
System: Ubuntu 22.04 Server
前言 因为最近团队对于GPU的需求量增加，但之前都是在工作电脑上直接使用GPU进行训练和使用，而几个人中只有一两台电脑有显卡，所以后期就更新了服务器。 随之而来的就会产生一个问题，大家直接用账号密码链接上去每个人的环境、配置都会造成环境、冲突，甚至导致系统出错，所有就有必要通过容器的解决方案让每个人都隔离，相互不影响，并且不能直接操作到宿主机，以保证所有人操作都在容器进行而不影响到宿主机，除此之外也需要给每一个容器映射显卡。 在这个基础上有三个相关的技术，分别为Docker、LXC、虚拟机（PVE、ESXI等）。首先排除掉Docker，Docker比较适用应用级的层面上，不符合需求。虚拟机虽然可以直通显卡等，但直通单张显卡后，其他虚拟机无法使用。所以最后就锁定到了LXD，LXD由Canonical有限公司发起，是一个类容器管理系统，而底层则基于LXC容器，额外提供了更加方便的API接口、分布式、网络管理、储存管理等，同时Ubuntu 22.04自集成了LXD，所以这里部署也是通过LXD来管理LXC容器。
准备 16T的机械盘，分为两个分区（分区可以使用fdisk），分1T用于给LXC作储存池，剩余的15T用于挂载到宿主机下的/data目录，后期映射到每个容器的/data目录下，用于所有容器之间的数据互传和数据存储（因为相互之间的数据不涉及隐私，所以可以共用），这样的好处在于大家都将重要的数据放置/data，即使容器出现了问题，也不会影响到数据的丢失问题。 显卡驱动可以直接通过Ubuntu的GPU驱动安装，如果你没有安装显卡驱动，你可以直接输入nvidia-smi，会得到相关的提示，而不用安装网上的教程去设置，因为非常麻烦。 使用apt安装zfsutils-linux，前者用于安装LXD的储存池驱动，LXD支持多种储存池，这用于储存LXD、LXC相关的数据。 使用apt安装bridge-utils，该工具是用于管理和创建网桥设备所需要的工具和程序。 初始化LXD 通过命令执行sudo lxd init，就会得到如下的问题：
LXD Clustering：用于集群配置，单节点不需要，默认为no，回车即可 new storage pool：需要创建一个存储池，输入yes Name of storage pool：给存储池命名，默认为default，回车即可 storage backend：存储后端，默认使用zfs，回车即可 Create a new ZFS pool：需要创建一个ZFS池，默认为no，输入yes use an existing block device：使用现有的块设备（硬盘），输入yes Path to block device：输入现有的硬盘，比如我的为sda1，那么就输入/dev/sda1 MAAS server：MAAS是一个用于将物理机视为云服务器的集群服务，默认为no，回车即可 new local network bridge：是否创建一个新的桥接网络，输入yes new bridge be called：命名新的网桥名称，默认即可 IPv4：IPv4相关配置，默认为auto，回车即可 IPv6：IPv6相关配置，默认为auto，回车即可 would you like lxd to be available over the network：使用想通过网络访问LXD，默认为no，回车即可 would you like stale cached images to be updated automatically：默认yes，回车即可 YAML printed：是否打印出lxd init的配置信息，默认为no，回车即可 创建容器模板 创建容器模板的意义在于你可以设置一个基础配置的容器，然后基于这个容器进行复制出多个容器出来，而不用再针对每个容器进行重复的基础设置。 在使用前需要下载一个已打包的容器镜像，因为需要下载，所以可以使用清华大学的国内镜像用于提升下载镜像的速度。 添加清华大学镜像源：</description>
    </item>
    <item>
      <title>基于LXC容器的Openwrt搭建</title>
      <link>/posts/lxc-openwrt/</link>
      <pubDate>Tue, 06 Sep 2022 05:02:51 +0000</pubDate>
      <guid>/posts/lxc-openwrt/</guid>
      <description>前期工作 环境和机器配置：
机器：5105v4 i226-v版本
pve： 7.1.2，内核Linux 5.13.19-2-pve
准备工作：
将网线连接到pve管理口，如果已安装openwrt，然后关闭原openwrt虚拟机，删除直通的网卡。
准备一份没有引导的openwrt固件包，可以是img也可以是tar.gz，但一定是没有引导的包，可以看文件名中包含rootfs字符，比如openwrt-x86-64-generic-ext4-rootfs.img或openwrt-21.02.0-x86-64-rootfs.tar.gz（前者是我自己编译的，重点在于rootfs）。
PVE直通配置 连接到PVE，输入命令：
nano /etc/default/grub 找到下面这一行：
GRUB_CMDLINE_LINUX_DEFAULT=&amp;#34;quiet&amp;#34; 然后添加&amp;quot;intel_iommu=on&amp;quot;，这是英特尔的直通配置，AMD需要自行查找配置命令：
GRUB_CMDLINE_LINUX_DEFAULT=&amp;#34;quiet intel_iommu=on&amp;#34; 然后更新引导：
update-grub 修改nano /etc/modules内核模块文件，添加直通的驱动，让系统启动的时候载入这些驱动：
vfio vfio_iommu_type1 vfio_pci vfio_virqfd 执行命令来更新内核：
update-initramfs -u -k all. 创建基础环境文件包 tar.gz格式 如果包后缀为tar.gz，则通过scp直接上传至pve，以下[]内的字符根据自己情况进行替换（包括[和]符号），然后跳至下一章节：
scp [固件路径.tar.gz] root@[pveIP地址]:/var/lib/vz/template/cache img格式 上传固件：
scp [固件路径.img] root@[pveIP地址]:/root 这里需要注意，如果你的固件包是带squashfs字符，比如openwrt-x86-64-generic-squashfs-rootfs.img，你需要按照下面的方式进行解压。 安装解压包：
apt install squashfs-tools 解压镜像文件：
unsquashfs [固件路径.img] 解压完成后你在同级目录下会得到squashfs-root文件夹，然后进入该文件夹，跳至3步骤。 如果你是不带squashfs字符，比如openwrt-x86-64-generic-ext4-rootfs.img，则需要通过挂载镜像，得到内部文件，首先创建一个挂载点（下面操作在root目录中进行）：
mkdir op 然后挂载镜像：
mount -t ext4 -o loop [固件路径.img] /root/op 然后进入/root/op，跳至3步骤（完成后，通过使用umount /root/op进行卸载镜像）。
打包为pve的CT模板包： 进入上述2步骤中得到的文件夹中，然后使用下列命令进行打包，得到的文件下文称为op-ct模版：
tar zcf /var/lib/vz/template/cache/[固件名称].tar.gz .</description>
    </item>
  </channel>
</rss>
