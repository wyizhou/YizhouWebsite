<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DataScience on Lucas Wu</title>
    <link>https://vec6.com/en/tags/datascience/</link>
    <description>Recent content in DataScience on Lucas Wu</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 08 Nov 2020 10:36:34 +0000</lastBuildDate>
    <atom:link href="https://vec6.com/en/tags/datascience/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(1) Talking About Chinese Word Segmentation: Maximum Matching, Bidirectional Matching, Minimal Word Count</title>
      <link>https://vec6.com/en/blog/chinesecutwords-1/</link>
      <pubDate>Sun, 08 Nov 2020 10:36:34 +0000</pubDate>
      <guid>https://vec6.com/en/blog/chinesecutwords-1/</guid>
      <description>&lt;p&gt;Chinese word segmentation refers to the process of dividing text into words, where the concatenated result equals the original text. Chinese word segmentation has always been a significant area in the NLP field. Most text mining tasks are based on it. However, Chinese differs from English, where words are separated by spaces, making English semantically less complex compared to Chinese.&lt;/p&gt;&#xA;&lt;p&gt;There has always been a business demand for Chinese word segmentation. However, due to being busy with other projects, I had not studied it thoroughly before. Recently, I started exploring Chinese word segmentation algorithms. While the field is relatively mature, its performance for out-of-vocabulary words or specific domain texts is often unsatisfactory. To achieve better results, a combination of algorithms or the use of manual dictionaries is often necessary.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
